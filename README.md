# üéì B√†i T·∫≠p L·ªõn M√¥n NLP - H·ªçc K√¨ 2025

**ƒê·ªÅ t√†i**: X·ª≠ L√Ω Ng√¥n Ng·ªØ T·ª± Nhi√™n - Text Classification & Machine Translation

**Sinh vi√™n th·ª±c hi·ªán**:
- Ng√¥ Vi·∫øt Thuy·∫øt
- Nguy·ªÖn ƒê·ª©c H∆∞ng

**Tr∆∞·ªùng**: ƒê·∫°i h·ªçc C√¥ng ngh·ªá - ƒêHQGHN

---

## üìã M·ª•c L·ª•c

- [T·ªïng Quan](#-t·ªïng-quan)
- [B√†i 1: Text Classification](#-b√†i-1-text-classification)
- [B√†i 2: Machine Translation](#-b√†i-2-machine-translation)
- [C·∫•u Tr√∫c Th∆∞ M·ª•c](#-c·∫•u-tr√∫c-th∆∞-m·ª•c)
- [Y√™u C·∫ßu H·ªá Th·ªëng](#-y√™u-c·∫ßu-h·ªá-th·ªëng)
- [H∆∞·ªõng D·∫´n C√†i ƒê·∫∑t](#-h∆∞·ªõng-d·∫´n-c√†i-ƒë·∫∑t)
- [K·∫øt Qu·∫£](#-k·∫øt-qu·∫£)
- [T√†i Li·ªáu Tham Kh·∫£o](#-t√†i-li·ªáu-tham-kh·∫£o)

---

## üéØ T·ªïng Quan

Repository n√†y ch·ª©a gi·∫£i ph√°p cho 2 b√†i t·∫≠p l·ªõn m√¥n NLP:

1. **B√†i 1**: Ph√¢n lo·∫°i vƒÉn b·∫£n (Text Classification) s·ª≠ d·ª•ng Transformer
2. **B√†i 2**: D·ªãch m√°y song ng·ªØ Anh-Vi·ªát (Machine Translation) s·ª≠ d·ª•ng mBART-50

---

## üìù B√†i 1: Text Classification

### üéØ M·ª•c ti√™u

X√¢y d·ª±ng m√¥ h√¨nh ph√¢n lo·∫°i vƒÉn b·∫£n s·ª≠ d·ª•ng ki·∫øn tr√∫c **Transformer t·ª´ scratch** (kh√¥ng d√πng pre-trained).

### üìä Dataset

- **Source**: Custom dataset
- **Task**: Multi-class text classification
- **Data location**: `Data/Data1/`

### üèóÔ∏è Ki·∫øn tr√∫c

**Transformer Architecture**:
- **Encoder**: Multi-head self-attention + Feed-forward network
- **Positional Encoding**: Sinusoidal encoding
- **Layers**: 6 encoder layers
- **Attention Heads**: 8 heads
- **Hidden Size**: 512
- **Feed-forward Dim**: 2048

### üìÅ Files

```
Code/Bai1/
‚îî‚îÄ‚îÄ transformerFinalHung.ipynb    # Notebook ch√≠nh v·ªõi Transformer implementation
```

### üöÄ C√°ch ch·∫°y

```bash
# 1. M·ªü Jupyter Notebook
jupyter notebook Code/Bai1/transformerFinalHung.ipynb

# 2. Ch·∫°y l·∫ßn l∆∞·ª£t c√°c cell t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi
```

---

## üåê B√†i 2: Machine Translation

### üéØ M·ª•c ti√™u

Fine-tune m√¥ h√¨nh **mBART-50** cho 2 h∆∞·ªõng d·ªãch:
1. **Ti·∫øng Vi·ªát ‚Üí Ti·∫øng Anh** (Medical domain)
2. **Ti·∫øng Anh ‚Üí Ti·∫øng Vi·ªát** (Medical domain)

### üìä Dataset

- **Source**: Medical research abstracts
- **Size**: 500,000 parallel sentences (EN-VI)
- **Domain**: Medical/Healthcare
- **Data location**: `Data/Data2/`

**Files**:
```
Data/Data2/
‚îú‚îÄ‚îÄ train.en.txt              (500,000 sentences)
‚îú‚îÄ‚îÄ train.vi.txt              (500,000 sentences)
‚îú‚îÄ‚îÄ public_test.en.txt        (3,000 sentences)
‚îî‚îÄ‚îÄ public_test.vi.txt        (3,000 sentences)
```

### üèóÔ∏è Model Architecture

**mBART-50** (facebook/mbart-large-50-many-to-many-mmt):
- **Type**: Multilingual Seq2Seq Transformer
- **Parameters**: ~611M
- **Languages**: 50 languages
- **Pre-training**: Denoising autoencoding
- **Fine-tuning**: Medical domain EN-VI translation

### üìÅ Files

```
Code/Bai2/
‚îú‚îÄ‚îÄ mBART50_VI_EN.ipynb          # Notebook VI‚ÜíEN (33 code cells + 33 markdown)
‚îú‚îÄ‚îÄ mBART50_EN_VI.ipynb          # Notebook EN‚ÜíVI (26 code cells + 26 markdown)
‚îú‚îÄ‚îÄ NOTEBOOK_GUIDE.md            # H∆∞·ªõng d·∫´n chi ti·∫øt c·∫•u tr√∫c notebook
‚îú‚îÄ‚îÄ MARKDOWN_VI_EN.txt           # Markdown cho VI‚ÜíEN
‚îú‚îÄ‚îÄ MARKDOWN_EN_VI.txt           # Markdown cho EN‚ÜíVI
‚îú‚îÄ‚îÄ insert_markdown.py           # Script t·ª± ƒë·ªông th√™m markdown
‚îî‚îÄ‚îÄ add_markdown.py              # Script helper
```

### üîß Training Configuration

#### **Common Config**:
```python
# Data preprocessing
N_TOTAL = 30000              # S·ªë pairs d√πng ƒë·ªÉ train
TRAIN_RATIO = 0.975          # 97.5% train, 2.5% val
MIN_CHARS = 2
MAX_CHARS = 400

# Training
EPOCHS = 3
BATCH_SIZE = 2
GRADIENT_ACCUMULATION = 4    # Effective batch = 8
WARMUP_RATIO = 0.03
WEIGHT_DECAY = 0.01
FP16 = True                  # Mixed precision training
```

#### **VI‚ÜíEN Specific**:
```python
SRC_LANG = "vi_VN"
TGT_LANG = "en_XX"
LEARNING_RATE = 1e-5
```

#### **EN‚ÜíVI Specific**:
```python
SRC_LANG = "en_XX"
TGT_LANG = "vi_VN"
LEARNING_RATE = 3e-5         # Higher LR for EN‚ÜíVI
```

### üìà K·∫øt qu·∫£

#### **VI‚ÜíEN (Vietnamese ‚Üí English)**

| Metric | Baseline | Fine-tuned | Improvement |
|--------|----------|------------|-------------|
| **BLEU** | 17.77 | **31.75** | +13.98 |
| **Gemini Judge** | - | 78.5/100 | - |
| **Medical Score** | 54.8/100 | 70.1/100 | +15.3 |

**Top Errors**:
- NE_mismatch_omit: 61.5% (Thi·∫øu t√™n ri√™ng)
- Terminology_miss: 33.5% (Thi·∫øu thu·∫≠t ng·ªØ y khoa)
- Number_mismatch: 11.5% (S·ªë kh√¥ng kh·ªõp)

#### **EN‚ÜíVI (English ‚Üí Vietnamese)**

| Metric | Baseline | Fine-tuned | Improvement |
|--------|----------|------------|-------------|
| **BLEU** | 26.09 | **43.42** | +17.33 |
| **Gemini Judge** | - | 75.2/100 | - |
| **Medical Score** | 67.1/100 | 72.9/100 | +5.8 |

**Top Errors**:
- Repetition: 19.5% (L·∫∑p t·ª´)
- Number_mismatch: 16.0% (S·ªë kh√¥ng kh·ªõp)
- Vietnamese_function_word_missing: 4.5% (Thi·∫øu t·ª´ ch·ª©c nƒÉng)

### üí° Features

#### **Data Pipeline**:
- ‚úÖ Auto dataset discovery tr√™n Kaggle
- ‚úÖ Data cleaning (remove whitespace, normalize)
- ‚úÖ Filtering (length constraints)
- ‚úÖ Deduplication (MD5 hash)
- ‚úÖ Train/Val/Test splitting

#### **Training**:
- ‚úÖ Mixed precision training (FP16)
- ‚úÖ Gradient accumulation
- ‚úÖ Early stopping (patience=2)
- ‚úÖ Best model selection (by val_loss)
- ‚úÖ Auto push to HuggingFace Hub

#### **Evaluation**:
- ‚úÖ sacreBLEU scoring
- ‚úÖ Error analysis (automatic error tagging)
- ‚úÖ Gemini Judge scoring (LLM-based evaluation)
- ‚úÖ Medical domain scoring (rule-based)
- ‚úÖ Visualization (loss curves, error distribution)

### üöÄ C√°ch ch·∫°y

#### **Option 1: Kaggle Notebook (Recommended)**

```bash
# 1. Upload dataset l√™n Kaggle
#    - T·∫°o dataset v·ªõi 4 files: train.en.txt, train.vi.txt, public_test.en.txt, public_test.vi.txt

# 2. T·∫°o notebook m·ªõi v√† enable GPU
#    Settings > Accelerator > GPU T4 x2

# 3. Add Kaggle Secrets
#    - HF_TOKEN: Hugging Face write token
#    - GeminiAPI: Google Gemini API key

# 4. Upload notebook
#    - mBART50_VI_EN.ipynb (cho VI‚ÜíEN)
#    - ho·∫∑c mBART50_EN_VI.ipynb (cho EN‚ÜíVI)

# 5. Ch·∫°y l·∫ßn l∆∞·ª£t c√°c cells
```

#### **Option 2: Local (v·ªõi GPU)**

```bash
# 1. Clone repo
git clone https://github.com/NgoVietThuyet/NLP.git
cd NLP

# 2. C√†i ƒë·∫∑t dependencies
pip install transformers datasets sacrebleu accelerate torch

# 3. Setup environment variables
export HF_TOKEN="your_huggingface_token"
export GEMINI_API_KEY="your_gemini_key"

# 4. Ch·∫°y notebook
jupyter notebook Code/Bai2/mBART50_VI_EN.ipynb
```

### üåü Model Checkpoints

**Hugging Face Hub**:
- **VI‚ÜíEN**: [ngothuyet/mbart50-vien](https://huggingface.co/ngothuyet/mbart50-vien)
- **EN‚ÜíVI**: [ngothuyet/mbart50-envi](https://huggingface.co/ngothuyet/mbart50-envi)

**Usage**:
```python
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# VI‚ÜíEN
model = AutoModelForSeq2SeqLM.from_pretrained("ngothuyet/mbart50-vien")
tokenizer = AutoTokenizer.from_pretrained("ngothuyet/mbart50-vien")

# EN‚ÜíVI
model = AutoModelForSeq2SeqLM.from_pretrained("ngothuyet/mbart50-envi")
tokenizer = AutoTokenizer.from_pretrained("ngothuyet/mbart50-envi")
```

---

## üìÇ C·∫•u Tr√∫c Th∆∞ M·ª•c

```
NLP/
‚îú‚îÄ‚îÄ README.md                               # File n√†y
‚îú‚îÄ‚îÄ BAO_CAO_BAI1_FINAL.tex                 # B√°o c√°o B√†i 1
‚îú‚îÄ‚îÄ BAO_CAO_BAI2_FINAL_V3.tex              # B√°o c√°o B√†i 2
‚îú‚îÄ‚îÄ Bao_cao_VLSP2025_MT.tex                # B√°o c√°o VLSP2025
‚îÇ
‚îú‚îÄ‚îÄ Code/
‚îÇ   ‚îú‚îÄ‚îÄ Bai1/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ transformerFinalHung.ipynb     # Transformer t·ª´ scratch
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Bai2/
‚îÇ       ‚îú‚îÄ‚îÄ mBART50_VI_EN.ipynb            # Fine-tune VI‚ÜíEN
‚îÇ       ‚îú‚îÄ‚îÄ mBART50_EN_VI.ipynb            # Fine-tune EN‚ÜíVI
‚îÇ       ‚îú‚îÄ‚îÄ NOTEBOOK_GUIDE.md              # H∆∞·ªõng d·∫´n chi ti·∫øt
‚îÇ       ‚îú‚îÄ‚îÄ MARKDOWN_VI_EN.txt             # Markdown VI‚ÜíEN
‚îÇ       ‚îú‚îÄ‚îÄ MARKDOWN_EN_VI.txt             # Markdown EN‚ÜíVI
‚îÇ       ‚îî‚îÄ‚îÄ *.py                           # Helper scripts
‚îÇ
‚îî‚îÄ‚îÄ Data/
    ‚îú‚îÄ‚îÄ Data1/                             # Dataset B√†i 1
    ‚îî‚îÄ‚îÄ Data2/                             # Dataset B√†i 2
        ‚îú‚îÄ‚îÄ train.en.txt                   # 500k English sentences
        ‚îú‚îÄ‚îÄ train.vi.txt                   # 500k Vietnamese sentences
        ‚îú‚îÄ‚îÄ public_test.en.txt             # 3k test EN
        ‚îî‚îÄ‚îÄ public_test.vi.txt             # 3k test VI
```

---

## üíª Y√™u C·∫ßu H·ªá Th·ªëng

### **Minimum Requirements**:

```
OS: Windows 10/11, Linux, macOS
Python: 3.8+
RAM: 16GB (32GB recommended)
GPU: NVIDIA GPU with 8GB+ VRAM (T4, P100, V100, A100)
CUDA: 11.0+
Storage: 20GB free space
```

### **Dependencies**:

```
torch>=2.0.0
transformers>=4.30.0
datasets>=2.14.0
accelerate>=0.20.0
sacrebleu>=2.3.0
jupyter
pandas
matplotlib
```

---

## üõ†Ô∏è H∆∞·ªõng D·∫´n C√†i ƒê·∫∑t

### **1. Clone Repository**

```bash
git clone https://github.com/NgoVietThuyet/NLP.git
cd NLP
```

### **2. T·∫°o Virtual Environment**

```bash
# D√πng venv
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows

# Ho·∫∑c d√πng conda
conda create -n nlp python=3.10
conda activate nlp
```

### **3. C√†i ƒê·∫∑t Dependencies**

```bash
# Install PyTorch (with CUDA)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Install Transformers & Co.
pip install transformers datasets accelerate

# Install evaluation tools
pip install sacrebleu google-generativeai

# Install Jupyter
pip install jupyter ipykernel
```

### **4. Setup API Keys**

#### **Hugging Face Token**:
```bash
# Get key from: https://huggingface.co/settings/tokens
export HF_TOKEN="hf_xxxxxxxxxxxx"

# Ho·∫∑c login qua CLI
huggingface-cli login
```

#### **Gemini API Key**:
```bash
# Get key from: https://aistudio.google.com/app/apikey
export GEMINI_API_KEY="AIzaxxxxxxxxxxxxx"
```

### **5. Run Notebooks**

```bash
# Start Jupyter
jupyter notebook

# M·ªü notebook:
# - Code/Bai1/transformerFinalHung.ipynb
# - Code/Bai2/mBART50_VI_EN.ipynb
# - Code/Bai2/mBART50_EN_VI.ipynb
```

---

## üìä K·∫øt Qu·∫£

### **B√†i 2: Machine Translation Summary**

| Direction | BLEU Baseline | BLEU Fine-tuned | Improvement |
|-----------|---------------|-----------------|-------------|
| **VI‚ÜíEN** | 17.77 | **31.75** | +13.98 |
| **EN‚ÜíVI** | 26.09 | **43.42** | +17.33 |

**Observation**: EN‚ÜíVI ƒë·∫°t BLEU cao h∆°n v√¨ model d·ªÖ h·ªçc pattern EN‚ÜíVI v√† medical terminology ti·∫øng Vi·ªát √≠t ambiguous h∆°n.

---

## üêõ Troubleshooting

### **Issue 1: CUDA Out of Memory**

```python
# Gi·∫£m batch size
TRAIN_BS = 1
GRAD_ACC = 8

# Clear cache
torch.cuda.empty_cache()
```

### **Issue 2: Tokenizer KeyError**

```python
# Ph·∫£i set src_lang v√† tgt_lang
tokenizer.src_lang = "vi_VN"
tokenizer.tgt_lang = "en_XX"
```

### **Issue 3: Gemini API Rate Limit**

```python
# TƒÉng sleep time
time.sleep(2)  # gi·ªØa m·ªói request
```

---

## üìö T√†i Li·ªáu Tham Kh·∫£o

### **Papers**:
1. **Attention Is All You Need** - https://arxiv.org/abs/1706.03762
2. **mBART** - https://arxiv.org/abs/2001.08210

### **Libraries**:
- **Transformers**: https://huggingface.co/docs/transformers
- **sacreBLEU**: https://github.com/mjpost/sacrebleu

### **Models**:
- **mBART-50**: https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt
- **Our models**:
  - https://huggingface.co/ngothuyet/mbart50-vien
  - https://huggingface.co/ngothuyet/mbart50-envi

---

## üë• Contributors

- **Ng√¥ Vi·∫øt Thuy·∫øt** - [GitHub](https://github.com/NgoVietThuyet)
- **Nguy·ªÖn ƒê·ª©c H∆∞ng**

---

## üìÑ License

MIT License - Free to use for research and education.

---

## üìß Contact

- **Repository**: https://github.com/NgoVietThuyet/NLP
- **Issues**: https://github.com/NgoVietThuyet/NLP/issues

---

**Last Updated**: December 2024
