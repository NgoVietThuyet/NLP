{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"CELL 0 — (Tuỳ chọn) Kiểm tra GPU + version môi trường","metadata":{}},{"cell_type":"code","source":"import os\n\n# Chặn Transformers tự động import TensorFlow/Flax (tránh lỗi protobuf / MessageFactory)\nos.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\nos.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n\nprint(\"[OK] Environment flags set (no TF / no Flax).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:54:27.113445Z","iopub.execute_input":"2025-12-14T18:54:27.113595Z","iopub.status.idle":"2025-12-14T18:54:27.120784Z","shell.execute_reply.started":"2025-12-14T18:54:27.113581Z","shell.execute_reply":"2025-12-14T18:54:27.120139Z"}},"outputs":[{"name":"stdout","text":"[OK] Environment flags set (no TF / no Flax).\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"CELL 1 — Import + kiểm tra GPU/version","metadata":{}},{"cell_type":"code","source":"import sys, torch\nimport numpy as np\n\nimport transformers, datasets, accelerate\n\nprint(\"Python:\", sys.version.split()[0])\nprint(\"Torch:\", torch.__version__)\nprint(\"CUDA:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"GPU:\", torch.cuda.get_device_name(0))\n\nprint(\"Transformers:\", transformers.__version__)\nprint(\"Datasets:\", datasets.__version__)\nprint(\"Accelerate:\", accelerate.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:54:27.122557Z","iopub.execute_input":"2025-12-14T18:54:27.122791Z","iopub.status.idle":"2025-12-14T18:54:39.122858Z","shell.execute_reply.started":"2025-12-14T18:54:27.122773Z","shell.execute_reply":"2025-12-14T18:54:39.122188Z"}},"outputs":[{"name":"stdout","text":"Python: 3.11.13\nTorch: 2.6.0+cu124\nCUDA: True\nGPU: Tesla T4\nTransformers: 4.53.3\nDatasets: 4.4.1\nAccelerate: 1.9.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"CELL 2 — Cài tối thiểu","metadata":{}},{"cell_type":"code","source":"import importlib.util, subprocess\n\ndef pip_install(pkg: str):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n\n# sacrebleu: bắt buộc\nif importlib.util.find_spec(\"sacrebleu\") is None:\n    print(\"[INFO] Installing sacrebleu ...\")\n    pip_install(\"sacrebleu\")\nelse:\n    print(\"[OK] sacrebleu already installed\")\n\n# hf_transfer: optional tăng tốc push/pull\nif importlib.util.find_spec(\"hf_transfer\") is None:\n    print(\"[INFO] Installing hf_transfer (optional) ...\")\n    pip_install(\"hf_transfer\")\nelse:\n    print(\"[OK] hf_transfer already installed\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:54:39.123680Z","iopub.execute_input":"2025-12-14T18:54:39.124125Z","iopub.status.idle":"2025-12-14T18:54:44.603187Z","shell.execute_reply.started":"2025-12-14T18:54:39.124094Z","shell.execute_reply":"2025-12-14T18:54:44.602389Z"}},"outputs":[{"name":"stdout","text":"[INFO] Installing sacrebleu ...\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 51.8/51.8 kB 1.6 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 104.1/104.1 kB 3.5 MB/s eta 0:00:00\n[OK] hf_transfer already installed\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"CELL 3 — Import phần còn lại","metadata":{}},{"cell_type":"code","source":"import re, math, random, hashlib\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n    set_seed,\n)\nimport sacrebleu\n\nprint(\"[OK] Imports done.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:54:44.604008Z","iopub.execute_input":"2025-12-14T18:54:44.604480Z","iopub.status.idle":"2025-12-14T18:55:24.595311Z","shell.execute_reply.started":"2025-12-14T18:54:44.604451Z","shell.execute_reply":"2025-12-14T18:55:24.594674Z"}},"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765738492.522072      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765738492.625769      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"[OK] Imports done.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"CELL 4 — Auto-find dataset folder","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\n\ndef find_dataset_dir() -> Path:\n    base = Path(\"/kaggle/input\")\n    for d in base.glob(\"*\"):\n        if d.is_dir() and (d / \"train.en.txt\").exists() and (d / \"train.vi.txt\").exists():\n            return d\n    for d in base.glob(\"*/*\"):\n        if d.is_dir() and (d / \"train.en.txt\").exists() and (d / \"train.vi.txt\").exists():\n            return d\n    raise FileNotFoundError(\"Could not find dataset folder containing train.en.txt/train.vi.txt\")\n\nDATA_DIR = find_dataset_dir()\nprint(\"[OK] Found dataset dir:\", DATA_DIR)\n\nTRAIN_EN = DATA_DIR / \"train.en.txt\"\nTRAIN_VI = DATA_DIR / \"train.vi.txt\"\nTEST_EN  = DATA_DIR / \"public_test.en.txt\"\nTEST_VI  = DATA_DIR / \"public_test.vi.txt\"\n\nfor p in [TRAIN_EN, TRAIN_VI, TEST_EN, TEST_VI]:\n    print(\"[CHECK]\", p.name, \"->\", \"OK\" if p.exists() else \"NOT FOUND\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:55:24.595928Z","iopub.execute_input":"2025-12-14T18:55:24.596426Z","iopub.status.idle":"2025-12-14T18:55:24.615137Z","shell.execute_reply.started":"2025-12-14T18:55:24.596405Z","shell.execute_reply":"2025-12-14T18:55:24.614580Z"}},"outputs":[{"name":"stdout","text":"[OK] Found dataset dir: /kaggle/input/databaitoanphu\n[CHECK] train.en.txt -> OK\n[CHECK] train.vi.txt -> OK\n[CHECK] public_test.en.txt -> OK\n[CHECK] public_test.vi.txt -> OK\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"CELL 5 — Load file song song EN/VI","metadata":{}},{"cell_type":"code","source":"def read_lines(path: Path, encoding=\"utf-8\"):\n    with open(path, \"r\", encoding=encoding, errors=\"replace\") as f:\n        return [line.rstrip(\"\\n\") for line in f]\n\ndef load_parallel(en_path: Path, vi_path: Path, name=\"data\"):\n    en_lines = read_lines(en_path)\n    vi_lines = read_lines(vi_path)\n    n = min(len(en_lines), len(vi_lines))\n    if len(en_lines) != len(vi_lines):\n        print(f\"[WARN] {name} mismatch -> EN={len(en_lines):,}, VI={len(vi_lines):,}. Using first {n:,} pairs.\")\n    else:\n        print(f\"[INFO] {name} lines -> {n:,} pairs.\")\n    return list(zip(en_lines[:n], vi_lines[:n]))\n\ntrain_pairs_raw = load_parallel(TRAIN_EN, TRAIN_VI, \"train_raw\")\ntest_pairs_raw  = load_parallel(TEST_EN,  TEST_VI,  \"test_raw\")\n\nprint(\"[SAMPLE train_raw]\", train_pairs_raw[0])\nprint(\"[SAMPLE test_raw ]\", test_pairs_raw[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:55:24.615654Z","iopub.execute_input":"2025-12-14T18:55:24.615857Z","iopub.status.idle":"2025-12-14T18:55:27.370139Z","shell.execute_reply.started":"2025-12-14T18:55:24.615841Z","shell.execute_reply":"2025-12-14T18:55:27.369542Z"}},"outputs":[{"name":"stdout","text":"[INFO] train_raw lines -> 500,000 pairs.\n[INFO] test_raw lines -> 3,000 pairs.\n[SAMPLE train_raw] ('To evaluate clinical, subclinical symptoms of patients with otitis media with effusion and V.a at otorhinolaryngology department – Thai Nguyen national hospital', 'Nghiên cứu đặc điểm lâm sàng, cận lâm sàng bệnh nhân viêm tai ứ dịch trên viêm V.A tại Khoa Tai mũi họng - Bệnh viện Trung ương Thái Nguyên')\n[SAMPLE test_raw ] ('Knowledge, practices in public health service utilization among health insurance card’s holders and influencing factors in Vientiane, Lao', 'Thực trạng kiến thức và thực hành của người có thẻ bảo hiểm y tế trong sử dụng dịch vụ khám chữa bệnh ở các cơ sở y tế công và một số yếu tố ảnh hưởng tại tỉnh Viêng Chăn, CHDCND Lào, năm 2017')\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"CELL 6 — Clean / Filter / Dedup","metadata":{}},{"cell_type":"code","source":"import re, hashlib\n\ndef basic_clean(s: str) -> str:\n    s = \"\" if s is None else s\n    s = s.strip()\n    s = re.sub(r\"\\s+\", \" \", s)\n    return s\n\nMIN_CHARS = 2\nMAX_CHARS = 400\n\ndef is_good_pair(src: str, tgt: str) -> bool:\n    if not src or not tgt:\n        return False\n    if len(src) < MIN_CHARS or len(tgt) < MIN_CHARS:\n        return False\n    if len(src) > MAX_CHARS or len(tgt) > MAX_CHARS:\n        return False\n    return True\n\ndef clean_filter_dedup(pairs, name=\"data\"):\n    # clean + filter\n    cleaned = []\n    for src, tgt in pairs:\n        src = basic_clean(src)\n        tgt = basic_clean(tgt)\n        if is_good_pair(src, tgt):\n            cleaned.append((src, tgt))\n    print(f\"[INFO] {name}: after clean+filter -> {len(cleaned):,} pairs\")\n\n    # dedup\n    seen = set()\n    dedup = []\n    for src, tgt in cleaned:\n        h = hashlib.md5((src + \"\\t\" + tgt).encode(\"utf-8\")).hexdigest()\n        if h not in seen:\n            seen.add(h)\n            dedup.append((src, tgt))\n    print(f\"[INFO] {name}: after dedup       -> {len(dedup):,} pairs\")\n    return dedup\n\ntrain_pairs = clean_filter_dedup(train_pairs_raw, \"train\")\ntest_pairs  = clean_filter_dedup(test_pairs_raw,  \"test\")\n\nprint(\"[SAMPLE clean train]\", train_pairs[0])\nprint(\"[SAMPLE clean test ]\", test_pairs[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:55:27.372435Z","iopub.execute_input":"2025-12-14T18:55:27.372638Z","iopub.status.idle":"2025-12-14T18:55:38.588753Z","shell.execute_reply.started":"2025-12-14T18:55:27.372622Z","shell.execute_reply":"2025-12-14T18:55:38.587997Z"}},"outputs":[{"name":"stdout","text":"[INFO] train: after clean+filter -> 489,008 pairs\n[INFO] train: after dedup       -> 340,522 pairs\n[INFO] test: after clean+filter -> 2,946 pairs\n[INFO] test: after dedup       -> 2,943 pairs\n[SAMPLE clean train] ('To evaluate clinical, subclinical symptoms of patients with otitis media with effusion and V.a at otorhinolaryngology department – Thai Nguyen national hospital', 'Nghiên cứu đặc điểm lâm sàng, cận lâm sàng bệnh nhân viêm tai ứ dịch trên viêm V.A tại Khoa Tai mũi họng - Bệnh viện Trung ương Thái Nguyên')\n[SAMPLE clean test ] ('Knowledge, practices in public health service utilization among health insurance card’s holders and influencing factors in Vientiane, Lao', 'Thực trạng kiến thức và thực hành của người có thẻ bảo hiểm y tế trong sử dụng dịch vụ khám chữa bệnh ở các cơ sở y tế công và một số yếu tố ảnh hưởng tại tỉnh Viêng Chăn, CHDCND Lào, năm 2017')\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"CELL 7 — Lấy đúng 5000 từ TRAIN, split 90/10, TEST giữ nguyên public_test","metadata":{}},{"cell_type":"code","source":"import random\nfrom datasets import Dataset, DatasetDict\n\nSEED = 42\nN_TOTAL = 20000\n\nTRAIN_RATIO = 0.9  # 90/10\n\n# 1) shuffle train_pairs\nrng = random.Random(SEED)\nrng.shuffle(train_pairs)\n\n# 2) lấy 5000 (nếu không đủ thì lấy hết)\nn_take = min(N_TOTAL, len(train_pairs))\nsubset = train_pairs[:n_take]\nprint(f\"[INFO] Train pairs available: {len(train_pairs):,}\")\nprint(f\"[INFO] Taking from train     : {n_take:,} pairs\")\n\n# 3) split 90/10\nn_train = int(n_take * TRAIN_RATIO)\nn_val = n_take - n_train\n\ntrain_subset = subset[:n_train]\nval_subset   = subset[n_train:]\n\nprint(f\"[INFO] Split -> train={len(train_subset):,} | val={len(val_subset):,}\")\n\n# 4) build datasets\ntrain_ds = Dataset.from_dict({\n    \"src_text\": [s for s, _ in train_subset],\n    \"tgt_text\": [t for _, t in train_subset],\n})\nval_ds = Dataset.from_dict({\n    \"src_text\": [s for s, _ in val_subset],\n    \"tgt_text\": [t for _, t in val_subset],\n})\n\n# test giữ nguyên từ public_test\ntest_ds = Dataset.from_dict({\n    \"src_text\": [s for s, _ in test_pairs],\n    \"tgt_text\": [t for _, t in test_pairs],\n})\n\nds = DatasetDict({\"train\": train_ds, \"validation\": val_ds, \"test\": test_ds})\nprint(ds)\n\nprint(\"[SAMPLE TRAIN]\", ds[\"train\"][0])\nprint(\"[SAMPLE VAL  ]\", ds[\"validation\"][0])\nprint(\"[SAMPLE TEST ]\", ds[\"test\"][0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:55:38.589594Z","iopub.execute_input":"2025-12-14T18:55:38.589902Z","iopub.status.idle":"2025-12-14T18:55:38.832906Z","shell.execute_reply.started":"2025-12-14T18:55:38.589876Z","shell.execute_reply":"2025-12-14T18:55:38.832146Z"}},"outputs":[{"name":"stdout","text":"[INFO] Train pairs available: 340,522\n[INFO] Taking from train     : 20,000 pairs\n[INFO] Split -> train=18,000 | val=2,000\nDatasetDict({\n    train: Dataset({\n        features: ['src_text', 'tgt_text'],\n        num_rows: 18000\n    })\n    validation: Dataset({\n        features: ['src_text', 'tgt_text'],\n        num_rows: 2000\n    })\n    test: Dataset({\n        features: ['src_text', 'tgt_text'],\n        num_rows: 2943\n    })\n})\n[SAMPLE TRAIN] {'src_text': 'Monitor and control ICP using sedatives, endotracheal intubation, hyperventilation, hydration, diuretics, measures to control blood pressure, and sometimes corticosteroids.', 'tgt_text': 'Theo dõi và kiểm soát ICP bằng cách sử dụng thuốc an thần, đặt nội khí quản, tăng thông khí, hydrat hoá, thuốc lợi tiểu, các biện pháp để kiểm soát huyết áp, và đôi khi là corticosteroid.'}\n[SAMPLE VAL  ] {'src_text': 'The menstrual cycle is one of the most common acne triggers.', 'tgt_text': 'Chu kỳ kinh nguyệt là một trong những nguyên nhân thường gặp nhất gây ra mụn trứng cá.'}\n[SAMPLE TEST ] {'src_text': 'Knowledge, practices in public health service utilization among health insurance card’s holders and influencing factors in Vientiane, Lao', 'tgt_text': 'Thực trạng kiến thức và thực hành của người có thẻ bảo hiểm y tế trong sử dụng dịch vụ khám chữa bệnh ở các cơ sở y tế công và một số yếu tố ảnh hưởng tại tỉnh Viêng Chăn, CHDCND Lào, năm 2017'}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"CELL 8a — Load tokenizer/model mBART-50","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\nMODEL_ID = \"facebook/mbart-large-50-many-to-many-mmt\"\nSRC_LANG = \"en_XX\"\nTGT_LANG = \"vi_VN\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=False)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_ID)\n\n# set src/tgt language (QUAN TRỌNG)\ntokenizer.src_lang = SRC_LANG\ntokenizer.tgt_lang = TGT_LANG   # <-- FIX KeyError: None\n\nforced_bos_token_id = tokenizer.lang_code_to_id[TGT_LANG]\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nprint(\"[OK] tokenizer/model loaded\")\nprint(\"[INFO] tokenizer.src_lang:\", tokenizer.src_lang)\nprint(\"[INFO] tokenizer.tgt_lang:\", tokenizer.tgt_lang)\nprint(\"[INFO] forced_bos_token_id:\", forced_bos_token_id)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:55:38.833628Z","iopub.execute_input":"2025-12-14T18:55:38.833841Z","iopub.status.idle":"2025-12-14T18:55:48.852651Z","shell.execute_reply.started":"2025-12-14T18:55:38.833817Z","shell.execute_reply":"2025-12-14T18:55:48.851955Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84aaf387c5fd4e158df374d0aab00374"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"088c85ad21bc400eb7b64a1ba5c8fa63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f5bf3742c664a0daf95f665ba694195"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c28ffb9df2b045ada5d4779420505494"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3aaea09894d428d98e8538020a0278b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"094eb266358d490b98de1efd2d156ee2"}},"metadata":{}},{"name":"stdout","text":"[OK] tokenizer/model loaded\n[INFO] tokenizer.src_lang: en_XX\n[INFO] tokenizer.tgt_lang: vi_VN\n[INFO] forced_bos_token_id: 250024\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"CELL 8 — Tokenize lại theo train/validation/test","metadata":{}},{"cell_type":"code","source":"# đảm bảo ds đã có ở cell split 5000 (train/validation/test)\nassert \"ds\" in globals(), \"You must run the data-loading/splitting cell to create `ds` first!\"\nassert \"tokenizer\" in globals(), \"You must run the tokenizer/model loading cell first!\"\n\nMAX_SRC_LEN = 256\nMAX_TGT_LEN = 256\n\ndef preprocess_batch(batch):\n    tokenizer.src_lang = SRC_LANG\n\n    model_inputs = tokenizer(\n        batch[\"src_text\"],\n        max_length=MAX_SRC_LEN,\n        truncation=True,\n        padding=False,\n    )\n\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            batch[\"tgt_text\"],\n            max_length=MAX_TGT_LEN,\n            truncation=True,\n            padding=False,\n        )\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntrain_tok = ds[\"train\"].map(preprocess_batch, batched=True, remove_columns=ds[\"train\"].column_names)\nval_tok   = ds[\"validation\"].map(preprocess_batch, batched=True, remove_columns=ds[\"validation\"].column_names)\ntest_tok  = ds[\"test\"].map(preprocess_batch, batched=True, remove_columns=ds[\"test\"].column_names)\n\nprint(\"[OK] Tokenized sizes:\",\n      \"train\", len(train_tok),\n      \"| val\", len(val_tok),\n      \"| test\", len(test_tok))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:55:48.853500Z","iopub.execute_input":"2025-12-14T18:55:48.853842Z","iopub.status.idle":"2025-12-14T18:55:58.149407Z","shell.execute_reply.started":"2025-12-14T18:55:48.853824Z","shell.execute_reply":"2025-12-14T18:55:58.148836Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/18000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fb68a31a0ef4cca9a1167bdfa7243c8"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3951: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4c208905b404ed3ba526b730aa3e171"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2943 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7dd1e9525ae4c76aef7dc43f29e299a"}},"metadata":{}},{"name":"stdout","text":"[OK] Tokenized sizes: train 18000 | val 2000 | test 2943\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"CELL 9 — Đánh giá BLEU trên TEST (public_test) trước khi train","metadata":{}},{"cell_type":"code","source":"@torch.inference_mode()\ndef translate_en2vi(texts, num_beams=5, length_penalty=1.0, no_repeat_ngram_size=3, max_new_tokens=128):\n    model.eval()\n    tokenizer.src_lang = SRC_LANG\n    enc = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_SRC_LEN).to(model.device)\n    out = model.generate(\n        **enc,\n        forced_bos_token_id=tokenizer.lang_code_to_id[TGT_LANG],\n        num_beams=num_beams,\n        length_penalty=length_penalty,\n        no_repeat_ngram_size=no_repeat_ngram_size,\n        max_new_tokens=max_new_tokens,\n        early_stopping=True\n    )\n    return tokenizer.batch_decode(out, skip_special_tokens=True)\n\ndef bleu_on_test(max_samples=None):\n    srcs = ds[\"test\"][\"src_text\"]\n    refs = ds[\"test\"][\"tgt_text\"]\n\n    if max_samples is not None:\n        srcs = srcs[:max_samples]\n        refs = refs[:max_samples]\n\n    hyps = []\n    bs = 16 if torch.cuda.is_available() else 4\n    for i in range(0, len(srcs), bs):\n        hyps.extend(translate_en2vi(srcs[i:i+bs]))\n\n    bleu = sacrebleu.corpus_bleu(hyps, [refs]).score\n    print(f\"[TEST] sacreBLEU ({len(srcs)} samples): {bleu:.2f}\")\n    return bleu\n\n_ = bleu_on_test(max_samples=200)   # đổi None để chạy full public_test\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:55:58.150186Z","iopub.execute_input":"2025-12-14T18:55:58.150452Z","iopub.status.idle":"2025-12-14T18:57:10.868499Z","shell.execute_reply.started":"2025-12-14T18:55:58.150435Z","shell.execute_reply":"2025-12-14T18:57:10.867742Z"}},"outputs":[{"name":"stdout","text":"[TEST] sacreBLEU (200 samples): 26.20\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"CELL — Đăng nhập Hugging Face","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\n\nuser_secrets = UserSecretsClient()\n\n# tên secret của anh là HF_TOKEN (đúng như panel)\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\n\nlogin(token=hf_token)\nprint(\"[OK] Logged in to Hugging Face.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:57:10.869325Z","iopub.execute_input":"2025-12-14T18:57:10.869770Z","iopub.status.idle":"2025-12-14T18:57:11.038322Z","shell.execute_reply.started":"2025-12-14T18:57:10.869741Z","shell.execute_reply":"2025-12-14T18:57:11.037582Z"}},"outputs":[{"name":"stdout","text":"[OK] Logged in to Hugging Face.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"Load model đã train sẵn từ trước trên hugging face về làm model train tiếp data mới \n","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nHF_REPO = \"ngothuyet/mbart50-envi\"\n\ntokenizer = AutoTokenizer.from_pretrained(HF_REPO, use_fast=False)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(HF_REPO)\n\ntokenizer.src_lang = \"en_XX\"\ntokenizer.tgt_lang = \"vi_VN\"\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nprint(\"[OK] Loaded model from HF:\", HF_REPO)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:57:11.039062Z","iopub.execute_input":"2025-12-14T18:57:11.039303Z","iopub.status.idle":"2025-12-14T18:57:22.477846Z","shell.execute_reply.started":"2025-12-14T18:57:11.039288Z","shell.execute_reply":"2025-12-14T18:57:22.477182Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/11.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e7f6458ed0c4930afb70dbc0ab1f408"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d91f64d59e26468f8f65e9196ae5deb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/992 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bac87ca058541f98984f18ece132989"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.37k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b095005d040f441995432fdcb65a43e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"588d6c8ceaa64a0bba9ed0eec85f1735"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/256 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22f509bd0b304f68a2eab21fa7976172"}},"metadata":{}},{"name":"stdout","text":"[OK] Loaded model from HF: ngothuyet/mbart50-envi\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"Đọc train, val loss từ model trước đó","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom huggingface_hub import hf_hub_download\n\nHF_REPO_ID = \"ngothuyet/mbart50-envi\"   # repo của anh\nFILENAME = \"train_val_loss.tsv\"\n\npath = hf_hub_download(repo_id=HF_REPO_ID, filename=FILENAME)\ndf = pd.read_csv(path, sep=\"\\t\")\n\nprint(\"===== ALL TRAIN/VAL LOSS (FROM HF) =====\")\nprint(df.to_string(index=False))   # in hết bảng\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:57:22.478541Z","iopub.execute_input":"2025-12-14T18:57:22.478886Z","iopub.status.idle":"2025-12-14T18:57:22.723191Z","shell.execute_reply.started":"2025-12-14T18:57:22.478866Z","shell.execute_reply":"2025-12-14T18:57:22.722409Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"train_val_loss.tsv:   0%|          | 0.00/114 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32f277959b304ec3b064e02671476043"}},"metadata":{}},{"name":"stdout","text":"===== ALL TRAIN/VAL LOSS (FROM HF) =====\n epoch  train_loss  val_loss\n   1.0      1.8957  1.658935\n   2.0      1.3897  1.586371\n   3.0      1.1786  1.592958\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import os, inspect\nimport torch\nimport pandas as pd\nfrom transformers import (\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n)\n\n# ====== CHECK ======\nassert \"model\" in globals(), \"Missing `model`\"\nassert \"tokenizer\" in globals(), \"Missing `tokenizer`\"\nassert \"train_tok\" in globals(), \"Missing `train_tok`\"\nassert \"val_tok\" in globals(), \"Missing `val_tok`\"\nassert \"SRC_LANG\" in globals() and \"TGT_LANG\" in globals(), \"Missing SRC_LANG/TGT_LANG\"\n\n# Fix stateful tokenizer for mBART-50\ntokenizer.src_lang = SRC_LANG\ntokenizer.tgt_lang = TGT_LANG\n\n# ====== CONFIG (anh chỉnh ở đây) ======\nOUTPUT_DIR  = \"mbart50_envi\"\nNUM_EPOCHS  = 3\n\nTRAIN_BS = 2\nEVAL_BS  = 2\nGRAD_ACC = 8\nLR       = 3e-5\n\n# Hugging Face repo\nHF_REPO_ID = \"ngothuyet/mbart50-envi\"\nHF_PRIVATE = True\n\n# ====== DATA COLLATOR ======\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n\n# ====== TrainingArguments ======\n# Mục tiêu: eval/log theo epoch để có loss, nhưng KHÔNG save checkpoint.\nta_kwargs = dict(\n    output_dir=OUTPUT_DIR,\n    overwrite_output_dir=True,\n\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=TRAIN_BS,\n    per_device_eval_batch_size=EVAL_BS,\n    gradient_accumulation_steps=GRAD_ACC,\n    learning_rate=LR,\n    warmup_ratio=0.03,\n    weight_decay=0.01,\n\n    # ✅ log + eval theo epoch để lấy train_loss & val_loss\n    predict_with_generate=False,     # chỉ cần loss -> nhanh\n    fp16=torch.cuda.is_available(),\n    dataloader_num_workers=2,\n    report_to=\"none\",\n\n    # ✅ KHÔNG LƯU CHECKPOINT (tránh lỗi write optimizer.pt)\n    save_strategy=\"no\",              # ❌ không checkpoint\n    save_only_model=True,            # ❌ không optimizer/scheduler (an toàn)\n)\n\nsig = inspect.signature(Seq2SeqTrainingArguments.__init__).parameters\n\n# logging theo epoch\nif \"logging_strategy\" in sig:\n    ta_kwargs[\"logging_strategy\"] = \"epoch\"\n\n# evaluation theo epoch (tên field có thể khác nhau theo version)\nif \"evaluation_strategy\" in sig:\n    ta_kwargs[\"evaluation_strategy\"] = \"epoch\"\nelif \"eval_strategy\" in sig:\n    ta_kwargs[\"eval_strategy\"] = \"epoch\"\nelse:\n    raise ValueError(\"Transformers version: missing evaluation_strategy/eval_strategy\")\n\n# ✅ Push chỉ 1 lần ở cuối (vì không save checkpoint)\n# (không dùng hub_strategy=\"every_save\" vì không còn save)\nta_kwargs.update(dict(\n    push_to_hub=True,\n    hub_model_id=HF_REPO_ID,\n    hub_private_repo=HF_PRIVATE,\n    hub_strategy=\"end\",              # ✅ chỉ push cuối\n))\n\ntraining_args = Seq2SeqTrainingArguments(**ta_kwargs)\n\n# ====== TRAINER ======\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_tok,\n    eval_dataset=val_tok,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n\nprint(\"[INFO] Start training...\")\ntrain_result = trainer.train()\nprint(\"\\n===== TRAIN DONE =====\")\nprint(train_result)\n\nprint(\"\\n===== FINAL EVAL (VAL) =====\")\nfinal_metrics = trainer.evaluate()\nfor k, v in final_metrics.items():\n    print(f\"{k}: {v}\")\n\n# ====== SAVE FINAL LOCAL (model + tokenizer) ======\ntrainer.save_model(OUTPUT_DIR)          # tạo model.safetensors, config.json,...\ntokenizer.save_pretrained(OUTPUT_DIR)   # tokenizer files\nprint(\"[OK] Saved final model/tokenizer to:\", OUTPUT_DIR)\n\n# ====== EXPORT TSV (train_loss + val_loss) ======\nlogs = trainer.state.log_history\n\ntrain_loss_by_epoch = {}\nval_loss_by_epoch = {}\n\nfor row in logs:\n    # train loss rows\n    if \"epoch\" in row and \"loss\" in row and \"eval_loss\" not in row:\n        train_loss_by_epoch[row[\"epoch\"]] = row[\"loss\"]\n    # eval loss rows\n    if \"epoch\" in row and \"eval_loss\" in row:\n        val_loss_by_epoch[row[\"epoch\"]] = row[\"eval_loss\"]\n\nepochs = sorted(set(list(train_loss_by_epoch.keys()) + list(val_loss_by_epoch.keys())))\ndf = pd.DataFrame({\n    \"epoch\": epochs,\n    \"train_loss\": [train_loss_by_epoch.get(e, None) for e in epochs],\n    \"val_loss\": [val_loss_by_epoch.get(e, None) for e in epochs],\n})\n\ntsv_path = os.path.join(OUTPUT_DIR, \"train_val_loss.tsv\")\ndf.to_csv(tsv_path, sep=\"\\t\", index=False)\nprint(\"[OK] Saved TSV:\", tsv_path)\ndisplay(df)\n\n# ====== PUSH FINAL + TSV ======\n# Vì OUTPUT_DIR chứa model + tokenizer + training_args.bin + train_val_loss.tsv\ntrainer.push_to_hub(commit_message=\"Final after training (no checkpoints) + train_val_loss.tsv\")\nprint(\"[OK] Pushed final model + TSV to Hugging Face.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:57:22.724330Z","iopub.execute_input":"2025-12-14T18:57:22.725042Z","iopub.status.idle":"2025-12-14T21:53:40.686351Z","shell.execute_reply.started":"2025-12-14T18:57:22.725023Z","shell.execute_reply":"2025-12-14T21:53:40.685666Z"}},"outputs":[{"name":"stdout","text":"[INFO] Start training...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/3259613353.py:88: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1689' max='1689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1689/1689 2:52:27, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.500800</td>\n      <td>1.360325</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.171500</td>\n      <td>1.301108</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.991700</td>\n      <td>1.295173</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\n===== TRAIN DONE =====\nTrainOutput(global_step=1689, training_loss=1.2213178129856794, metrics={'train_runtime': 10354.0679, 'train_samples_per_second': 5.215, 'train_steps_per_second': 0.163, 'total_flos': 6193749995618304.0, 'train_loss': 1.2213178129856794, 'epoch': 3.0})\n\n===== FINAL EVAL (VAL) =====\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 02:33]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"eval_loss: 1.2951726913452148\neval_runtime: 153.5677\neval_samples_per_second: 13.024\neval_steps_per_second: 3.256\nepoch: 3.0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4deb083b5a99493782a3d41af32f11ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b389fd918ce47878352c431494c0aa8"}},"metadata":{}},{"name":"stdout","text":"[OK] Saved final model/tokenizer to: mbart50_envi\n[OK] Saved TSV: mbart50_envi/train_val_loss.tsv\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   epoch  train_loss  val_loss\n0    1.0      1.5008  1.360325\n1    2.0      1.1715  1.301108\n2    3.0      0.9917  1.295173","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>val_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>1.5008</td>\n      <td>1.360325</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.0</td>\n      <td>1.1715</td>\n      <td>1.301108</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.0</td>\n      <td>0.9917</td>\n      <td>1.295173</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3053199a76db4ae79982d2fe3ca191c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89510f84fa554845bf9df0a36ec3853c"}},"metadata":{}},{"name":"stdout","text":"[OK] Pushed final model + TSV to Hugging Face.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# lấy log_history và gom theo epoch\nlogs = trainer.state.log_history\n\ntrain_loss_by_epoch = {}\nval_loss_by_epoch = {}\n\nfor row in logs:\n    if \"epoch\" in row and \"loss\" in row and \"eval_loss\" not in row:\n        train_loss_by_epoch[row[\"epoch\"]] = row[\"loss\"]\n    if \"epoch\" in row and \"eval_loss\" in row:\n        val_loss_by_epoch[row[\"epoch\"]] = row[\"eval_loss\"]\n\nepochs = sorted(set(list(train_loss_by_epoch.keys()) + list(val_loss_by_epoch.keys())))\n\ndf = pd.DataFrame({\n    \"epoch\": epochs,\n    \"train_loss\": [train_loss_by_epoch.get(e, None) for e in epochs],\n    \"val_loss\": [val_loss_by_epoch.get(e, None) for e in epochs],\n})\n\ntsv_path = f\"{OUTPUT_DIR}/train_val_loss.tsv\"\ndf.to_csv(tsv_path, sep=\"\\t\", index=False)\n\nprint(\"[OK] Saved TSV:\", tsv_path)\ndisplay(df)\n\n# push TSV lên hub (nằm trong output_dir sẽ được push)\ntrainer.push_to_hub(commit_message=\"Add train_val_loss.tsv\")\nprint(\"[OK] Pushed TSV to hub.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T21:53:40.687206Z","iopub.execute_input":"2025-12-14T21:53:40.687462Z","iopub.status.idle":"2025-12-14T21:54:06.532200Z","shell.execute_reply.started":"2025-12-14T21:53:40.687447Z","shell.execute_reply":"2025-12-14T21:54:06.531446Z"}},"outputs":[{"name":"stdout","text":"[OK] Saved TSV: mbart50_envi/train_val_loss.tsv\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   epoch  train_loss  val_loss\n0    1.0      1.5008  1.360325\n1    2.0      1.1715  1.301108\n2    3.0      0.9917  1.295173","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>val_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>1.5008</td>\n      <td>1.360325</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.0</td>\n      <td>1.1715</td>\n      <td>1.301108</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.0</td>\n      <td>0.9917</td>\n      <td>1.295173</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a2a240869e549509851904199a67ff5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b7a445805434825b1062fb9b200dab7"}},"metadata":{}},{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\nWARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"name":"stdout","text":"[OK] Pushed TSV to hub.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"Load model từ hugging face ","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nHF_REPO_ID = \"ngothuyet/mbart50-envi\"  # <-- đổi\n\nSRC_LANG = \"en_XX\"\nTGT_LANG = \"vi_VN\"\n\ntokenizer = AutoTokenizer.from_pretrained(HF_REPO_ID, use_fast=False)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(HF_REPO_ID)\n\ntokenizer.src_lang = SRC_LANG\ntokenizer.tgt_lang = TGT_LANG\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\n\nprint(\"[OK] Loaded from hub:\", HF_REPO_ID)\nprint(\"[INFO] device:\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T21:54:06.533042Z","iopub.execute_input":"2025-12-14T21:54:06.533391Z","iopub.status.idle":"2025-12-14T21:54:16.678053Z","shell.execute_reply.started":"2025-12-14T21:54:06.533366Z","shell.execute_reply":"2025-12-14T21:54:16.677207Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.68k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d99e2db64a64433801969b0512a438c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f74a99c36aa4482e95f1477b1f5c665c"}},"metadata":{}},{"name":"stdout","text":"[OK] Loaded from hub: ngothuyet/mbart50-envi\n[INFO] device: cuda\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"Sử dụng beam search để dịch","metadata":{}},{"cell_type":"code","source":"import torch\nfrom typing import List\n\nMAX_SRC_LEN = 256\n\n@torch.inference_mode()\ndef translate_en2vi(\n    texts: List[str],\n    num_beams: int = 5,\n    length_penalty: float = 1.0,\n    no_repeat_ngram_size: int = 3,\n    max_new_tokens: int = 128,\n):\n    model.eval()\n    tokenizer.src_lang = SRC_LANG\n    tokenizer.tgt_lang = TGT_LANG\n\n    enc = tokenizer(\n        texts,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=MAX_SRC_LEN\n    ).to(model.device)\n\n    generated = model.generate(\n        **enc,\n        forced_bos_token_id=tokenizer.lang_code_to_id[TGT_LANG],\n        num_beams=num_beams,\n        length_penalty=length_penalty,\n        no_repeat_ngram_size=no_repeat_ngram_size,\n        max_new_tokens=max_new_tokens,\n        early_stopping=True,\n    )\n\n    return tokenizer.batch_decode(generated, skip_special_tokens=True)\n\n# Demo nhanh\ndemo = [\n    \"It's necessary to provide health insurance communication and education for people who live in remote areas and participate interupted health insurance.\",\n    \"I am learning natural language processing.\"\n]\nprint(\"[INPUT ]\", demo)\nprint(\"[OUTPUT]\", translate_en2vi(demo, num_beams=5, length_penalty=1.0, no_repeat_ngram_size=3))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T21:54:16.678956Z","iopub.execute_input":"2025-12-14T21:54:16.679222Z","iopub.status.idle":"2025-12-14T21:54:17.413027Z","shell.execute_reply.started":"2025-12-14T21:54:16.679204Z","shell.execute_reply":"2025-12-14T21:54:17.412249Z"}},"outputs":[{"name":"stdout","text":"[INPUT ] [\"It's necessary to provide health insurance communication and education for people who live in remote areas and participate interupted health insurance.\", 'I am learning natural language processing.']\n[OUTPUT] ['Cần thiết cung cấp dịch vụ truyền thông và giáo dục bảo hiểm y tế cho những người sống ở vùng xa xôi và tham gia BHYT.', 'Tôi đang học cách xử lý ngôn ngữ tự nhiên.']\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"Tính bleu trên toàn bộ tập test","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\n\n# đọc song song public_test.en/vi -> list[(en, vi)]\ntest_pairs_raw = load_parallel(TEST_EN, TEST_VI, \"test_raw\")\n\n# clean/filter/dedup giống pipeline train\ntest_pairs = clean_filter_dedup(test_pairs_raw, \"test\")\n\nprint(f\"[INFO] public_test after clean: {len(test_pairs):,} pairs\")\nprint(\"[SAMPLE TEST]\", test_pairs[0])\n\n# tạo ds_test (giữ nguyên ds train/val của anh nếu đã có)\ntest_ds = Dataset.from_dict({\n    \"src_text\": [s for s, _ in test_pairs],\n    \"tgt_text\": [t for _, t in test_pairs],\n})\n\n# gắn vào DatasetDict nếu đã có ds, còn không thì tạo ds mới\nif \"ds\" in globals():\n    ds[\"test\"] = test_ds\nelse:\n    from datasets import DatasetDict\n    ds = DatasetDict({\"test\": test_ds})\n\nprint(\"[OK] ds['test'] size:\", len(ds[\"test\"]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T21:54:17.413985Z","iopub.execute_input":"2025-12-14T21:54:17.414553Z","iopub.status.idle":"2025-12-14T21:54:17.520486Z","shell.execute_reply.started":"2025-12-14T21:54:17.414533Z","shell.execute_reply":"2025-12-14T21:54:17.519929Z"}},"outputs":[{"name":"stdout","text":"[INFO] test_raw lines -> 3,000 pairs.\n[INFO] test: after clean+filter -> 2,946 pairs\n[INFO] test: after dedup       -> 2,943 pairs\n[INFO] public_test after clean: 2,943 pairs\n[SAMPLE TEST] ('Knowledge, practices in public health service utilization among health insurance card’s holders and influencing factors in Vientiane, Lao', 'Thực trạng kiến thức và thực hành của người có thẻ bảo hiểm y tế trong sử dụng dịch vụ khám chữa bệnh ở các cơ sở y tế công và một số yếu tố ảnh hưởng tại tỉnh Viêng Chăn, CHDCND Lào, năm 2017')\n[OK] ds['test'] size: 2943\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import os, json, time\nimport torch\nimport sacrebleu\nfrom tqdm.auto import tqdm\n\nassert \"ds\" in globals() and \"test\" in ds, \"Missing ds['test'] - run TEST-LOAD cell first.\"\nassert \"translate_en2vi\" in globals(), \"Missing translate_en2vi() - run inference cell first.\"\nassert \"OUTPUT_DIR\" in globals(), \"Missing OUTPUT_DIR (e.g., 'mbart50_envi').\"\nassert \"trainer\" in globals(), \"Missing trainer - run training cell first (to push easily).\"\n\ndef bleu_on_public_test_and_push(batch_size=16):\n    srcs = ds[\"test\"][\"src_text\"]\n    refs = ds[\"test\"][\"tgt_text\"]\n\n    bs = batch_size if torch.cuda.is_available() else max(2, batch_size // 4)\n\n    hyps = []\n    t0 = time.time()\n\n    for i in tqdm(range(0, len(srcs), bs), desc=\"BLEU on public_test\", total=(len(srcs) + bs - 1)//bs):\n        batch = srcs[i:i+bs]\n        hyps.extend(\n            translate_en2vi(\n                batch,\n                num_beams=5,\n                length_penalty=1.0,\n                no_repeat_ngram_size=3,\n                max_new_tokens=128\n            )\n        )\n\n    bleu = sacrebleu.corpus_bleu(hyps, [refs]).score\n    elapsed = time.time() - t0\n\n    metrics = {\n        \"public_test_sacrebleu\": float(bleu),\n        \"public_test_size\": int(len(srcs)),\n        \"batch_size\": int(bs),\n        \"elapsed_sec\": float(elapsed),\n    }\n\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n    json_path = os.path.join(OUTPUT_DIR, \"public_test_metrics.json\")\n    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(metrics, f, ensure_ascii=False, indent=2)\n\n    txt_path = os.path.join(OUTPUT_DIR, \"public_test_bleu.txt\")\n    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(f\"public_test_sacrebleu\\t{bleu:.4f}\\n\")\n\n    print(f\"\\n[PUBLIC_TEST] sacreBLEU ({len(srcs)} samples): {bleu:.2f}\")\n    print(\"[OK] Saved:\", json_path)\n    print(\"[OK] Saved:\", txt_path)\n\n    # push lên repo model\n    trainer.push_to_hub(commit_message=f\"Add public_test sacreBLEU={bleu:.2f}\")\n    print(\"[OK] Pushed public_test BLEU files to Hugging Face.\")\n\n    return bleu, metrics\n\n_ = bleu_on_public_test_and_push(batch_size=16)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T21:54:17.521272Z","iopub.execute_input":"2025-12-14T21:54:17.521519Z","iopub.status.idle":"2025-12-14T22:13:58.014987Z","shell.execute_reply.started":"2025-12-14T21:54:17.521502Z","shell.execute_reply":"2025-12-14T22:13:58.014344Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"BLEU on public_test:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e0418e059664ae48d3569246017b9ba"}},"metadata":{}},{"name":"stdout","text":"\n[PUBLIC_TEST] sacreBLEU (2943 samples): 41.51\n[OK] Saved: mbart50_envi/public_test_metrics.json\n[OK] Saved: mbart50_envi/public_test_bleu.txt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a133b21813441459c70fb33334f989a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"548dadfaf90d4a228b7f8f40a8767e97"}},"metadata":{}},{"name":"stdout","text":"[OK] Pushed public_test BLEU files to Hugging Face.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"Hiển thị điểm bleu được lấy từ hugging face","metadata":{}},{"cell_type":"code","source":"import os, json\n\njson_path = os.path.join(OUTPUT_DIR, \"public_test_metrics.json\")\n\nwith open(json_path, \"r\", encoding=\"utf-8\") as f:\n    metrics = json.load(f)\n\nbleu = metrics[\"public_test_sacrebleu\"]\nn = metrics[\"public_test_size\"]\n\nprint(\"=\"*60)\nprint(f\"✅ sacreBLEU on public_test ({n} sentences): {bleu:.2f}\")\nprint(\"=\"*60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T22:13:58.015630Z","iopub.execute_input":"2025-12-14T22:13:58.015854Z","iopub.status.idle":"2025-12-14T22:13:58.021502Z","shell.execute_reply.started":"2025-12-14T22:13:58.015838Z","shell.execute_reply":"2025-12-14T22:13:58.020861Z"}},"outputs":[{"name":"stdout","text":"============================================================\n✅ sacreBLEU on public_test (2943 sentences): 41.51\n============================================================\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"Print 5 dòng test ","metadata":{}},{"cell_type":"code","source":"# kiểm tra điều kiện\nassert \"ds\" in globals() and \"test\" in ds, \"Missing ds['test']\"\nassert \"translate_en2vi\" in globals(), \"Missing translate_en2vi()\"\n\nN_SHOW = 5  # số dòng muốn in\n\nsrcs = ds[\"test\"][\"src_text\"][:N_SHOW]\ntgts = ds[\"test\"][\"tgt_text\"][:N_SHOW]\n\n# dịch\npreds = translate_en2vi(\n    srcs,\n    num_beams=5,\n    length_penalty=1.0,\n    no_repeat_ngram_size=3,\n    max_new_tokens=128,\n)\n\nprint(\"=\" * 100)\nfor i, (src, tgt, pred) in enumerate(zip(srcs, tgts, preds), start=1):\n    print(f\"[{i}] SRC (EN): {src}\")\n    print(f\"    TGT (VI): {tgt}\")\n    print(f\"    PRED(VI): {pred}\")\n    print(\"-\" * 100)\nprint(\"=\" * 100)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T22:13:58.024490Z","iopub.execute_input":"2025-12-14T22:13:58.024703Z","iopub.status.idle":"2025-12-14T22:13:59.314514Z","shell.execute_reply.started":"2025-12-14T22:13:58.024688Z","shell.execute_reply":"2025-12-14T22:13:59.313906Z"}},"outputs":[{"name":"stdout","text":"====================================================================================================\n[1] SRC (EN): Knowledge, practices in public health service utilization among health insurance card’s holders and influencing factors in Vientiane, Lao\n    TGT (VI): Thực trạng kiến thức và thực hành của người có thẻ bảo hiểm y tế trong sử dụng dịch vụ khám chữa bệnh ở các cơ sở y tế công và một số yếu tố ảnh hưởng tại tỉnh Viêng Chăn, CHDCND Lào, năm 2017\n    PRED(VI): Kiến thức, thực hành sử dụng dịch vụ y tế công cộng của người chủ thẻ BHYT và một số yếu tố ảnh hưởng ở Vientiane, Lao\n----------------------------------------------------------------------------------------------------\n[2] SRC (EN): Describe knowledge, practices in public health service utilization among health insurance card's holders and influencing factors in Vientiane, Lao PDR, 2017.\n    TGT (VI): Mô tả thực trạng kiến thức, thực hành của người có thẻ bảo hiểm y tế trong sử dụng dịch vụ khám chữa bệnh ở các cơ sở y tế công và một số yếu tố liên quan tại tỉnh Viêng Chăn, Cộng hoà Dân chủ Nhân dân Lào năm 2017.\n    PRED(VI): Mô tả kiến thức, thực hành sử dụng dịch vụ y tế công cộng của người chủ thẻ BHYT và một số yếu tố ảnh hưởng ở Vientiane, Lao PDR, 2017.\n----------------------------------------------------------------------------------------------------\n[3] SRC (EN): Methodology: A cross sectional study was used among 928 adult health insurance card's holders in Phone Hong and Keo Oudom districts, Vientiane province.\n    TGT (VI): Phương pháp: Thiết kế nghiên mô tả cắt ngang được thực hiện trên 928 người trưởng thành có thẻ bảo hiểm y tế tại 2 huyện Phone Hong và Keo Oudom, tỉnh Viêng Chăn.\n    PRED(VI): Đối tượng và phương pháp nghiên cứu: Nghiên cứu mô tả cắt ngang trên 928 người chủ thẻ bảo hiểm y tế người lớn tại huyện Tông Hẫu và huyện Kèo Vương, tỉnh Vientiane.\n----------------------------------------------------------------------------------------------------\n[4] SRC (EN): Results: Percentage of card's holders who knew the finance-free utilization of the first registered public health services was 44.5% and being provided health insurance information was 34.8%.\n    TGT (VI): Kết quả: Tỷ lệ người biết được khám chữa bệnh (KCB) miễn phí tại nơi đăng ký ban đầu chiếm 44,5%, được cung cấp thông tin về bảo hiểm y tế (BHYT) chiếm 34,8%.\n    PRED(VI): Kết quả: Tỷ lệ người sở hữu thẻ biết sử dụng dịch vụ y tế công cộng đăng ký đầu tiên miễn phí là 44,5% và được cung cấp thông tin bảo hiểm là 34,8%.\n----------------------------------------------------------------------------------------------------\n[5] SRC (EN): Percentage of card's holders who went to the first registered public health services was 61.8%.\n    TGT (VI): Tỷ lệ người có thẻ BHYT thực hành khám chữa bệnh đúng nơi đăng ký KCB ban đầu chiếm 61,8%.\n    PRED(VI): Tỷ lệ người có thẻ đi khám bệnh tại trạm y tế công lập đầu tiên là 61,8%.\n----------------------------------------------------------------------------------------------------\n====================================================================================================\n","output_type":"stream"}],"execution_count":22}]}