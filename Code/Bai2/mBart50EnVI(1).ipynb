{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CELL 0 — Thiết lập biến môi trường\n",
    "\n",
    "## Mục đích\n",
    "Cell này thiết lập các biến môi trường **TRƯỚC KHI** import các thư viện để tránh các lỗi xung đột.\n",
    "\n",
    "## Giải thích chi tiết các biến:\n",
    "\n",
    "| Biến môi trường | Giá trị | Mục đích |\n",
    "|-----------------|---------|----------|\n",
    "| `TRANSFORMERS_NO_TF` | \"1\" | Chặn Transformers tự động import TensorFlow |\n",
    "| `TRANSFORMERS_NO_FLAX` | \"1\" | Chặn Transformers tự động import Flax |\n",
    "| `TF_CPP_MIN_LOG_LEVEL` | \"3\" | Tắt các log warning của TensorFlow C++ backend |\n",
    "| `TOKENIZERS_PARALLELISM` | \"false\" | Tắt tokenization song song để tránh deadlock |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:54:27.113595Z",
     "iopub.status.busy": "2025-12-14T18:54:27.113445Z",
     "iopub.status.idle": "2025-12-14T18:54:27.120784Z",
     "shell.execute_reply": "2025-12-14T18:54:27.120139Z",
     "shell.execute_reply.started": "2025-12-14T18:54:27.113581Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Environment flags set (no TF / no Flax).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Chặn Transformers tự động import TensorFlow/Flax (tránh lỗi protobuf / MessageFactory)\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "print(\"[OK] Environment flags set (no TF / no Flax).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CELL 1 — Kiểm tra môi trường và GPU\n",
    "\n",
    "## Mục đích\n",
    "Kiểm tra và hiển thị thông tin về môi trường runtime để đảm bảo:\n",
    "- Phiên bản Python và các thư viện tương thích\n",
    "- GPU có sẵn để tăng tốc training\n",
    "\n",
    "## Các thông tin được kiểm tra:\n",
    "\n",
    "| Thông tin | Mô tả |\n",
    "|-----------|-------|\n",
    "| `sys.version` | Phiên bản Python đang chạy |\n",
    "| `torch.__version__` | Phiên bản PyTorch |\n",
    "| `torch.cuda.is_available()` | Kiểm tra CUDA có sẵn không |\n",
    "| `torch.cuda.get_device_name(0)` | Tên GPU (nếu có) |\n",
    "| `transformers.__version__` | Phiên bản Hugging Face Transformers |\n",
    "| `datasets.__version__` | Phiên bản Hugging Face Datasets |\n",
    "| `accelerate.__version__` | Phiên bản Accelerate (hỗ trợ distributed training) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:54:27.122791Z",
     "iopub.status.busy": "2025-12-14T18:54:27.122557Z",
     "iopub.status.idle": "2025-12-14T18:54:39.122858Z",
     "shell.execute_reply": "2025-12-14T18:54:39.122188Z",
     "shell.execute_reply.started": "2025-12-14T18:54:27.122773Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.13\n",
      "Torch: 2.6.0+cu124\n",
      "CUDA: True\n",
      "GPU: Tesla T4\n",
      "Transformers: 4.53.3\n",
      "Datasets: 4.4.1\n",
      "Accelerate: 1.9.0\n"
     ]
    }
   ],
   "source": [
    "import sys, torch\n",
    "import numpy as np\n",
    "\n",
    "import transformers, datasets, accelerate\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"Datasets:\", datasets.__version__)\n",
    "print(\"Accelerate:\", accelerate.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CELL 2 — Cài đặt các package bổ sung\n",
    "\n",
    "## Mục đích\n",
    "Cài đặt các package cần thiết nếu chưa có trong môi trường.\n",
    "\n",
    "## Các package được cài:\n",
    "\n",
    "| Package | Mục đích | Bắt buộc? |\n",
    "|---------|----------|-----------|\n",
    "| `sacrebleu` | Tính điểm BLEU chuẩn cho đánh giá dịch máy | ✅ Bắt buộc |\n",
    "| `hf_transfer` | Tăng tốc download/upload model từ Hugging Face Hub | ⚪ Tùy chọn |\n",
    "\n",
    "## Về sacrebleu\n",
    "- **SacreBLEU** là công cụ chuẩn để đánh giá chất lượng dịch máy\n",
    "- Tính điểm BLEU (Bilingual Evaluation Understudy) từ 0-100\n",
    "- Điểm càng cao = bản dịch càng giống với reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:54:39.124125Z",
     "iopub.status.busy": "2025-12-14T18:54:39.123680Z",
     "iopub.status.idle": "2025-12-14T18:54:44.603187Z",
     "shell.execute_reply": "2025-12-14T18:54:44.602389Z",
     "shell.execute_reply.started": "2025-12-14T18:54:39.124094Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Installing sacrebleu ...\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 51.8/51.8 kB 1.6 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 104.1/104.1 kB 3.5 MB/s eta 0:00:00\n",
      "[OK] hf_transfer already installed\n"
     ]
    }
   ],
   "source": [
    "import importlib.util, subprocess\n",
    "\n",
    "def pip_install(pkg: str):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "# sacrebleu: bắt buộc\n",
    "if importlib.util.find_spec(\"sacrebleu\") is None:\n",
    "    print(\"[INFO] Installing sacrebleu ...\")\n",
    "    pip_install(\"sacrebleu\")\n",
    "else:\n",
    "    print(\"[OK] sacrebleu already installed\")\n",
    "\n",
    "# hf_transfer: optional tăng tốc push/pull\n",
    "if importlib.util.find_spec(\"hf_transfer\") is None:\n",
    "    print(\"[INFO] Installing hf_transfer (optional) ...\")\n",
    "    pip_install(\"hf_transfer\")\n",
    "else:\n",
    "    print(\"[OK] hf_transfer already installed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CELL 3 — Import các thư viện chính\n",
    "\n",
    "## Mục đích\n",
    "Import tất cả các thư viện cần thiết cho pipeline dịch máy.\n",
    "\n",
    "## Các thư viện được import:\n",
    "\n",
    "### Thư viện chuẩn Python\n",
    "\n",
    "\n",
    "### Hugging Face Libraries \n",
    "Mục đích: nơi lưu trữ trạng thái và thông số sau khi train model\n",
    "\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "| `sacrebleu` | Tính điểm BLEU chuẩn |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:54:44.604480Z",
     "iopub.status.busy": "2025-12-14T18:54:44.604008Z",
     "iopub.status.idle": "2025-12-14T18:55:24.595311Z",
     "shell.execute_reply": "2025-12-14T18:55:24.594674Z",
     "shell.execute_reply.started": "2025-12-14T18:54:44.604451Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765738492.522072      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765738492.625769      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Imports done.\n"
     ]
    }
   ],
   "source": [
    "import re, math, random, hashlib\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "import sacrebleu\n",
    "\n",
    "print(\"[OK] Imports done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CELL 4 — Tự động tìm thư mục dataset\n",
    "\n",
    "## Mục đích\n",
    "Tự động dò tìm thư mục chứa dataset trên Kaggle mà không cần hardcode đường dẫn.\n",
    "\n",
    "## Các file dataset cần có:\n",
    "\n",
    "| File | Nội dung |\n",
    "|------|----------|\n",
    "| `train.en.txt` | Câu tiếng Anh để training |\n",
    "| `train.vi.txt` | Câu tiếng Việt tương ứng (parallel corpus) |\n",
    "| `public_test.en.txt` | Câu tiếng Anh để test |\n",
    "| `public_test.vi.txt` | Câu tiếng Việt reference để tính BLEU |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:55:24.596426Z",
     "iopub.status.busy": "2025-12-14T18:55:24.595928Z",
     "iopub.status.idle": "2025-12-14T18:55:24.615137Z",
     "shell.execute_reply": "2025-12-14T18:55:24.614580Z",
     "shell.execute_reply.started": "2025-12-14T18:55:24.596405Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Found dataset dir: /kaggle/input/databaitoanphu\n",
      "[CHECK] train.en.txt -> OK\n",
      "[CHECK] train.vi.txt -> OK\n",
      "[CHECK] public_test.en.txt -> OK\n",
      "[CHECK] public_test.vi.txt -> OK\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def find_dataset_dir() -> Path:\n",
    "    base = Path(\"/kaggle/input\")\n",
    "    for d in base.glob(\"*\"):\n",
    "        if d.is_dir() and (d / \"train.en.txt\").exists() and (d / \"train.vi.txt\").exists():\n",
    "            return d\n",
    "    for d in base.glob(\"*/*\"):\n",
    "        if d.is_dir() and (d / \"train.en.txt\").exists() and (d / \"train.vi.txt\").exists():\n",
    "            return d\n",
    "    raise FileNotFoundError(\"Could not find dataset folder containing train.en.txt/train.vi.txt\")\n",
    "\n",
    "DATA_DIR = find_dataset_dir()\n",
    "print(\"[OK] Found dataset dir:\", DATA_DIR)\n",
    "\n",
    "TRAIN_EN = DATA_DIR / \"train.en.txt\"\n",
    "TRAIN_VI = DATA_DIR / \"train.vi.txt\"\n",
    "TEST_EN  = DATA_DIR / \"public_test.en.txt\"\n",
    "TEST_VI  = DATA_DIR / \"public_test.vi.txt\"\n",
    "\n",
    "for p in [TRAIN_EN, TRAIN_VI, TEST_EN, TEST_VI]:\n",
    "    print(\"[CHECK]\", p.name, \"->\", \"OK\" if p.exists() else \"NOT FOUND\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CELL 5 — Đọc file song song EN/VI\n",
    "\n",
    "## Mục đích\n",
    "Đọc và ghép cặp các câu tiếng Anh - tiếng Việt từ file text.\n",
    "\n",
    "## Các hàm được định nghĩa:\n",
    "\n",
    "### `read_lines(path, encoding=\"utf-8\")`\n",
    "\n",
    "### `load_parallel(en_path, vi_path, name=\"data\")`\n",
    "\n",
    "## Parallel Corpus là gì?\n",
    "- Là tập dữ liệu gồm các cặp câu song ngữ (EN-VI) được align theo từng dòng\n",
    "- Dòng 1 file EN ↔ Dòng 1 file VI\n",
    "- Dòng 2 file EN ↔ Dòng 2 file VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:55:24.615857Z",
     "iopub.status.busy": "2025-12-14T18:55:24.615654Z",
     "iopub.status.idle": "2025-12-14T18:55:27.370139Z",
     "shell.execute_reply": "2025-12-14T18:55:27.369542Z",
     "shell.execute_reply.started": "2025-12-14T18:55:24.615841Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] train_raw lines -> 500,000 pairs.\n",
      "[INFO] test_raw lines -> 3,000 pairs.\n",
      "[SAMPLE train_raw] ('To evaluate clinical, subclinical symptoms of patients with otitis media with effusion and V.a at otorhinolaryngology department – Thai Nguyen national hospital', 'Nghiên cứu đặc điểm lâm sàng, cận lâm sàng bệnh nhân viêm tai ứ dịch trên viêm V.A tại Khoa Tai mũi họng - Bệnh viện Trung ương Thái Nguyên')\n",
      "[SAMPLE test_raw ] ('Knowledge, practices in public health service utilization among health insurance card’s holders and influencing factors in Vientiane, Lao', 'Thực trạng kiến thức và thực hành của người có thẻ bảo hiểm y tế trong sử dụng dịch vụ khám chữa bệnh ở các cơ sở y tế công và một số yếu tố ảnh hưởng tại tỉnh Viêng Chăn, CHDCND Lào, năm 2017')\n"
     ]
    }
   ],
   "source": [
    "def read_lines(path: Path, encoding=\"utf-8\"):\n",
    "    with open(path, \"r\", encoding=encoding, errors=\"replace\") as f:\n",
    "        return [line.rstrip(\"\\n\") for line in f]\n",
    "\n",
    "def load_parallel(en_path: Path, vi_path: Path, name=\"data\"):\n",
    "    en_lines = read_lines(en_path)\n",
    "    vi_lines = read_lines(vi_path)\n",
    "    n = min(len(en_lines), len(vi_lines))\n",
    "    if len(en_lines) != len(vi_lines):\n",
    "        print(f\"[WARN] {name} mismatch -> EN={len(en_lines):,}, VI={len(vi_lines):,}. Using first {n:,} pairs.\")\n",
    "    else:\n",
    "        print(f\"[INFO] {name} lines -> {n:,} pairs.\")\n",
    "    return list(zip(en_lines[:n], vi_lines[:n]))\n",
    "\n",
    "train_pairs_raw = load_parallel(TRAIN_EN, TRAIN_VI, \"train_raw\")\n",
    "test_pairs_raw  = load_parallel(TEST_EN,  TEST_VI,  \"test_raw\")\n",
    "\n",
    "print(\"[SAMPLE train_raw]\", train_pairs_raw[0])\n",
    "print(\"[SAMPLE test_raw ]\", test_pairs_raw[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  CELL 6 — Làm sạch, Lọc và Loại bỏ trùng lặp\n",
    "\n",
    "## Mục đích\n",
    "Tiền xử lý dữ liệu để đảm bảo chất lượng training data.\n",
    "\n",
    "## Ba bước xử lý:\n",
    "\n",
    "### 1. Clean (Làm sạch) - Hàm `basic_clean(s)`\n",
    "```python\n",
    "s.strip()                    # Xóa whitespace đầu/cuối\n",
    "re.sub(r\"\\s+\", \" \", s)       # Chuẩn hóa nhiều space thành 1 space\n",
    "```\n",
    "\n",
    "### 2. Filter (Lọc) - Hàm `is_good_pair(src, tgt)`\n",
    "| Điều kiện | Giá trị | Mục đích |\n",
    "|-----------|---------|----------|\n",
    "| `MIN_CHARS` | 2 | Loại câu quá ngắn (noise) |\n",
    "| `MAX_CHARS` | 400 | Loại câu quá dài (tránh truncate quá nhiều) |\n",
    "| `not empty` | - | Loại cặp có câu rỗng |\n",
    "\n",
    "### 3. Dedup (Loại bỏ trùng lặp)\n",
    "- Tạo **MD5 hash** từ `src + \"\\t\" + tgt`\n",
    "- Dùng `set()` để track các hash đã thấy\n",
    "- Chỉ giữ cặp có hash chưa xuất hiện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:55:27.372638Z",
     "iopub.status.busy": "2025-12-14T18:55:27.372435Z",
     "iopub.status.idle": "2025-12-14T18:55:38.588753Z",
     "shell.execute_reply": "2025-12-14T18:55:38.587997Z",
     "shell.execute_reply.started": "2025-12-14T18:55:27.372622Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] train: after clean+filter -> 489,008 pairs\n",
      "[INFO] train: after dedup       -> 340,522 pairs\n",
      "[INFO] test: after clean+filter -> 2,946 pairs\n",
      "[INFO] test: after dedup       -> 2,943 pairs\n",
      "[SAMPLE clean train] ('To evaluate clinical, subclinical symptoms of patients with otitis media with effusion and V.a at otorhinolaryngology department – Thai Nguyen national hospital', 'Nghiên cứu đặc điểm lâm sàng, cận lâm sàng bệnh nhân viêm tai ứ dịch trên viêm V.A tại Khoa Tai mũi họng - Bệnh viện Trung ương Thái Nguyên')\n",
      "[SAMPLE clean test ] ('Knowledge, practices in public health service utilization among health insurance card’s holders and influencing factors in Vientiane, Lao', 'Thực trạng kiến thức và thực hành của người có thẻ bảo hiểm y tế trong sử dụng dịch vụ khám chữa bệnh ở các cơ sở y tế công và một số yếu tố ảnh hưởng tại tỉnh Viêng Chăn, CHDCND Lào, năm 2017')\n"
     ]
    }
   ],
   "source": [
    "import re, hashlib\n",
    "\n",
    "def basic_clean(s: str) -> str:\n",
    "    s = \"\" if s is None else s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "MIN_CHARS = 2\n",
    "MAX_CHARS = 400\n",
    "\n",
    "def is_good_pair(src: str, tgt: str) -> bool:\n",
    "    if not src or not tgt:\n",
    "        return False\n",
    "    if len(src) < MIN_CHARS or len(tgt) < MIN_CHARS:\n",
    "        return False\n",
    "    if len(src) > MAX_CHARS or len(tgt) > MAX_CHARS:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def clean_filter_dedup(pairs, name=\"data\"):\n",
    "    # clean + filter\n",
    "    cleaned = []\n",
    "    for src, tgt in pairs:\n",
    "        src = basic_clean(src)\n",
    "        tgt = basic_clean(tgt)\n",
    "        if is_good_pair(src, tgt):\n",
    "            cleaned.append((src, tgt))\n",
    "    print(f\"[INFO] {name}: after clean+filter -> {len(cleaned):,} pairs\")\n",
    "\n",
    "    # dedup\n",
    "    seen = set()\n",
    "    dedup = []\n",
    "    for src, tgt in cleaned:\n",
    "        h = hashlib.md5((src + \"\\t\" + tgt).encode(\"utf-8\")).hexdigest()\n",
    "        if h not in seen:\n",
    "            seen.add(h)\n",
    "            dedup.append((src, tgt))\n",
    "    print(f\"[INFO] {name}: after dedup       -> {len(dedup):,} pairs\")\n",
    "    return dedup\n",
    "\n",
    "train_pairs = clean_filter_dedup(train_pairs_raw, \"train\")\n",
    "test_pairs  = clean_filter_dedup(test_pairs_raw,  \"test\")\n",
    "\n",
    "print(\"[SAMPLE clean train]\", train_pairs[0])\n",
    "print(\"[SAMPLE clean test ]\", test_pairs[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CELL 7 — Sampling và Chia Train/Validation/Test\n",
    "\n",
    "## Mục đích\n",
    "Tạo dataset với kích thước cố định và chia thành các tập train/validation/test.\n",
    "\n",
    "## Các tham số:\n",
    "\n",
    "| Tham số | Giá trị | Mô tả |\n",
    "|---------|---------|-------|\n",
    "| `SEED` | 42 | Seed cho reproducibility (kết quả giống nhau mỗi lần chạy) |\n",
    "| `N_TOTAL` | 20,000 | Số cặp câu lấy từ train_pairs |\n",
    "| `TRAIN_RATIO` | 0.9 | 90% cho train, 10% cho validation |\n",
    "\n",
    "| train | 18,000 | 90% từ subset |\n",
    "| validation | 2,000 | 10% từ subset |\n",
    "| test | 2,943 | public_test (giữ nguyên) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:55:38.589902Z",
     "iopub.status.busy": "2025-12-14T18:55:38.589594Z",
     "iopub.status.idle": "2025-12-14T18:55:38.832906Z",
     "shell.execute_reply": "2025-12-14T18:55:38.832146Z",
     "shell.execute_reply.started": "2025-12-14T18:55:38.589876Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Train pairs available: 340,522\n",
      "[INFO] Taking from train     : 20,000 pairs\n",
      "[INFO] Split -> train=18,000 | val=2,000\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['src_text', 'tgt_text'],\n",
      "        num_rows: 18000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['src_text', 'tgt_text'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['src_text', 'tgt_text'],\n",
      "        num_rows: 2943\n",
      "    })\n",
      "})\n",
      "[SAMPLE TRAIN] {'src_text': 'Monitor and control ICP using sedatives, endotracheal intubation, hyperventilation, hydration, diuretics, measures to control blood pressure, and sometimes corticosteroids.', 'tgt_text': 'Theo dõi và kiểm soát ICP bằng cách sử dụng thuốc an thần, đặt nội khí quản, tăng thông khí, hydrat hoá, thuốc lợi tiểu, các biện pháp để kiểm soát huyết áp, và đôi khi là corticosteroid.'}\n",
      "[SAMPLE VAL  ] {'src_text': 'The menstrual cycle is one of the most common acne triggers.', 'tgt_text': 'Chu kỳ kinh nguyệt là một trong những nguyên nhân thường gặp nhất gây ra mụn trứng cá.'}\n",
      "[SAMPLE TEST ] {'src_text': 'Knowledge, practices in public health service utilization among health insurance card’s holders and influencing factors in Vientiane, Lao', 'tgt_text': 'Thực trạng kiến thức và thực hành của người có thẻ bảo hiểm y tế trong sử dụng dịch vụ khám chữa bệnh ở các cơ sở y tế công và một số yếu tố ảnh hưởng tại tỉnh Viêng Chăn, CHDCND Lào, năm 2017'}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "SEED = 42\n",
    "N_TOTAL = 20000\n",
    "\n",
    "TRAIN_RATIO = 0.9  # 90/10\n",
    "\n",
    "# 1) shuffle train_pairs\n",
    "rng = random.Random(SEED)\n",
    "rng.shuffle(train_pairs)\n",
    "\n",
    "# 2) lấy 5000 (nếu không đủ thì lấy hết)\n",
    "n_take = min(N_TOTAL, len(train_pairs))\n",
    "subset = train_pairs[:n_take]\n",
    "print(f\"[INFO] Train pairs available: {len(train_pairs):,}\")\n",
    "print(f\"[INFO] Taking from train     : {n_take:,} pairs\")\n",
    "\n",
    "# 3) split 90/10\n",
    "n_train = int(n_take * TRAIN_RATIO)\n",
    "n_val = n_take - n_train\n",
    "\n",
    "train_subset = subset[:n_train]\n",
    "val_subset   = subset[n_train:]\n",
    "\n",
    "print(f\"[INFO] Split -> train={len(train_subset):,} | val={len(val_subset):,}\")\n",
    "\n",
    "# 4) build datasets\n",
    "train_ds = Dataset.from_dict({\n",
    "    \"src_text\": [s for s, _ in train_subset],\n",
    "    \"tgt_text\": [t for _, t in train_subset],\n",
    "})\n",
    "val_ds = Dataset.from_dict({\n",
    "    \"src_text\": [s for s, _ in val_subset],\n",
    "    \"tgt_text\": [t for _, t in val_subset],\n",
    "})\n",
    "\n",
    "# test giữ nguyên từ public_test\n",
    "test_ds = Dataset.from_dict({\n",
    "    \"src_text\": [s for s, _ in test_pairs],\n",
    "    \"tgt_text\": [t for _, t in test_pairs],\n",
    "})\n",
    "\n",
    "ds = DatasetDict({\"train\": train_ds, \"validation\": val_ds, \"test\": test_ds})\n",
    "print(ds)\n",
    "\n",
    "print(\"[SAMPLE TRAIN]\", ds[\"train\"][0])\n",
    "print(\"[SAMPLE VAL  ]\", ds[\"validation\"][0])\n",
    "print(\"[SAMPLE TEST ]\", ds[\"test\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CELL 8a — Load Tokenizer và Model mBART-50\n",
    "\n",
    "## Mục đích\n",
    "Load model mBART-50 pre-trained từ Hugging Face Hub để fine-tune cho task dịch EN→VI.\n",
    "\n",
    "## Về mBART-50\n",
    "\n",
    "### Model ID\n",
    "```python\n",
    "MODEL_ID = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "```\n",
    "\n",
    "### Đặc điểm:\n",
    "| Thuộc tính | Giá trị |\n",
    "|------------|---------|\n",
    "| Kiến trúc | Transformer Encoder-Decoder |\n",
    "| Số ngôn ngữ | 50 ngôn ngữ |\n",
    "| Số parameters | ~611M |\n",
    "| Pre-training | Denoising auto-encoding trên 50 ngôn ngữ |\n",
    "| Fine-tuning | Many-to-many machine translation |\n",
    "\n",
    "## Cấu hình ngôn ngữ\n",
    "\n",
    "| Biến | Giá trị | Ý nghĩa |\n",
    "|------|---------|---------|\n",
    "| `SRC_LANG` | \"en_XX\" | Mã ngôn ngữ nguồn (English) |\n",
    "| `TGT_LANG` | \"vi_VN\" | Mã ngôn ngữ đích (Vietnamese) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:55:38.833841Z",
     "iopub.status.busy": "2025-12-14T18:55:38.833628Z",
     "iopub.status.idle": "2025-12-14T18:55:48.852651Z",
     "shell.execute_reply": "2025-12-14T18:55:48.851955Z",
     "shell.execute_reply.started": "2025-12-14T18:55:38.833817Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84aaf387c5fd4e158df374d0aab00374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "088c85ad21bc400eb7b64a1ba5c8fa63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5bf3742c664a0daf95f665ba694195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28ffb9df2b045ada5d4779420505494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3aaea09894d428d98e8538020a0278b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094eb266358d490b98de1efd2d156ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] tokenizer/model loaded\n",
      "[INFO] tokenizer.src_lang: en_XX\n",
      "[INFO] tokenizer.tgt_lang: vi_VN\n",
      "[INFO] forced_bos_token_id: 250024\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "MODEL_ID = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "SRC_LANG = \"en_XX\"\n",
    "TGT_LANG = \"vi_VN\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=False)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_ID)\n",
    "\n",
    "# set src/tgt language (QUAN TRỌNG)\n",
    "tokenizer.src_lang = SRC_LANG\n",
    "tokenizer.tgt_lang = TGT_LANG   # <-- FIX KeyError: None\n",
    "\n",
    "forced_bos_token_id = tokenizer.lang_code_to_id[TGT_LANG]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"[OK] tokenizer/model loaded\")\n",
    "print(\"[INFO] tokenizer.src_lang:\", tokenizer.src_lang)\n",
    "print(\"[INFO] tokenizer.tgt_lang:\", tokenizer.tgt_lang)\n",
    "print(\"[INFO] forced_bos_token_id:\", forced_bos_token_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CELL 8 — Tokenize Dataset\n",
    "\n",
    "## Mục đích\n",
    "Chuyển đổi text thô thành token IDs để model có thể xử lý.\n",
    "\n",
    "## Tham số Tokenization:\n",
    "\n",
    "| Tham số | Giá trị | Mô tả |\n",
    "|---------|---------|-------|\n",
    "| `MAX_SRC_LEN` | 256 | Độ dài tối đa source (EN) |\n",
    "| `MAX_TGT_LEN` | 256 | Độ dài tối đa target (VI) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:55:48.853842Z",
     "iopub.status.busy": "2025-12-14T18:55:48.853500Z",
     "iopub.status.idle": "2025-12-14T18:55:58.149407Z",
     "shell.execute_reply": "2025-12-14T18:55:58.148836Z",
     "shell.execute_reply.started": "2025-12-14T18:55:48.853824Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb68a31a0ef4cca9a1167bdfa7243c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3951: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c208905b404ed3ba526b730aa3e171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7dd1e9525ae4c76aef7dc43f29e299a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2943 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Tokenized sizes: train 18000 | val 2000 | test 2943\n"
     ]
    }
   ],
   "source": [
    "# đảm bảo ds đã có ở cell split 5000 (train/validation/test)\n",
    "assert \"ds\" in globals(), \"You must run the data-loading/splitting cell to create `ds` first!\"\n",
    "assert \"tokenizer\" in globals(), \"You must run the tokenizer/model loading cell first!\"\n",
    "\n",
    "MAX_SRC_LEN = 256\n",
    "MAX_TGT_LEN = 256\n",
    "\n",
    "def preprocess_batch(batch):\n",
    "    tokenizer.src_lang = SRC_LANG\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        batch[\"src_text\"],\n",
    "        max_length=MAX_SRC_LEN,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch[\"tgt_text\"],\n",
    "            max_length=MAX_TGT_LEN,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_tok = ds[\"train\"].map(preprocess_batch, batched=True, remove_columns=ds[\"train\"].column_names)\n",
    "val_tok   = ds[\"validation\"].map(preprocess_batch, batched=True, remove_columns=ds[\"validation\"].column_names)\n",
    "test_tok  = ds[\"test\"].map(preprocess_batch, batched=True, remove_columns=ds[\"test\"].column_names)\n",
    "\n",
    "print(\"[OK] Tokenized sizes:\",\n",
    "      \"train\", len(train_tok),\n",
    "      \"| val\", len(val_tok),\n",
    "      \"| test\", len(test_tok))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CELL 9 — Đánh giá BLEU trước khi Training (Baseline)\n",
    "\n",
    "## Mục đích\n",
    "Đo điểm BLEU của model **TRƯỚC** fine-tuning để có baseline so sánh.\n",
    "\n",
    "## Hàm `translate_en2vi(texts, ...)`\n",
    "\n",
    "### Tham số Generation:\n",
    "| Tham số | Giá trị | Mô tả |\n",
    "|---------|---------|-------|\n",
    "| `num_beams` | 5 | Beam search với 5 beams |\n",
    "| `length_penalty` | 1.0 | Không penalty độ dài (1.0 = neutral) |\n",
    "| `no_repeat_ngram_size` | 3 | Không cho phép lặp trigram |\n",
    "| `max_new_tokens` | 128 | Tối đa 128 tokens output |\n",
    "| `early_stopping` | True | Dừng khi tất cả beams đạt EOS |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:55:58.150452Z",
     "iopub.status.busy": "2025-12-14T18:55:58.150186Z",
     "iopub.status.idle": "2025-12-14T18:57:10.868499Z",
     "shell.execute_reply": "2025-12-14T18:57:10.867742Z",
     "shell.execute_reply.started": "2025-12-14T18:55:58.150435Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] sacreBLEU (200 samples): 26.20\n"
     ]
    }
   ],
   "source": [
    "@torch.inference_mode()\n",
    "def translate_en2vi(texts, num_beams=5, length_penalty=1.0, no_repeat_ngram_size=3, max_new_tokens=128):\n",
    "    model.eval()\n",
    "    tokenizer.src_lang = SRC_LANG\n",
    "    enc = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_SRC_LEN).to(model.device)\n",
    "    out = model.generate(\n",
    "        **enc,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[TGT_LANG],\n",
    "        num_beams=num_beams,\n",
    "        length_penalty=length_penalty,\n",
    "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "\n",
    "def bleu_on_test(max_samples=None):\n",
    "    srcs = ds[\"test\"][\"src_text\"]\n",
    "    refs = ds[\"test\"][\"tgt_text\"]\n",
    "\n",
    "    if max_samples is not None:\n",
    "        srcs = srcs[:max_samples]\n",
    "        refs = refs[:max_samples]\n",
    "\n",
    "    hyps = []\n",
    "    bs = 16 if torch.cuda.is_available() else 4\n",
    "    for i in range(0, len(srcs), bs):\n",
    "        hyps.extend(translate_en2vi(srcs[i:i+bs]))\n",
    "\n",
    "    bleu = sacrebleu.corpus_bleu(hyps, [refs]).score\n",
    "    print(f\"[TEST] sacreBLEU ({len(srcs)} samples): {bleu:.2f}\")\n",
    "    return bleu\n",
    "\n",
    "_ = bleu_on_test(max_samples=200)   # đổi None để chạy full public_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CELL — Đăng nhập Hugging Face Hub\n",
    "\n",
    "## Mục đích\n",
    "Xác thực với Hugging Face để có thể push model lên Hub sau khi training.\n",
    "\n",
    "## Cách lấy Token trên Kaggle\n",
    "\n",
    "### Bước 1: Tạo HF Token\n",
    "1. Vào https://huggingface.co/settings/tokens\n",
    "2. Tạo token với quyền `write`\n",
    "\n",
    "### Bước 2: Lưu vào Kaggle Secrets\n",
    "### Bước 3: Sử dụng trong code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:57:10.869770Z",
     "iopub.status.busy": "2025-12-14T18:57:10.869325Z",
     "iopub.status.idle": "2025-12-14T18:57:11.038322Z",
     "shell.execute_reply": "2025-12-14T18:57:11.037582Z",
     "shell.execute_reply.started": "2025-12-14T18:57:10.869741Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Logged in to Hugging Face.\n"
     ]
    }
   ],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "from huggingface_hub import login\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "# tên secret của anh là HF_TOKEN (đúng như panel)\n",
    "hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "login(token=hf_token)\n",
    "print(\"[OK] Logged in to Hugging Face.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model đã train sẵn từ trước trên hugging face về làm model train tiếp data mới \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:57:11.039303Z",
     "iopub.status.busy": "2025-12-14T18:57:11.039062Z",
     "iopub.status.idle": "2025-12-14T18:57:22.477846Z",
     "shell.execute_reply": "2025-12-14T18:57:22.477182Z",
     "shell.execute_reply.started": "2025-12-14T18:57:11.039288Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7f6458ed0c4930afb70dbc0ab1f408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/11.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91f64d59e26468f8f65e9196ae5deb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bac87ca058541f98984f18ece132989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/992 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b095005d040f441995432fdcb65a43e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588d6c8ceaa64a0bba9ed0eec85f1735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f509bd0b304f68a2eab21fa7976172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/256 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Loaded model from HF: ngothuyet/mbart50-envi\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "HF_REPO = \"ngothuyet/mbart50-envi\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_REPO, use_fast=False)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(HF_REPO)\n",
    "\n",
    "tokenizer.src_lang = \"en_XX\"\n",
    "tokenizer.tgt_lang = \"vi_VN\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"[OK] Loaded model from HF:\", HF_REPO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đọc train, val loss từ model trước đó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:57:22.478886Z",
     "iopub.status.busy": "2025-12-14T18:57:22.478541Z",
     "iopub.status.idle": "2025-12-14T18:57:22.723191Z",
     "shell.execute_reply": "2025-12-14T18:57:22.722409Z",
     "shell.execute_reply.started": "2025-12-14T18:57:22.478866Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f277959b304ec3b064e02671476043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_val_loss.tsv:   0%|          | 0.00/114 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== ALL TRAIN/VAL LOSS (FROM HF) =====\n",
      " epoch  train_loss  val_loss\n",
      "   1.0      1.8957  1.658935\n",
      "   2.0      1.3897  1.586371\n",
      "   3.0      1.1786  1.592958\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "HF_REPO_ID = \"ngothuyet/mbart50-envi\"   # repo của anh\n",
    "FILENAME = \"train_val_loss.tsv\"\n",
    "\n",
    "path = hf_hub_download(repo_id=HF_REPO_ID, filename=FILENAME)\n",
    "df = pd.read_csv(path, sep=\"\\t\")\n",
    "\n",
    "print(\"===== ALL TRAIN/VAL LOSS (FROM HF) =====\")\n",
    "print(df.to_string(index=False))   # in hết bảng\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:57:22.725042Z",
     "iopub.status.busy": "2025-12-14T18:57:22.724330Z",
     "iopub.status.idle": "2025-12-14T21:53:40.686351Z",
     "shell.execute_reply": "2025-12-14T21:53:40.685666Z",
     "shell.execute_reply.started": "2025-12-14T18:57:22.725023Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47/3259613353.py:88: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1689' max='1689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1689/1689 2:52:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.500800</td>\n",
       "      <td>1.360325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.171500</td>\n",
       "      <td>1.301108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.991700</td>\n",
       "      <td>1.295173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAIN DONE =====\n",
      "TrainOutput(global_step=1689, training_loss=1.2213178129856794, metrics={'train_runtime': 10354.0679, 'train_samples_per_second': 5.215, 'train_steps_per_second': 0.163, 'total_flos': 6193749995618304.0, 'train_loss': 1.2213178129856794, 'epoch': 3.0})\n",
      "\n",
      "===== FINAL EVAL (VAL) =====\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 02:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_loss: 1.2951726913452148\n",
      "eval_runtime: 153.5677\n",
      "eval_samples_per_second: 13.024\n",
      "eval_steps_per_second: 3.256\n",
      "epoch: 3.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4deb083b5a99493782a3d41af32f11ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b389fd918ce47878352c431494c0aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved final model/tokenizer to: mbart50_envi\n",
      "[OK] Saved TSV: mbart50_envi/train_val_loss.tsv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5008</td>\n",
       "      <td>1.360325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.1715</td>\n",
       "      <td>1.301108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9917</td>\n",
       "      <td>1.295173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss  val_loss\n",
       "0    1.0      1.5008  1.360325\n",
       "1    2.0      1.1715  1.301108\n",
       "2    3.0      0.9917  1.295173"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3053199a76db4ae79982d2fe3ca191c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89510f84fa554845bf9df0a36ec3853c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Pushed final model + TSV to Hugging Face.\n"
     ]
    }
   ],
   "source": [
    "import os, inspect\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "\n",
    "# ====== CHECK ======\n",
    "assert \"model\" in globals(), \"Missing `model`\"\n",
    "assert \"tokenizer\" in globals(), \"Missing `tokenizer`\"\n",
    "assert \"train_tok\" in globals(), \"Missing `train_tok`\"\n",
    "assert \"val_tok\" in globals(), \"Missing `val_tok`\"\n",
    "assert \"SRC_LANG\" in globals() and \"TGT_LANG\" in globals(), \"Missing SRC_LANG/TGT_LANG\"\n",
    "\n",
    "# Fix stateful tokenizer for mBART-50\n",
    "tokenizer.src_lang = SRC_LANG\n",
    "tokenizer.tgt_lang = TGT_LANG\n",
    "\n",
    "# ====== CONFIG (anh chỉnh ở đây) ======\n",
    "OUTPUT_DIR  = \"mbart50_envi\"\n",
    "NUM_EPOCHS  = 3\n",
    "\n",
    "TRAIN_BS = 2\n",
    "EVAL_BS  = 2\n",
    "GRAD_ACC = 8\n",
    "LR       = 3e-5\n",
    "\n",
    "# Hugging Face repo\n",
    "HF_REPO_ID = \"ngothuyet/mbart50-envi\"\n",
    "HF_PRIVATE = True\n",
    "\n",
    "# ====== DATA COLLATOR ======\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# ====== TrainingArguments ======\n",
    "# Mục tiêu: eval/log theo epoch để có loss, nhưng KHÔNG save checkpoint.\n",
    "ta_kwargs = dict(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BS,\n",
    "    per_device_eval_batch_size=EVAL_BS,\n",
    "    gradient_accumulation_steps=GRAD_ACC,\n",
    "    learning_rate=LR,\n",
    "    warmup_ratio=0.03,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    # ✅ log + eval theo epoch để lấy train_loss & val_loss\n",
    "    predict_with_generate=False,     # chỉ cần loss -> nhanh\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=2,\n",
    "    report_to=\"none\",\n",
    "\n",
    "    # ✅ KHÔNG LƯU CHECKPOINT (tránh lỗi write optimizer.pt)\n",
    "    save_strategy=\"no\",              # ❌ không checkpoint\n",
    "    save_only_model=True,            # ❌ không optimizer/scheduler (an toàn)\n",
    ")\n",
    "\n",
    "sig = inspect.signature(Seq2SeqTrainingArguments.__init__).parameters\n",
    "\n",
    "# logging theo epoch\n",
    "if \"logging_strategy\" in sig:\n",
    "    ta_kwargs[\"logging_strategy\"] = \"epoch\"\n",
    "\n",
    "# evaluation theo epoch (tên field có thể khác nhau theo version)\n",
    "if \"evaluation_strategy\" in sig:\n",
    "    ta_kwargs[\"evaluation_strategy\"] = \"epoch\"\n",
    "elif \"eval_strategy\" in sig:\n",
    "    ta_kwargs[\"eval_strategy\"] = \"epoch\"\n",
    "else:\n",
    "    raise ValueError(\"Transformers version: missing evaluation_strategy/eval_strategy\")\n",
    "\n",
    "# ✅ Push chỉ 1 lần ở cuối (vì không save checkpoint)\n",
    "# (không dùng hub_strategy=\"every_save\" vì không còn save)\n",
    "ta_kwargs.update(dict(\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=HF_REPO_ID,\n",
    "    hub_private_repo=HF_PRIVATE,\n",
    "    hub_strategy=\"end\",              # ✅ chỉ push cuối\n",
    "))\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(**ta_kwargs)\n",
    "\n",
    "# ====== TRAINER ======\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"[INFO] Start training...\")\n",
    "train_result = trainer.train()\n",
    "print(\"\\n===== TRAIN DONE =====\")\n",
    "print(train_result)\n",
    "\n",
    "print(\"\\n===== FINAL EVAL (VAL) =====\")\n",
    "final_metrics = trainer.evaluate()\n",
    "for k, v in final_metrics.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# ====== SAVE FINAL LOCAL (model + tokenizer) ======\n",
    "trainer.save_model(OUTPUT_DIR)          # tạo model.safetensors, config.json,...\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)   # tokenizer files\n",
    "print(\"[OK] Saved final model/tokenizer to:\", OUTPUT_DIR)\n",
    "\n",
    "# ====== EXPORT TSV (train_loss + val_loss) ======\n",
    "logs = trainer.state.log_history\n",
    "\n",
    "train_loss_by_epoch = {}\n",
    "val_loss_by_epoch = {}\n",
    "\n",
    "for row in logs:\n",
    "    # train loss rows\n",
    "    if \"epoch\" in row and \"loss\" in row and \"eval_loss\" not in row:\n",
    "        train_loss_by_epoch[row[\"epoch\"]] = row[\"loss\"]\n",
    "    # eval loss rows\n",
    "    if \"epoch\" in row and \"eval_loss\" in row:\n",
    "        val_loss_by_epoch[row[\"epoch\"]] = row[\"eval_loss\"]\n",
    "\n",
    "epochs = sorted(set(list(train_loss_by_epoch.keys()) + list(val_loss_by_epoch.keys())))\n",
    "df = pd.DataFrame({\n",
    "    \"epoch\": epochs,\n",
    "    \"train_loss\": [train_loss_by_epoch.get(e, None) for e in epochs],\n",
    "    \"val_loss\": [val_loss_by_epoch.get(e, None) for e in epochs],\n",
    "})\n",
    "\n",
    "tsv_path = os.path.join(OUTPUT_DIR, \"train_val_loss.tsv\")\n",
    "df.to_csv(tsv_path, sep=\"\\t\", index=False)\n",
    "print(\"[OK] Saved TSV:\", tsv_path)\n",
    "display(df)\n",
    "\n",
    "# ====== PUSH FINAL + TSV ======\n",
    "# Vì OUTPUT_DIR chứa model + tokenizer + training_args.bin + train_val_loss.tsv\n",
    "trainer.push_to_hub(commit_message=\"Final after training (no checkpoints) + train_val_loss.tsv\")\n",
    "print(\"[OK] Pushed final model + TSV to Hugging Face.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T21:53:40.687462Z",
     "iopub.status.busy": "2025-12-14T21:53:40.687206Z",
     "iopub.status.idle": "2025-12-14T21:54:06.532200Z",
     "shell.execute_reply": "2025-12-14T21:54:06.531446Z",
     "shell.execute_reply.started": "2025-12-14T21:53:40.687447Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved TSV: mbart50_envi/train_val_loss.tsv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5008</td>\n",
       "      <td>1.360325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.1715</td>\n",
       "      <td>1.301108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9917</td>\n",
       "      <td>1.295173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss  val_loss\n",
       "0    1.0      1.5008  1.360325\n",
       "1    2.0      1.1715  1.301108\n",
       "2    3.0      0.9917  1.295173"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2a240869e549509851904199a67ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b7a445805434825b1062fb9b200dab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Pushed TSV to hub.\n"
     ]
    }
   ],
   "source": [
    "# lấy log_history và gom theo epoch\n",
    "logs = trainer.state.log_history\n",
    "\n",
    "train_loss_by_epoch = {}\n",
    "val_loss_by_epoch = {}\n",
    "\n",
    "for row in logs:\n",
    "    if \"epoch\" in row and \"loss\" in row and \"eval_loss\" not in row:\n",
    "        train_loss_by_epoch[row[\"epoch\"]] = row[\"loss\"]\n",
    "    if \"epoch\" in row and \"eval_loss\" in row:\n",
    "        val_loss_by_epoch[row[\"epoch\"]] = row[\"eval_loss\"]\n",
    "\n",
    "epochs = sorted(set(list(train_loss_by_epoch.keys()) + list(val_loss_by_epoch.keys())))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"epoch\": epochs,\n",
    "    \"train_loss\": [train_loss_by_epoch.get(e, None) for e in epochs],\n",
    "    \"val_loss\": [val_loss_by_epoch.get(e, None) for e in epochs],\n",
    "})\n",
    "\n",
    "tsv_path = f\"{OUTPUT_DIR}/train_val_loss.tsv\"\n",
    "df.to_csv(tsv_path, sep=\"\\t\", index=False)\n",
    "\n",
    "print(\"[OK] Saved TSV:\", tsv_path)\n",
    "display(df)\n",
    "\n",
    "# push TSV lên hub (nằm trong output_dir sẽ được push)\n",
    "trainer.push_to_hub(commit_message=\"Add train_val_loss.tsv\")\n",
    "print(\"[OK] Pushed TSV to hub.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model từ hugging face "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T21:54:06.533391Z",
     "iopub.status.busy": "2025-12-14T21:54:06.533042Z",
     "iopub.status.idle": "2025-12-14T21:54:16.678053Z",
     "shell.execute_reply": "2025-12-14T21:54:16.677207Z",
     "shell.execute_reply.started": "2025-12-14T21:54:06.533366Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d99e2db64a64433801969b0512a438c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.68k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f74a99c36aa4482e95f1477b1f5c665c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Loaded from hub: ngothuyet/mbart50-envi\n",
      "[INFO] device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "HF_REPO_ID = \"ngothuyet/mbart50-envi\"  # <-- đổi\n",
    "\n",
    "SRC_LANG = \"en_XX\"\n",
    "TGT_LANG = \"vi_VN\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_REPO_ID, use_fast=False)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(HF_REPO_ID)\n",
    "\n",
    "tokenizer.src_lang = SRC_LANG\n",
    "tokenizer.tgt_lang = TGT_LANG\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"[OK] Loaded from hub:\", HF_REPO_ID)\n",
    "print(\"[INFO] device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sử dụng beam search để dịch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T21:54:16.679222Z",
     "iopub.status.busy": "2025-12-14T21:54:16.678956Z",
     "iopub.status.idle": "2025-12-14T21:54:17.413027Z",
     "shell.execute_reply": "2025-12-14T21:54:17.412249Z",
     "shell.execute_reply.started": "2025-12-14T21:54:16.679204Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INPUT ] [\"It's necessary to provide health insurance communication and education for people who live in remote areas and participate interupted health insurance.\", 'I am learning natural language processing.']\n",
      "[OUTPUT] ['Cần thiết cung cấp dịch vụ truyền thông và giáo dục bảo hiểm y tế cho những người sống ở vùng xa xôi và tham gia BHYT.', 'Tôi đang học cách xử lý ngôn ngữ tự nhiên.']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from typing import List\n",
    "\n",
    "MAX_SRC_LEN = 256\n",
    "\n",
    "@torch.inference_mode()\n",
    "def translate_en2vi(\n",
    "    texts: List[str],\n",
    "    num_beams: int = 5,\n",
    "    length_penalty: float = 1.0,\n",
    "    no_repeat_ngram_size: int = 3,\n",
    "    max_new_tokens: int = 128,\n",
    "):\n",
    "    model.eval()\n",
    "    tokenizer.src_lang = SRC_LANG\n",
    "    tokenizer.tgt_lang = TGT_LANG\n",
    "\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_SRC_LEN\n",
    "    ).to(model.device)\n",
    "\n",
    "    generated = model.generate(\n",
    "        **enc,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[TGT_LANG],\n",
    "        num_beams=num_beams,\n",
    "        length_penalty=length_penalty,\n",
    "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    return tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "\n",
    "# Demo nhanh\n",
    "demo = [\n",
    "    \"It's necessary to provide health insurance communication and education for people who live in remote areas and participate interupted health insurance.\",\n",
    "    \"I am learning natural language processing.\"\n",
    "]\n",
    "print(\"[INPUT ]\", demo)\n",
    "print(\"[OUTPUT]\", translate_en2vi(demo, num_beams=5, length_penalty=1.0, no_repeat_ngram_size=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tính bleu trên toàn bộ tập test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T21:54:17.414553Z",
     "iopub.status.busy": "2025-12-14T21:54:17.413985Z",
     "iopub.status.idle": "2025-12-14T21:54:17.520486Z",
     "shell.execute_reply": "2025-12-14T21:54:17.519929Z",
     "shell.execute_reply.started": "2025-12-14T21:54:17.414533Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] test_raw lines -> 3,000 pairs.\n",
      "[INFO] test: after clean+filter -> 2,946 pairs\n",
      "[INFO] test: after dedup       -> 2,943 pairs\n",
      "[INFO] public_test after clean: 2,943 pairs\n",
      "[SAMPLE TEST] ('Knowledge, practices in public health service utilization among health insurance card’s holders and influencing factors in Vientiane, Lao', 'Thực trạng kiến thức và thực hành của người có thẻ bảo hiểm y tế trong sử dụng dịch vụ khám chữa bệnh ở các cơ sở y tế công và một số yếu tố ảnh hưởng tại tỉnh Viêng Chăn, CHDCND Lào, năm 2017')\n",
      "[OK] ds['test'] size: 2943\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# đọc song song public_test.en/vi -> list[(en, vi)]\n",
    "test_pairs_raw = load_parallel(TEST_EN, TEST_VI, \"test_raw\")\n",
    "\n",
    "# clean/filter/dedup giống pipeline train\n",
    "test_pairs = clean_filter_dedup(test_pairs_raw, \"test\")\n",
    "\n",
    "print(f\"[INFO] public_test after clean: {len(test_pairs):,} pairs\")\n",
    "print(\"[SAMPLE TEST]\", test_pairs[0])\n",
    "\n",
    "# tạo ds_test (giữ nguyên ds train/val của anh nếu đã có)\n",
    "test_ds = Dataset.from_dict({\n",
    "    \"src_text\": [s for s, _ in test_pairs],\n",
    "    \"tgt_text\": [t for _, t in test_pairs],\n",
    "})\n",
    "\n",
    "# gắn vào DatasetDict nếu đã có ds, còn không thì tạo ds mới\n",
    "if \"ds\" in globals():\n",
    "    ds[\"test\"] = test_ds\n",
    "else:\n",
    "    from datasets import DatasetDict\n",
    "    ds = DatasetDict({\"test\": test_ds})\n",
    "\n",
    "print(\"[OK] ds['test'] size:\", len(ds[\"test\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T21:54:17.521519Z",
     "iopub.status.busy": "2025-12-14T21:54:17.521272Z",
     "iopub.status.idle": "2025-12-14T22:13:58.014987Z",
     "shell.execute_reply": "2025-12-14T22:13:58.014344Z",
     "shell.execute_reply.started": "2025-12-14T21:54:17.521502Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e0418e059664ae48d3569246017b9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BLEU on public_test:   0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PUBLIC_TEST] sacreBLEU (2943 samples): 41.51\n",
      "[OK] Saved: mbart50_envi/public_test_metrics.json\n",
      "[OK] Saved: mbart50_envi/public_test_bleu.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a133b21813441459c70fb33334f989a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "548dadfaf90d4a228b7f8f40a8767e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Pushed public_test BLEU files to Hugging Face.\n"
     ]
    }
   ],
   "source": [
    "import os, json, time\n",
    "import torch\n",
    "import sacrebleu\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "assert \"ds\" in globals() and \"test\" in ds, \"Missing ds['test'] - run TEST-LOAD cell first.\"\n",
    "assert \"translate_en2vi\" in globals(), \"Missing translate_en2vi() - run inference cell first.\"\n",
    "assert \"OUTPUT_DIR\" in globals(), \"Missing OUTPUT_DIR (e.g., 'mbart50_envi').\"\n",
    "assert \"trainer\" in globals(), \"Missing trainer - run training cell first (to push easily).\"\n",
    "\n",
    "def bleu_on_public_test_and_push(batch_size=16):\n",
    "    srcs = ds[\"test\"][\"src_text\"]\n",
    "    refs = ds[\"test\"][\"tgt_text\"]\n",
    "\n",
    "    bs = batch_size if torch.cuda.is_available() else max(2, batch_size // 4)\n",
    "\n",
    "    hyps = []\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i in tqdm(range(0, len(srcs), bs), desc=\"BLEU on public_test\", total=(len(srcs) + bs - 1)//bs):\n",
    "        batch = srcs[i:i+bs]\n",
    "        hyps.extend(\n",
    "            translate_en2vi(\n",
    "                batch,\n",
    "                num_beams=5,\n",
    "                length_penalty=1.0,\n",
    "                no_repeat_ngram_size=3,\n",
    "                max_new_tokens=128\n",
    "            )\n",
    "        )\n",
    "\n",
    "    bleu = sacrebleu.corpus_bleu(hyps, [refs]).score\n",
    "    elapsed = time.time() - t0\n",
    "\n",
    "    metrics = {\n",
    "        \"public_test_sacrebleu\": float(bleu),\n",
    "        \"public_test_size\": int(len(srcs)),\n",
    "        \"batch_size\": int(bs),\n",
    "        \"elapsed_sec\": float(elapsed),\n",
    "    }\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    json_path = os.path.join(OUTPUT_DIR, \"public_test_metrics.json\")\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    txt_path = os.path.join(OUTPUT_DIR, \"public_test_bleu.txt\")\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"public_test_sacrebleu\\t{bleu:.4f}\\n\")\n",
    "\n",
    "    print(f\"\\n[PUBLIC_TEST] sacreBLEU ({len(srcs)} samples): {bleu:.2f}\")\n",
    "    print(\"[OK] Saved:\", json_path)\n",
    "    print(\"[OK] Saved:\", txt_path)\n",
    "\n",
    "    # push lên repo model\n",
    "    trainer.push_to_hub(commit_message=f\"Add public_test sacreBLEU={bleu:.2f}\")\n",
    "    print(\"[OK] Pushed public_test BLEU files to Hugging Face.\")\n",
    "\n",
    "    return bleu, metrics\n",
    "\n",
    "_ = bleu_on_public_test_and_push(batch_size=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hiển thị điểm bleu được lấy từ hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T22:13:58.015854Z",
     "iopub.status.busy": "2025-12-14T22:13:58.015630Z",
     "iopub.status.idle": "2025-12-14T22:13:58.021502Z",
     "shell.execute_reply": "2025-12-14T22:13:58.020861Z",
     "shell.execute_reply.started": "2025-12-14T22:13:58.015838Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "✅ sacreBLEU on public_test (2943 sentences): 41.51\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "\n",
    "json_path = os.path.join(OUTPUT_DIR, \"public_test_metrics.json\")\n",
    "\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "bleu = metrics[\"public_test_sacrebleu\"]\n",
    "n = metrics[\"public_test_size\"]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"✅ sacreBLEU on public_test ({n} sentences): {bleu:.2f}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print 5 dòng test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T22:13:58.024703Z",
     "iopub.status.busy": "2025-12-14T22:13:58.024490Z",
     "iopub.status.idle": "2025-12-14T22:13:59.314514Z",
     "shell.execute_reply": "2025-12-14T22:13:59.313906Z",
     "shell.execute_reply.started": "2025-12-14T22:13:58.024688Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "[1] SRC (EN): Knowledge, practices in public health service utilization among health insurance card’s holders and influencing factors in Vientiane, Lao\n",
      "    TGT (VI): Thực trạng kiến thức và thực hành của người có thẻ bảo hiểm y tế trong sử dụng dịch vụ khám chữa bệnh ở các cơ sở y tế công và một số yếu tố ảnh hưởng tại tỉnh Viêng Chăn, CHDCND Lào, năm 2017\n",
      "    PRED(VI): Kiến thức, thực hành sử dụng dịch vụ y tế công cộng của người chủ thẻ BHYT và một số yếu tố ảnh hưởng ở Vientiane, Lao\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[2] SRC (EN): Describe knowledge, practices in public health service utilization among health insurance card's holders and influencing factors in Vientiane, Lao PDR, 2017.\n",
      "    TGT (VI): Mô tả thực trạng kiến thức, thực hành của người có thẻ bảo hiểm y tế trong sử dụng dịch vụ khám chữa bệnh ở các cơ sở y tế công và một số yếu tố liên quan tại tỉnh Viêng Chăn, Cộng hoà Dân chủ Nhân dân Lào năm 2017.\n",
      "    PRED(VI): Mô tả kiến thức, thực hành sử dụng dịch vụ y tế công cộng của người chủ thẻ BHYT và một số yếu tố ảnh hưởng ở Vientiane, Lao PDR, 2017.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[3] SRC (EN): Methodology: A cross sectional study was used among 928 adult health insurance card's holders in Phone Hong and Keo Oudom districts, Vientiane province.\n",
      "    TGT (VI): Phương pháp: Thiết kế nghiên mô tả cắt ngang được thực hiện trên 928 người trưởng thành có thẻ bảo hiểm y tế tại 2 huyện Phone Hong và Keo Oudom, tỉnh Viêng Chăn.\n",
      "    PRED(VI): Đối tượng và phương pháp nghiên cứu: Nghiên cứu mô tả cắt ngang trên 928 người chủ thẻ bảo hiểm y tế người lớn tại huyện Tông Hẫu và huyện Kèo Vương, tỉnh Vientiane.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[4] SRC (EN): Results: Percentage of card's holders who knew the finance-free utilization of the first registered public health services was 44.5% and being provided health insurance information was 34.8%.\n",
      "    TGT (VI): Kết quả: Tỷ lệ người biết được khám chữa bệnh (KCB) miễn phí tại nơi đăng ký ban đầu chiếm 44,5%, được cung cấp thông tin về bảo hiểm y tế (BHYT) chiếm 34,8%.\n",
      "    PRED(VI): Kết quả: Tỷ lệ người sở hữu thẻ biết sử dụng dịch vụ y tế công cộng đăng ký đầu tiên miễn phí là 44,5% và được cung cấp thông tin bảo hiểm là 34,8%.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[5] SRC (EN): Percentage of card's holders who went to the first registered public health services was 61.8%.\n",
      "    TGT (VI): Tỷ lệ người có thẻ BHYT thực hành khám chữa bệnh đúng nơi đăng ký KCB ban đầu chiếm 61,8%.\n",
      "    PRED(VI): Tỷ lệ người có thẻ đi khám bệnh tại trạm y tế công lập đầu tiên là 61,8%.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# kiểm tra điều kiện\n",
    "assert \"ds\" in globals() and \"test\" in ds, \"Missing ds['test']\"\n",
    "assert \"translate_en2vi\" in globals(), \"Missing translate_en2vi()\"\n",
    "\n",
    "N_SHOW = 5  # số dòng muốn in\n",
    "\n",
    "srcs = ds[\"test\"][\"src_text\"][:N_SHOW]\n",
    "tgts = ds[\"test\"][\"tgt_text\"][:N_SHOW]\n",
    "\n",
    "# dịch\n",
    "preds = translate_en2vi(\n",
    "    srcs,\n",
    "    num_beams=5,\n",
    "    length_penalty=1.0,\n",
    "    no_repeat_ngram_size=3,\n",
    "    max_new_tokens=128,\n",
    ")\n",
    "\n",
    "print(\"=\" * 100)\n",
    "for i, (src, tgt, pred) in enumerate(zip(srcs, tgts, preds), start=1):\n",
    "    print(f\"[{i}] SRC (EN): {src}\")\n",
    "    print(f\"    TGT (VI): {tgt}\")\n",
    "    print(f\"    PRED(VI): {pred}\")\n",
    "    print(\"-\" * 100)\n",
    "print(\"=\" * 100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
