{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 14128026,
     "sourceType": "datasetVersion",
     "datasetId": 9001785
    }
   ],
   "dockerImageVersionId": 31193,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# üîß CELL 0 ‚Äî Thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng\n\n## M·ª•c ƒë√≠ch\nCell n√†y thi·∫øt l·∫≠p c√°c bi·∫øn m√¥i tr∆∞·ªùng **TR∆Ø·ªöC KHI** import c√°c th∆∞ vi·ªán ƒë·ªÉ tr√°nh c√°c l·ªói xung ƒë·ªôt gi·ªØa PyTorch v√† TensorFlow.\n\n## Gi·∫£i th√≠ch chi ti·∫øt c√°c bi·∫øn:\n\n| Bi·∫øn m√¥i tr∆∞·ªùng | Gi√° tr·ªã | M·ª•c ƒë√≠ch |\n|-----------------|---------|----------|\n| `TRANSFORMERS_NO_TF` | \"1\" | Ch·∫∑n Transformers t·ª± ƒë·ªông import TensorFlow |\n| `TRANSFORMERS_NO_FLAX` | \"1\" | Ch·∫∑n Transformers t·ª± ƒë·ªông import Flax (JAX framework) |\n| `TF_CPP_MIN_LOG_LEVEL` | \"3\" | T·∫Øt c√°c log warning c·ªßa TensorFlow C++ backend |\n| `TOKENIZERS_PARALLELISM` | \"false\" | T·∫Øt tokenization song song ƒë·ªÉ tr√°nh deadlock trong multi-processing |\n\n## T·∫°i sao c·∫ßn thi·∫øt?\n- **Tr√°nh l·ªói protobuf/MessageFactory**: Khi TensorFlow v√† PyTorch c√πng t·ªìn t·∫°i, c√≥ th·ªÉ x·∫£y ra xung ƒë·ªôt protobuf version\n- **Gi·∫£m warning kh√¥ng c·∫ßn thi·∫øt**: C√°c log c·ªßa TF c√≥ th·ªÉ g√¢y nhi·ªÖu output\n- **·ªîn ƒë·ªãnh m√¥i tr∆∞·ªùng**: ƒê·∫£m b·∫£o ch·ªâ s·ª≠ d·ª•ng PyTorch backend cho to√†n b·ªô pipeline\n\n## L∆∞u √Ω\n- **Ph·∫£i ch·∫°y cell n√†y TR∆Ø·ªöC** khi import b·∫•t k·ª≥ th∆∞ vi·ªán n√†o kh√°c\n- N·∫øu b·ªè qua cell n√†y, c√≥ th·ªÉ g·∫∑p l·ªói MessageFactory ho·∫∑c protobuf conflicts",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-16T17:09:21.955329Z",
     "iopub.execute_input": "2025-12-16T17:09:21.955558Z",
     "iopub.status.idle": "2025-12-16T17:09:21.962689Z",
     "shell.execute_reply.started": "2025-12-16T17:09:21.955539Z",
     "shell.execute_reply": "2025-12-16T17:09:21.962094Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import sys, torch\nimport numpy as np\n\nimport transformers, datasets, accelerate\n\nprint(\"Python:\", sys.version.split()[0])\nprint(\"Torch:\", torch.__version__)\nprint(\"CUDA:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"GPU:\", torch.cuda.get_device_name(0))\n\nprint(\"Transformers:\", transformers.__version__)\nprint(\"Datasets:\", datasets.__version__)\nprint(\"Accelerate:\", accelerate.__version__)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-16T17:09:21.964666Z",
     "iopub.execute_input": "2025-12-16T17:09:21.964924Z",
     "iopub.status.idle": "2025-12-16T17:09:30.292513Z",
     "shell.execute_reply.started": "2025-12-16T17:09:21.964901Z",
     "shell.execute_reply": "2025-12-16T17:09:30.291588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Python: 3.11.13\nTorch: 2.6.0+cu124\nCUDA: True\nGPU: Tesla T4\nTransformers: 4.53.3\nDatasets: 4.4.1\nAccelerate: 1.9.0\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": "# üìä CELL 1 ‚Äî Ki·ªÉm tra m√¥i tr∆∞·ªùng v√† GPU\n\n## M·ª•c ƒë√≠ch\nKi·ªÉm tra v√† hi·ªÉn th·ªã th√¥ng tin v·ªÅ m√¥i tr∆∞·ªùng runtime ƒë·ªÉ ƒë·∫£m b·∫£o:\n- Phi√™n b·∫£n Python v√† c√°c th∆∞ vi·ªán t∆∞∆°ng th√≠ch\n- GPU c√≥ s·∫µn ƒë·ªÉ tƒÉng t·ªëc training\n- Tr√°nh l·ªói compatibility gi·ªØa c√°c version\n\n## C√°c th√¥ng tin ƒë∆∞·ª£c ki·ªÉm tra:\n\n| Th√¥ng tin | √ù nghƒ©a |\n|-----------|---------|\n| `Python: 3.11.13` | Phi√™n b·∫£n Python ƒëang ch·∫°y |\n| `Torch: 2.6.0+cu124` | PyTorch 2.6.0 v·ªõi CUDA 12.4 |\n| `CUDA: True` | GPU c√≥ s·∫µn v√† ƒë∆∞·ª£c k√≠ch ho·∫°t |\n| `GPU: Tesla T4` | Card ƒë·ªì h·ªça (16GB VRAM tr√™n Kaggle) |\n| `Transformers: 4.53.3` | Hugging Face Transformers library |\n| `Datasets: 4.4.1` | Hugging Face Datasets library |\n| `Accelerate: 1.9.0` | Library h·ªó tr·ª£ distributed training |\n\n## L∆∞u √Ω quan tr·ªçng\n- **Tesla T4** l√† GPU ph·ªï bi·∫øn tr√™n Kaggle/Colab v·ªõi 16GB VRAM\n- N·∫øu `CUDA: False`, model s·∫Ω ch·∫°y tr√™n CPU (ch·∫≠m h∆°n 10-100x)\n- Transformers 4.53.3 l√† version m·ªõi nh·∫•t, t∆∞∆°ng th√≠ch v·ªõi mBART-50",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# üì¶ CELL 2 ‚Äî C√†i ƒë·∫∑t packages b·ªï sung\n\n## M·ª•c ƒë√≠ch\nC√†i ƒë·∫∑t c√°c package c·∫ßn thi·∫øt n·∫øu ch∆∞a c√≥ trong m√¥i tr∆∞·ªùng Kaggle.\n\n## Packages ƒë∆∞·ª£c c√†i:\n\n| Package | M·ª•c ƒë√≠ch | B·∫Øt bu·ªôc? |\n|---------|----------|-----------|\n| `sacrebleu` | T√≠nh ƒëi·ªÉm BLEU chu·∫©n cho ƒë√°nh gi√° d·ªãch m√°y | ‚úÖ B·∫Øt bu·ªôc |\n| `hf_transfer` | TƒÉng t·ªëc download/upload model t·ª´ Hugging Face Hub | ‚ö™ T√πy ch·ªçn |\n\n## C√°ch ho·∫°t ƒë·ªông:\n1. `importlib.util.find_spec()` ki·ªÉm tra package ƒë√£ ƒë∆∞·ª£c c√†i ch∆∞a\n2. N·∫øu ch∆∞a c√≥ ‚Üí g·ªçi `pip install -q` (quiet mode)\n3. N·∫øu ƒë√£ c√≥ ‚Üí b·ªè qua, in th√¥ng b√°o \"already installed\"\n\n## V·ªÅ sacrebleu\n- **SacreBLEU** l√† c√¥ng c·ª• chu·∫©n ƒë·ªÉ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng d·ªãch m√°y\n- T√≠nh ƒëi·ªÉm BLEU (Bilingual Evaluation Understudy) t·ª´ 0-100\n- ƒêi·ªÉm c√†ng cao = b·∫£n d·ªãch c√†ng gi·ªëng v·ªõi reference\n- Cung c·∫•p k·∫øt qu·∫£ consistent v√† reproducible (kh√¥ng ph·ª• thu·ªôc v√†o tokenization)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import importlib.util, subprocess\n\ndef pip_install(pkg: str):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n\n# sacrebleu: b·∫Øt bu·ªôc\nif importlib.util.find_spec(\"sacrebleu\") is None:\n    print(\"[INFO] Installing sacrebleu ...\")\n    pip_install(\"sacrebleu\")\nelse:\n    print(\"[OK] sacrebleu already installed\")\n\n# hf_transfer: optional tƒÉng t·ªëc push/pull\nif importlib.util.find_spec(\"hf_transfer\") is None:\n    print(\"[INFO] Installing hf_transfer (optional) ...\")\n    pip_install(\"hf_transfer\")\nelse:\n    print(\"[OK] hf_transfer already installed\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-16T17:09:30.293537Z",
     "iopub.execute_input": "2025-12-16T17:09:30.294252Z",
     "iopub.status.idle": "2025-12-16T17:09:34.715208Z",
     "shell.execute_reply.started": "2025-12-16T17:09:30.294228Z",
     "shell.execute_reply": "2025-12-16T17:09:34.714525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "[INFO] Installing sacrebleu ...\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51.8/51.8 kB 1.6 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 104.1/104.1 kB 5.3 MB/s eta 0:00:00\n[OK] hf_transfer already installed\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": "import re, math, random, hashlib\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n    set_seed,\n)\nimport sacrebleu\n\nprint(\"[OK] Imports done.\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-16T17:09:34.715859Z",
     "iopub.execute_input": "2025-12-16T17:09:34.716081Z",
     "iopub.status.idle": "2025-12-16T17:10:02.410176Z",
     "shell.execute_reply.started": "2025-12-16T17:09:34.716021Z",
     "shell.execute_reply": "2025-12-16T17:10:02.409526Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765904980.968127      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765904981.036662      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ],
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ],
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ],
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ],
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ],
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error"
    },
    {
     "name": "stdout",
     "text": "[OK] Imports done.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": "from pathlib import Path\n\ndef find_dataset_dir() -> Path:\n    base = Path(\"/kaggle/input\")\n    for d in base.glob(\"*\"):\n        if d.is_dir() and (d / \"train.en.txt\").exists() and (d / \"train.vi.txt\").exists():\n            return d\n    for d in base.glob(\"*/*\"):\n        if d.is_dir() and (d / \"train.en.txt\").exists() and (d / \"train.vi.txt\").exists():\n            return d\n    raise FileNotFoundError(\"Could not find dataset folder containing train.en.txt/train.vi.txt\")\n\nDATA_DIR = find_dataset_dir()\nprint(\"[OK] Found dataset dir:\", DATA_DIR)\n\nTRAIN_EN = DATA_DIR / \"train.en.txt\"\nTRAIN_VI = DATA_DIR / \"train.vi.txt\"\nTEST_EN  = DATA_DIR / \"public_test.en.txt\"\nTEST_VI  = DATA_DIR / \"public_test.vi.txt\"\n\nfor p in [TRAIN_EN, TRAIN_VI, TEST_EN, TEST_VI]:\n    print(\"[CHECK]\", p.name, \"->\", \"OK\" if p.exists() else \"NOT FOUND\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-16T17:10:02.410863Z",
     "iopub.execute_input": "2025-12-16T17:10:02.411438Z",
     "iopub.status.idle": "2025-12-16T17:10:02.430564Z",
     "shell.execute_reply.started": "2025-12-16T17:10:02.411417Z",
     "shell.execute_reply": "2025-12-16T17:10:02.429710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "[OK] Found dataset dir: /kaggle/input/databaitoanphu\n[CHECK] train.en.txt -> OK\n[CHECK] train.vi.txt -> OK\n[CHECK] public_test.en.txt -> OK\n[CHECK] public_test.vi.txt -> OK\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": "def read_lines(path: Path, encoding=\"utf-8\"):\n    with open(path, \"r\", encoding=encoding, errors=\"replace\") as f:\n        return [line.rstrip(\"\\n\") for line in f]\n\ndef load_parallel(en_path: Path, vi_path: Path, name=\"data\"):\n    en_lines = read_lines(en_path)\n    vi_lines = read_lines(vi_path)\n    n = min(len(en_lines), len(vi_lines))\n    if len(en_lines) != len(vi_lines):\n        print(f\"[WARN] {name} mismatch -> EN={len(en_lines):,}, VI={len(vi_lines):,}. Using first {n:,} pairs.\")\n    else:\n        print(f\"[INFO] {name} lines -> {n:,} pairs.\")\n    return list(zip(en_lines[:n], vi_lines[:n]))\n\ntrain_pairs_raw = load_parallel(TRAIN_EN, TRAIN_VI, \"train_raw\")\ntest_pairs_raw  = load_parallel(TEST_EN,  TEST_VI,  \"test_raw\")\n\nprint(\"[SAMPLE train_raw]\", train_pairs_raw[0])\nprint(\"[SAMPLE test_raw ]\", test_pairs_raw[0])",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-16T17:10:02.431324Z",
     "iopub.execute_input": "2025-12-16T17:10:02.431495Z",
     "iopub.status.idle": "2025-12-16T17:10:05.063488Z",
     "shell.execute_reply.started": "2025-12-16T17:10:02.431480Z",
     "shell.execute_reply": "2025-12-16T17:10:05.062816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "[INFO] train_raw lines -> 500,000 pairs.\n[INFO] test_raw lines -> 3,000 pairs.\n[SAMPLE train_raw] ('To evaluate clinical, subclinical symptoms of patients with otitis media with effusion and V.a at otorhinolaryngology department ‚Äì Thai Nguyen national hospital', 'Nghi√™n c·ª©u ƒë·∫∑c ƒëi·ªÉm l√¢m s√†ng, c·∫≠n l√¢m s√†ng b·ªánh nh√¢n vi√™m tai ·ª© d·ªãch tr√™n vi√™m V.A t·∫°i Khoa Tai m≈©i h·ªçng - B·ªánh vi·ªán Trung ∆∞∆°ng Th√°i Nguy√™n')\n[SAMPLE test_raw ] ('Knowledge, practices in public health service utilization among health insurance card‚Äôs holders and influencing factors in Vientiane, Lao', 'Th·ª±c tr·∫°ng ki·∫øn th·ª©c v√† th·ª±c h√†nh c·ªßa ng∆∞·ªùi c√≥ th·∫ª b·∫£o hi·ªÉm y t·∫ø trong s·ª≠ d·ª•ng d·ªãch v·ª• kh√°m ch·ªØa b·ªánh ·ªü c√°c c∆° s·ªü y t·∫ø c√¥ng v√† m·ªôt s·ªë y·∫øu t·ªë ·∫£nh h∆∞·ªüng t·∫°i t·ªânh Vi√™ng ChƒÉn, CHDCND L√†o, nƒÉm 2017')\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": "import re, hashlib\n\ndef basic_clean(s: str) -> str:\n    s = \"\" if s is None else s\n    s = s.strip()\n    s = re.sub(r\"\\s+\", \" \", s)\n    return s\n\nMIN_CHARS = 2\nMAX_CHARS = 400\n\ndef is_good_pair(src: str, tgt: str) -> bool:\n    if not src or not tgt:\n        return False\n    if len(src) < MIN_CHARS or len(tgt) < MIN_CHARS:\n        return False\n    if len(src) > MAX_CHARS or len(tgt) > MAX_CHARS:\n        return False\n    return True\n\ndef clean_filter_dedup(pairs, name=\"data\"):\n    # clean + filter\n    cleaned = []\n    for src, tgt in pairs:\n        src = basic_clean(src)\n        tgt = basic_clean(tgt)\n        if is_good_pair(src, tgt):\n            cleaned.append((src, tgt))\n    print(f\"[INFO] {name}: after clean+filter -> {len(cleaned):,} pairs\")\n\n    # dedup\n    seen = set()\n    dedup = []\n    for src, tgt in cleaned:\n        h = hashlib.md5((src + \"\\t\" + tgt).encode(\"utf-8\")).hexdigest()\n        if h not in seen:\n            seen.add(h)\n            dedup.append((src, tgt))\n    print(f\"[INFO] {name}: after dedup       -> {len(dedup):,} pairs\")\n    return dedup\n\ntrain_pairs = clean_filter_dedup(train_pairs_raw, \"train\")\ntest_pairs  = clean_filter_dedup(test_pairs_raw,  \"test\")\n\nprint(\"[SAMPLE clean train]\", train_pairs[0])\nprint(\"[SAMPLE clean test ]\", test_pairs[0])",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-16T17:10:05.065356Z",
     "iopub.execute_input": "2025-12-16T17:10:05.065615Z",
     "iopub.status.idle": "2025-12-16T17:10:16.267620Z",
     "shell.execute_reply.started": "2025-12-16T17:10:05.065596Z",
     "shell.execute_reply": "2025-12-16T17:10:16.266894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "[INFO] train: after clean+filter -> 489,008 pairs\n[INFO] train: after dedup       -> 340,522 pairs\n[INFO] test: after clean+filter -> 2,946 pairs\n[INFO] test: after dedup       -> 2,943 pairs\n[SAMPLE clean train] ('To evaluate clinical, subclinical symptoms of patients with otitis media with effusion and V.a at otorhinolaryngology department ‚Äì Thai Nguyen national hospital', 'Nghi√™n c·ª©u ƒë·∫∑c ƒëi·ªÉm l√¢m s√†ng, c·∫≠n l√¢m s√†ng b·ªánh nh√¢n vi√™m tai ·ª© d·ªãch tr√™n vi√™m V.A t·∫°i Khoa Tai m≈©i h·ªçng - B·ªánh vi·ªán Trung ∆∞∆°ng Th√°i Nguy√™n')\n[SAMPLE clean test ] ('Knowledge, practices in public health service utilization among health insurance card‚Äôs holders and influencing factors in Vientiane, Lao', 'Th·ª±c tr·∫°ng ki·∫øn th·ª©c v√† th·ª±c h√†nh c·ªßa ng∆∞·ªùi c√≥ th·∫ª b·∫£o hi·ªÉm y t·∫ø trong s·ª≠ d·ª•ng d·ªãch v·ª• kh√°m ch·ªØa b·ªánh ·ªü c√°c c∆° s·ªü y t·∫ø c√¥ng v√† m·ªôt s·ªë y·∫øu t·ªë ·∫£nh h∆∞·ªüng t·∫°i t·ªânh Vi√™ng ChƒÉn, CHDCND L√†o, nƒÉm 2017')\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": "import random\nfrom datasets import Dataset, DatasetDict\n\nSEED = 42\nN_TOTAL = 10000\n\nTRAIN_RATIO = 0.9  # 90/10\n\n# 1) shuffle train_pairs\nrng = random.Random(SEED)\nrng.shuffle(train_pairs)\n\n# 2) l·∫•y 5000 (n·∫øu kh√¥ng ƒë·ªß th√¨ l·∫•y h·∫øt)\nn_take = min(N_TOTAL, len(train_pairs))\nsubset = train_pairs[:n_take]\nprint(f\"[INFO] Train pairs available: {len(train_pairs):,}\")\nprint(f\"[INFO] Taking from train     : {n_take:,} pairs\")\n\n# 3) split 90/10\nn_train = int(n_take * TRAIN_RATIO)\nn_val = n_take - n_train\n\ntrain_subset = subset[:n_train]\nval_subset   = subset[n_train:]\n\nprint(f\"[INFO] Split -> train={len(train_subset):,} | val={len(val_subset):,}\")\n\n# 4) build datasets\ntrain_ds = Dataset.from_dict({\n    \"src_text\": [s for s, _ in train_subset],\n    \"tgt_text\": [t for _, t in train_subset],\n})\nval_ds = Dataset.from_dict({\n    \"src_text\": [s for s, _ in val_subset],\n    \"tgt_text\": [t for _, t in val_subset],\n})\n\n# test gi·ªØ nguy√™n t·ª´ public_test\ntest_ds = Dataset.from_dict({\n    \"src_text\": [s for s, _ in test_pairs],\n    \"tgt_text\": [t for _, t in test_pairs],\n})\n\nds = DatasetDict({\"train\": train_ds, \"validation\": val_ds, \"test\": test_ds})\nprint(ds)\n\nprint(\"[SAMPLE TRAIN]\", ds[\"train\"][0])\nprint(\"[SAMPLE VAL  ]\", ds[\"validation\"][0])\nprint(\"[SAMPLE TEST ]\", ds[\"test\"][0])",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-16T17:10:16.268289Z",
     "iopub.execute_input": "2025-12-16T17:10:16.268551Z",
     "iopub.status.idle": "2025-12-16T17:10:16.472854Z",
     "shell.execute_reply.started": "2025-12-16T17:10:16.268527Z",
     "shell.execute_reply": "2025-12-16T17:10:16.472234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "[INFO] Train pairs available: 340,522\n[INFO] Taking from train     : 10,000 pairs\n[INFO] Split -> train=9,000 | val=1,000\nDatasetDict({\n    train: Dataset({\n        features: ['src_text', 'tgt_text'],\n        num_rows: 9000\n    })\n    validation: Dataset({\n        features: ['src_text', 'tgt_text'],\n        num_rows: 1000\n    })\n    test: Dataset({\n        features: ['src_text', 'tgt_text'],\n        num_rows: 2943\n    })\n})\n[SAMPLE TRAIN] {'src_text': 'Monitor and control ICP using sedatives, endotracheal intubation, hyperventilation, hydration, diuretics, measures to control blood pressure, and sometimes corticosteroids.', 'tgt_text': 'Theo d√µi v√† ki·ªÉm so√°t ICP b·∫±ng c√°ch s·ª≠ d·ª•ng thu·ªëc an th·∫ßn, ƒë·∫∑t n·ªôi kh√≠ qu·∫£n, tƒÉng th√¥ng kh√≠, hydrat ho√°, thu·ªëc l·ª£i ti·ªÉu, c√°c bi·ªán ph√°p ƒë·ªÉ ki·ªÉm so√°t huy·∫øt √°p, v√† ƒë√¥i khi l√† corticosteroid.'}\n[SAMPLE VAL  ] {'src_text': 'The most obvious result of nonadherence is that the disorder may not be relieved or cured.', 'tgt_text': 'K·∫øt qu·∫£ r√µ r√†ng nh·∫•t c·ªßa s·ª± kh√¥ng tu√¢n th·ªß l√† b·ªánh c√≥ th·ªÉ kh√¥ng ƒë·ª° ho·∫∑c kh√¥ng h·∫øt b·ªánh.'}\n[SAMPLE TEST ] {'src_text': 'Knowledge, practices in public health service utilization among health insurance card‚Äôs holders and influencing factors in Vientiane, Lao', 'tgt_text': 'Th·ª±c tr·∫°ng ki·∫øn th·ª©c v√† th·ª±c h√†nh c·ªßa ng∆∞·ªùi c√≥ th·∫ª b·∫£o hi·ªÉm y t·∫ø trong s·ª≠ d·ª•ng d·ªãch v·ª• kh√°m ch·ªØa b·ªánh ·ªü c√°c c∆° s·ªü y t·∫ø c√¥ng v√† m·ªôt s·ªë y·∫øu t·ªë ·∫£nh h∆∞·ªüng t·∫°i t·ªânh Vi√™ng ChƒÉn, CHDCND L√†o, nƒÉm 2017'}\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\nMODEL_ID = \"facebook/mbart-large-50-many-to-many-mmt\"\nSRC_LANG = \"en_XX\"\nTGT_LANG = \"vi_VN\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=False)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_ID)\n\n# set src/tgt language (QUAN TR·ªåNG)\ntokenizer.src_lang = SRC_LANG\ntokenizer.tgt_lang = TGT_LANG   # <-- FIX KeyError: None\n\nforced_bos_token_id = tokenizer.lang_code_to_id[TGT_LANG]\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nprint(\"[OK] tokenizer/model loaded\")\nprint(\"[INFO] tokenizer.src_lang:\", tokenizer.src_lang)\nprint(\"[INFO] tokenizer.tgt_lang:\", tokenizer.tgt_lang)\nprint(\"[INFO] forced_bos_token_id:\", forced_bos_token_id)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-16T17:10:16.473567Z",
     "iopub.execute_input": "2025-12-16T17:10:16.473869Z",
     "iopub.status.idle": "2025-12-16T17:10:27.508674Z",
     "shell.execute_reply.started": "2025-12-16T17:10:16.473851Z",
     "shell.execute_reply": "2025-12-16T17:10:27.507957Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba4f9d64229f46ea93692f4223499bae"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "config.json: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c3ebe74ad65a47e68bc18fcc3c28d026"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c645a4dd17d24c5cb1e057f0285c9e2b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "91ab7ca0d94d4936a587bdc3f1d9bc1f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "efd8450025c9482f83c2843d3f4fcffa"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6dbba4161838444cbb93346643194eb5"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "[OK] tokenizer/model loaded\n[INFO] tokenizer.src_lang: en_XX\n[INFO] tokenizer.tgt_lang: vi_VN\n[INFO] forced_bos_token_id: 250024\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": "# ƒë·∫£m b·∫£o ds ƒë√£ c√≥ ·ªü cell split 5000 (train/validation/test)\nassert \"ds\" in globals(), \"You must run the data-loading/splitting cell to create `ds` first!\"\nassert \"tokenizer\" in globals(), \"You must run the tokenizer/model loading cell first!\"\n\nMAX_SRC_LEN = 256\nMAX_TGT_LEN = 256\n\ndef preprocess_batch(batch):\n    tokenizer.src_lang = SRC_LANG\n\n    model_inputs = tokenizer(\n        batch[\"src_text\"],\n        max_length=MAX_SRC_LEN,\n        truncation=True,\n        padding=False,\n    )\n\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            batch[\"tgt_text\"],\n            max_length=MAX_TGT_LEN,\n            truncation=True,\n            padding=False,\n        )\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntrain_tok = ds[\"train\"].map(preprocess_batch, batched=True, remove_columns=ds[\"train\"].column_names)\nval_tok   = ds[\"validation\"].map(preprocess_batch, batched=True, remove_columns=ds[\"validation\"].column_names)\ntest_tok  = ds[\"test\"].map(preprocess_batch, batched=True, remove_columns=ds[\"test\"].column_names)\n\nprint(\"[OK] Tokenized sizes:\",\n      \"train\", len(train_tok),\n      \"| val\", len(val_tok),\n      \"| test\", len(test_tok))",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-16T17:10:27.509374Z",
     "iopub.execute_input": "2025-12-16T17:10:27.509620Z",
     "iopub.status.idle": "2025-12-16T17:10:32.520434Z",
     "shell.execute_reply.started": "2025-12-16T17:10:27.509598Z",
     "shell.execute_reply": "2025-12-16T17:10:32.519832Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Map:   0%|          | 0/9000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c62a985c6fd7478b9702b2291a8f298b"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3951: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f165a39dae694d7694edc1b3081f09eb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Map:   0%|          | 0/2943 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "31be91b2c45b41459c93d6be6c050e7c"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "[OK] Tokenized sizes: train 9000 | val 1000 | test 2943\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": "@torch.inference_mode()\ndef translate_en2vi(texts, num_beams=5, length_penalty=1.0, no_repeat_ngram_size=3, max_new_tokens=128):\n    model.eval()\n    tokenizer.src_lang = SRC_LANG\n    enc = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_SRC_LEN).to(model.device)\n    out = model.generate(\n        **enc,\n        forced_bos_token_id=tokenizer.lang_code_to_id[TGT_LANG],\n        num_beams=num_beams,\n        length_penalty=length_penalty,\n        no_repeat_ngram_size=no_repeat_ngram_size,\n        max_new_tokens=max_new_tokens,\n        early_stopping=True\n    )\n    return tokenizer.batch_decode(out, skip_special_tokens=True)\n\ndef bleu_on_test(max_samples=None):\n    srcs = ds[\"test\"][\"src_text\"]\n    refs = ds[\"test\"][\"tgt_text\"]\n\n    if max_samples is not None:\n        srcs = srcs[:max_samples]\n        refs = refs[:max_samples]\n\n    hyps = []\n    bs = 16 if torch.cuda.is_available() else 4\n    for i in range(0, len(srcs), bs):\n        hyps.extend(translate_en2vi(srcs[i:i+bs]))\n\n    bleu = sacrebleu.corpus_bleu(hyps, [refs]).score\n    print(f\"[TEST] sacreBLEU ({len(srcs)} samples): {bleu:.2f}\")\n    return bleu\n\n_ = bleu_on_test(max_samples=200)   # ƒë·ªïi None ƒë·ªÉ ch·∫°y full public_test",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-16T17:10:32.521147Z",
     "iopub.execute_input": "2025-12-16T17:10:32.521406Z",
     "iopub.status.idle": "2025-12-16T17:11:43.015692Z",
     "shell.execute_reply.started": "2025-12-16T17:10:32.521390Z",
     "shell.execute_reply": "2025-12-16T17:11:43.014996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "[TEST] sacreBLEU (200 samples): 26.20\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": "# üéØ CELL 13 ‚Äî Training v·ªõi Early Stopping v√† Best Model Selection\n\n## M·ª•c ƒë√≠ch\nTrain model mBART-50 tr√™n 10,000 m·∫´u v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u cho th√≠ nghi·ªám 2.\n\n## ƒêi·ªÉm kh√°c bi·ªát so v·ªõi Th√≠ nghi·ªám 1:\n\n| Tham s·ªë | Th√≠ nghi·ªám 1 (file 1) | Th√≠ nghi·ªám 2 (file n√†y) |\n|---------|------------------------|-------------------------|\n| D·ªØ li·ªáu training | 20,000 m·∫´u | 10,000 m·∫´u |\n| Train/Val split | 18K/2K | 9K/1K |\n| Learning rate | 3e-5 | 1e-5 (th·∫•p h∆°n 3x) |\n| S·ªë epochs | 3 | 7 |\n| Early stopping | Kh√¥ng | C√≥ (patience=2) |\n| Save strategy | No checkpoint | Save best by eval_loss |\n| BLEU cu·ªëi c√πng | 41.51 | 38.09 |\n\n## Chi·∫øn l∆∞·ª£c training:\n\n### 1. Lower Learning Rate (1e-5)\n- LR th·∫•p h∆°n ƒë·ªÉ training ·ªïn ƒë·ªãnh h∆°n v·ªõi √≠t data h∆°n\n- Tr√°nh overfitting khi ch·ªâ c√≥ 10K m·∫´u\n\n### 2. Early Stopping\n```python\nEarlyStoppingCallback(early_stopping_patience=2)\n```\n- D·ª´ng training n·∫øu validation loss kh√¥ng c·∫£i thi·ªán sau 2 epochs\n- Ti·∫øt ki·ªám th·ªùi gian v√† tr√°nh overfitting\n\n### 3. Best Model Selection\n```python\nload_best_model_at_end=True\nmetric_for_best_model=\"eval_loss\"\ngreater_is_better=False\nsave_total_limit=1  # ch·ªâ gi·ªØ checkpoint t·ªët nh·∫•t\n```\n- Model cu·ªëi c√πng l√† checkpoint c√≥ val_loss th·∫•p nh·∫•t\n- Kh√¥ng ph·∫£i model ·ªü epoch cu·ªëi\n\n## K·∫øt qu·∫£ Training:\n\n### Loss theo epochs:\n| Epoch | Train Loss | Val Loss | C·∫£i thi·ªán |\n|-------|------------|----------|-----------|\n| 1 | 1.9404 | 1.7236 | - |\n| 2 | 1.5553 | 1.6193 | 6.1% |\n| 3 | 1.3820 | 1.5737 | 2.8% |\n| 4 | 1.2750 | 1.5550 | 1.2% |\n| **5** | **1.1969** | **1.5457** | **0.6%** ‚Üê Best |\n| 6 | 1.1411 | 1.5458 | 0.0% |\n| 7 | 1.1068 | 1.5457 | 0.0% |\n\n### Quan s√°t:\n- **Best epoch**: Epoch 5 v·ªõi val_loss = 1.5457\n- **Plateau**: Val loss kh√¥ng c·∫£i thi·ªán t·ª´ epoch 5‚Üí7\n- **Training time**: ~3.2 gi·ªù tr√™n Tesla T4\n- **Final BLEU**: 38.09 (th·∫•p h∆°n Th√≠ nghi·ªám 1 do √≠t data h∆°n)\n\n## Files ƒë∆∞·ª£c l∆∞u:\n1. `mbart50_envi/` - Model weights (best checkpoint)\n2. `train_val_loss.tsv` - Training history\n3. Pushed l√™n HuggingFace: `ngothuyet/mbart50-envi-e7`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\n\nuser_secrets = UserSecretsClient()\n\n# t√™n secret c·ªßa anh l√† HF_TOKEN (ƒë√∫ng nh∆∞ panel)\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\n\nlogin(token=hf_token)\nprint(\"[OK] Logged in to Hugging Face.\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-16T17:11:43.016452Z",
     "iopub.execute_input": "2025-12-16T17:11:43.016703Z",
     "iopub.status.idle": "2025-12-16T17:11:43.211103Z",
     "shell.execute_reply.started": "2025-12-16T17:11:43.016676Z",
     "shell.execute_reply": "2025-12-16T17:11:43.210479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "[OK] Logged in to Hugging Face.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": "# import torch\n# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# HF_REPO = \"ngothuyet/mbart50-envi-e7\"\n\n# tokenizer = AutoTokenizer.from_pretrained(HF_REPO, use_fast=False)\n# model = AutoModelForSeq2SeqLM.from_pretrained(HF_REPO)\n\n# tokenizer.src_lang = \"en_XX\"\n# tokenizer.tgt_lang = \"vi_VN\"\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model.to(device)\n\n# print(\"[OK] Loaded model from HF:\", HF_REPO)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-16T17:11:43.211732Z",
     "iopub.execute_input": "2025-12-16T17:11:43.211969Z",
     "iopub.status.idle": "2025-12-16T17:11:43.215076Z",
     "shell.execute_reply.started": "2025-12-16T17:11:43.211945Z",
     "shell.execute_reply": "2025-12-16T17:11:43.214490Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": "import os, inspect\nimport torch\nimport pandas as pd\nfrom transformers import (\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n    EarlyStoppingCallback,\n)\n\n# ====== CHECK ======\nassert \"model\" in globals(), \"Missing `model`\"\nassert \"tokenizer\" in globals(), \"Missing `tokenizer`\"\nassert \"train_tok\" in globals(), \"Missing `train_tok`\"\nassert \"val_tok\" in globals(), \"Missing `val_tok`\"\nassert \"SRC_LANG\" in globals() and \"TGT_LANG\" in globals(), \"Missing SRC_LANG/TGT_LANG\"\n\ntokenizer.src_lang = SRC_LANG\ntokenizer.tgt_lang = TGT_LANG\n\nOUTPUT_DIR  = \"mbart50_envi\"\nNUM_EPOCHS  = 7\n\nTRAIN_BS = 2\nEVAL_BS  = 2\nGRAD_ACC = 8\nLR       = 1e-5\n\nHF_REPO_ID = \"ngothuyet/mbart50-envi-e7\"\nHF_PRIVATE = True\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n\nta_kwargs = dict(\n    output_dir=OUTPUT_DIR,\n    overwrite_output_dir=True,\n\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=TRAIN_BS,\n    per_device_eval_batch_size=EVAL_BS,\n    gradient_accumulation_steps=GRAD_ACC,\n    learning_rate=LR,\n    warmup_ratio=0.03,\n    weight_decay=0.01,\n\n    predict_with_generate=False,\n    fp16=torch.cuda.is_available(),\n    dataloader_num_workers=2,\n    report_to=\"none\",\n    \n    # ‚úÖ SAVE BEST theo VAL LOSS (eval_loss)\n    save_strategy=\"epoch\",        # c·∫ßn save ƒë·ªÉ ch·ªçn best\n    save_only_model=True,         # tr√°nh l·ªói optimizer.pt\n    save_total_limit=1,           # ch·ªâ gi·ªØ 1 c√°i t·ªët nh·∫•t\n\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n)\n\nsig = inspect.signature(Seq2SeqTrainingArguments.__init__).parameters\n\nif \"logging_strategy\" in sig:\n    ta_kwargs[\"logging_strategy\"] = \"epoch\"\n\nif \"evaluation_strategy\" in sig:\n    ta_kwargs[\"evaluation_strategy\"] = \"epoch\"\nelif \"eval_strategy\" in sig:\n    ta_kwargs[\"eval_strategy\"] = \"epoch\"\nelse:\n    raise ValueError(\"Transformers version: missing evaluation_strategy/eval_strategy\")\n\nta_kwargs.update(dict(\n    push_to_hub=True,\n    hub_model_id=HF_REPO_ID,\n    hub_private_repo=HF_PRIVATE,\n    hub_strategy=\"end\",           # ch·ªâ push cu·ªëi\n))\n\ntraining_args = Seq2SeqTrainingArguments(**ta_kwargs)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_tok,\n    eval_dataset=val_tok,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)], \n)\n\nprint(\"[INFO] Start training...\")\ntrain_result = trainer.train()\nprint(\"\\n===== TRAIN DONE =====\")\nprint(train_result)\n\nprint(\"\\n===== BEST (by eval_loss) =====\")\nprint(\"best_model_checkpoint:\", trainer.state.best_model_checkpoint)\nprint(\"best_metric(eval_loss):\", trainer.state.best_metric)\n\nprint(\"\\n===== FINAL EVAL (VAL) on BEST =====\")\nfinal_metrics = trainer.evaluate()\nfor k, v in final_metrics.items():\n    print(f\"{k}: {v}\")\n\n# L∆∞u BEST local (trainer.model ƒë√£ l√† best v√¨ load_best_model_at_end=True)\ntrainer.save_model(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\nprint(\"[OK] Saved BEST model/tokenizer to:\", OUTPUT_DIR)\n\n# TSV train/val loss nh∆∞ c≈©\nlogs = trainer.state.log_history\ntrain_loss_by_epoch = {}\nval_loss_by_epoch = {}\n\nfor row in logs:\n    if \"epoch\" in row and \"loss\" in row and \"eval_loss\" not in row:\n        train_loss_by_epoch[row[\"epoch\"]] = row[\"loss\"]\n    if \"epoch\" in row and \"eval_loss\" in row:\n        val_loss_by_epoch[row[\"epoch\"]] = row[\"eval_loss\"]\n\nepochs = sorted(set(list(train_loss_by_epoch.keys()) + list(val_loss_by_epoch.keys())))\ndf = pd.DataFrame({\n    \"epoch\": epochs,\n    \"train_loss\": [train_loss_by_epoch.get(e, None) for e in epochs],\n    \"val_loss\": [val_loss_by_epoch.get(e, None) for e in epochs],\n})\n\ntsv_path = os.path.join(OUTPUT_DIR, \"train_val_loss.tsv\")\ndf.to_csv(tsv_path, sep=\"\\t\", index=False)\nprint(\"[OK] Saved TSV:\", tsv_path)\ndisplay(df)\n\n# ‚úÖ Push BEST + TSV (kh√¥ng c√≤n push ‚Äúb·∫£n cu·ªëi‚Äù theo nghƒ©a last-epoch)\ntrainer.push_to_hub(commit_message=f\"Push BEST by eval_loss={trainer.state.best_metric} + train_val_loss.tsv\")\nprint(\"[OK] Pushed BEST model + TSV to Hugging Face.\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-16T17:11:43.215885Z",
     "iopub.execute_input": "2025-12-16T17:11:43.216108Z",
     "iopub.status.idle": "2025-12-16T20:23:14.605930Z",
     "shell.execute_reply.started": "2025-12-16T17:11:43.216083Z",
     "shell.execute_reply": "2025-12-16T20:23:14.605263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "[INFO] Start training...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/tmp/ipykernel_47/1399803177.py:82: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1974' max='1974' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1974/1974 3:09:03, Epoch 7/7]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.940400</td>\n      <td>1.723647</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.555300</td>\n      <td>1.619265</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.382000</td>\n      <td>1.573678</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.275000</td>\n      <td>1.555019</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.196900</td>\n      <td>1.545686</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.141100</td>\n      <td>1.545778</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.106800</td>\n      <td>1.547994</td>\n    </tr>\n  </tbody>\n</table><p>"
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3685: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n===== TRAIN DONE =====\nTrainOutput(global_step=1974, training_loss=1.3710759867288422, metrics={'train_runtime': 11349.5282, 'train_samples_per_second': 5.551, 'train_steps_per_second': 0.174, 'total_flos': 7306723507765248.0, 'train_loss': 1.3710759867288422, 'epoch': 7.0})\n\n===== BEST (by eval_loss) =====\nbest_model_checkpoint: mbart50_envi/checkpoint-1410\nbest_metric(eval_loss): 1.5456864833831787\n\n===== FINAL EVAL (VAL) on BEST =====\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 01:09]\n    </div>\n    "
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "eval_loss: 1.5456864833831787\neval_runtime: 70.183\neval_samples_per_second: 14.248\neval_steps_per_second: 3.562\nepoch: 7.0\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Processing Files (0 / 0): |          |  0.00B /  0.00B            ",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9a4702748950408badda905bd6f6e4d8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "New Data Upload: |          |  0.00B /  0.00B            ",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5dab793444e943d8bf50aef5cfa72e48"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "[OK] Saved BEST model/tokenizer to: mbart50_envi\n[OK] Saved TSV: mbart50_envi/train_val_loss.tsv\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "   epoch  train_loss  val_loss\n0    1.0      1.9404  1.723647\n1    2.0      1.5553  1.619265\n2    3.0      1.3820  1.573678\n3    4.0      1.2750  1.555019\n4    5.0      1.1969  1.545686\n5    6.0      1.1411  1.545778\n6    7.0      1.1068  1.545686",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>val_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>1.9404</td>\n      <td>1.723647</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.0</td>\n      <td>1.5553</td>\n      <td>1.619265</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.0</td>\n      <td>1.3820</td>\n      <td>1.573678</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.0</td>\n      <td>1.2750</td>\n      <td>1.555019</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>1.1969</td>\n      <td>1.545686</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6.0</td>\n      <td>1.1411</td>\n      <td>1.545778</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7.0</td>\n      <td>1.1068</td>\n      <td>1.545686</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Processing Files (0 / 0): |          |  0.00B /  0.00B            ",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0191c034ab774901898e98686da7ea35"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "New Data Upload: |          |  0.00B /  0.00B            ",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "28acfe1e88964007881ab83a124f51e9"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "[OK] Pushed BEST model + TSV to Hugging Face.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": "# l·∫•y log_history v√† gom theo epoch\nlogs = trainer.state.log_history\n\ntrain_loss_by_epoch = {}\nval_loss_by_epoch = {}\n\nfor row in logs:\n    if \"epoch\" in row and \"loss\" in row and \"eval_loss\" not in row:\n        train_loss_by_epoch[row[\"epoch\"]] = row[\"loss\"]\n    if \"epoch\" in row and \"eval_loss\" in row:\n        val_loss_by_epoch[row[\"epoch\"]] = row[\"eval_loss\"]\n\nepochs = sorted(set(list(train_loss_by_epoch.keys()) + list(val_loss_by_epoch.keys())))\n\ndf = pd.DataFrame({\n    \"epoch\": epochs,\n    \"train_loss\": [train_loss_by_epoch.get(e, None) for e in epochs],\n    \"val_loss\": [val_loss_by_epoch.get(e, None) for e in epochs],\n})\n\ntsv_path = f\"{OUTPUT_DIR}/train_val_loss.tsv\"\ndf.to_csv(tsv_path, sep=\"\\t\", index=False)\n\nprint(\"[OK] Saved TSV:\", tsv_path)\ndisplay(df)\n\n# push TSV l√™n hub (n·∫±m trong output_dir s·∫Ω ƒë∆∞·ª£c push)\ntrainer.push_to_hub(commit_message=\"Add train_val_loss.tsv\")\nprint(\"[OK] Pushed TSV to hub.\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-16T20:23:14.606801Z",
     "iopub.execute_input": "2025-12-16T20:23:14.607071Z",
     "iopub.status.idle": "2025-12-16T20:23:40.566638Z",
     "shell.execute_reply.started": "2025-12-16T20:23:14.607027Z",
     "shell.execute_reply": "2025-12-16T20:23:40.566024Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "[OK] Saved TSV: mbart50_envi/train_val_loss.tsv\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "   epoch  train_loss  val_loss\n0    1.0      1.9404  1.723647\n1    2.0      1.5553  1.619265\n2    3.0      1.3820  1.573678\n3    4.0      1.2750  1.555019\n4    5.0      1.1969  1.545686\n5    6.0      1.1411  1.545778\n6    7.0      1.1068  1.545686",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>val_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>1.9404</td>\n      <td>1.723647</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.0</td>\n      <td>1.5553</td>\n      <td>1.619265</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.0</td>\n      <td>1.3820</td>\n      <td>1.573678</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.0</td>\n      <td>1.2750</td>\n      <td>1.555019</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>1.1969</td>\n      <td>1.545686</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6.0</td>\n      <td>1.1411</td>\n      <td>1.545778</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7.0</td>\n      <td>1.1068</td>\n      <td>1.545686</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Processing Files (0 / 0): |          |  0.00B /  0.00B            ",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0289a94bd1da47bfa2145e36e3b3ae83"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "New Data Upload: |          |  0.00B /  0.00B            ",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b74c77d7098146ebbdd4d22140ec5e7d"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "No files have been modified since last commit. Skipping to prevent empty commit.\nWARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "[OK] Pushed TSV to hub.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": "# üìà CELL 17 ‚Äî T√≠nh BLEU tr√™n Public Test v√† Push l√™n Hub\n\n## M·ª•c ƒë√≠ch\nƒê√°nh gi√° model ƒë√£ train tr√™n to√†n b·ªô t·∫≠p public test (2,943 c√¢u) v√† l∆∞u k·∫øt qu·∫£ l√™n Hugging Face.\n\n## H√†m `bleu_on_public_test_and_push()`\n\n### Quy tr√¨nh ƒë√°nh gi√°:\n\n1. **Load d·ªØ li·ªáu test**\n   ```python\n   srcs = ds[\"test\"][\"src_text\"]  # 2,943 c√¢u ti·∫øng Anh\n   refs = ds[\"test\"][\"tgt_text\"]  # 2,943 reference ti·∫øng Vi·ªát\n   ```\n\n2. **D·ªãch theo batch**\n   - Batch size: 16 (tr√™n GPU)\n   - D√πng beam search v·ªõi num_beams=5\n   - Progress bar v·ªõi tqdm\n\n3. **T√≠nh BLEU**\n   ```python\n   bleu = sacrebleu.corpus_bleu(hyps, [refs]).score\n   ```\n   - Corpus BLEU: ƒë√°nh gi√° tr√™n to√†n b·ªô t·∫≠p, kh√¥ng ph·∫£i trung b√¨nh\n   - Ch√≠nh x√°c h∆°n sentence-level BLEU\n\n4. **L∆∞u k·∫øt qu·∫£**\n   - `public_test_metrics.json`: BLEU, size, batch_size, time\n   - `public_test_bleu.txt`: Ch·ªâ ƒëi·ªÉm BLEU\n   - Push files l√™n Hugging Face repo\n\n## K·∫øt qu·∫£ Th√≠ nghi·ªám 2:\n\n```\n[PUBLIC_TEST] sacreBLEU (2943 samples): 38.09\n```\n\n### So s√°nh v·ªõi Th√≠ nghi·ªám 1:\n\n| Metric | Th√≠ nghi·ªám 1 | Th√≠ nghi·ªám 2 | Ch√™nh l·ªách |\n|--------|--------------|--------------|------------|\n| Data size | 20K | 10K | -50% |\n| Epochs | 3 | 7 (best at 5) | +133% |\n| Learning rate | 3e-5 | 1e-5 | -67% |\n| BLEU score | **41.51** | **38.09** | **-3.42** |\n\n### Ph√¢n t√≠ch:\n- **T√°c ƒë·ªông d·ªØ li·ªáu**: Gi·∫£m 50% data ‚Üí gi·∫£m 8.2% BLEU\n- **Trade-off**: Nhi·ªÅu data quan tr·ªçng h∆°n nhi·ªÅu epochs\n- **Hi·ªáu qu·∫£**: 10K m·∫´u v·∫´n ƒë·∫°t 38.09 BLEU (v∆∞·ª£t baseline 26.20)\n\n## Files output:\n- `/kaggle/working/mbart50_envi/public_test_metrics.json`\n- `/kaggle/working/mbart50_envi/public_test_bleu.txt`\n- Pushed to: `https://huggingface.co/ngothuyet/mbart50-envi-e7`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch\nfrom typing import List\n\nMAX_SRC_LEN = 256\n\n@torch.inference_mode()\ndef translate_en2vi(\n    texts: List[str],\n    num_beams: int = 5,\n    length_penalty: float = 1.0,\n    no_repeat_ngram_size: int = 3,\n    max_new_tokens: int = 128,\n):\n    model.eval()\n    tokenizer.src_lang = SRC_LANG\n    tokenizer.tgt_lang = TGT_LANG\n\n    enc = tokenizer(\n        texts,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=MAX_SRC_LEN\n    ).to(model.device)\n\n    generated = model.generate(\n        **enc,\n        forced_bos_token_id=tokenizer.lang_code_to_id[TGT_LANG],\n        num_beams=num_beams,\n        length_penalty=length_penalty,\n        no_repeat_ngram_size=no_repeat_ngram_size,\n        max_new_tokens=max_new_tokens,\n        early_stopping=True,\n    )\n\n    return tokenizer.batch_decode(generated, skip_special_tokens=True)\n\n# Demo nhanh\ndemo = [\n    \"It's necessary to provide health insurance communication and education for people who live in remote areas and participate interupted health insurance.\",\n    \"I am learning natural language processing.\"\n]\nprint(\"[INPUT ]\", demo)\nprint(\"[OUTPUT]\", translate_en2vi(demo, num_beams=5, length_penalty=1.0, no_repeat_ngram_size=3))",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-16T20:23:40.567382Z",
     "iopub.execute_input": "2025-12-16T20:23:40.567635Z",
     "iopub.status.idle": "2025-12-16T20:23:41.348250Z",
     "shell.execute_reply.started": "2025-12-16T20:23:40.567608Z",
     "shell.execute_reply": "2025-12-16T20:23:41.347454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "[INPUT ] [\"It's necessary to provide health insurance communication and education for people who live in remote areas and participate interupted health insurance.\", 'I am learning natural language processing.']\n[OUTPUT] ['C·∫ßn cung c·∫•p d·ªãch v·ª• b·∫£o hi·ªÉm y t·∫ø, truy·ªÅn th√¥ng v√† gi√°o d·ª•c cho nh·ªØng ng∆∞·ªùi s·ªëng ·ªü nh·ªØng v√πng xa x√¥i v√† tham gia b·∫£o hi·ªÉm Y t·∫ø li√™n k·∫øt.', 'T√¥i ƒëang h·ªçc x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n.']\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": "from datasets import Dataset\n\n# ƒë·ªçc song song public_test.en/vi -> list[(en, vi)]\ntest_pairs_raw = load_parallel(TEST_EN, TEST_VI, \"test_raw\")\n\n# clean/filter/dedup gi·ªëng pipeline train\ntest_pairs = clean_filter_dedup(test_pairs_raw, \"test\")\n\nprint(f\"[INFO] public_test after clean: {len(test_pairs):,} pairs\")\nprint(\"[SAMPLE TEST]\", test_pairs[0])\n\n# t·∫°o ds_test (gi·ªØ nguy√™n ds train/val c·ªßa anh n·∫øu ƒë√£ c√≥)\ntest_ds = Dataset.from_dict({\n    \"src_text\": [s for s, _ in test_pairs],\n    \"tgt_text\": [t for _, t in test_pairs],\n})\n\n# g·∫Øn v√†o DatasetDict n·∫øu ƒë√£ c√≥ ds, c√≤n kh√¥ng th√¨ t·∫°o ds m·ªõi\nif \"ds\" in globals():\n    ds[\"test\"] = test_ds\nelse:\n    from datasets import DatasetDict\n    ds = DatasetDict({\"test\": test_ds})\n\nprint(\"[OK] ds['test'] size:\", len(ds[\"test\"]))",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-16T20:23:41.349135Z",
     "iopub.execute_input": "2025-12-16T20:23:41.349375Z",
     "iopub.status.idle": "2025-12-16T20:23:41.504200Z",
     "shell.execute_reply.started": "2025-12-16T20:23:41.349356Z",
     "shell.execute_reply": "2025-12-16T20:23:41.503409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "[INFO] test_raw lines -> 3,000 pairs.\n[INFO] test: after clean+filter -> 2,946 pairs\n[INFO] test: after dedup       -> 2,943 pairs\n[INFO] public_test after clean: 2,943 pairs\n[SAMPLE TEST] ('Knowledge, practices in public health service utilization among health insurance card‚Äôs holders and influencing factors in Vientiane, Lao', 'Th·ª±c tr·∫°ng ki·∫øn th·ª©c v√† th·ª±c h√†nh c·ªßa ng∆∞·ªùi c√≥ th·∫ª b·∫£o hi·ªÉm y t·∫ø trong s·ª≠ d·ª•ng d·ªãch v·ª• kh√°m ch·ªØa b·ªánh ·ªü c√°c c∆° s·ªü y t·∫ø c√¥ng v√† m·ªôt s·ªë y·∫øu t·ªë ·∫£nh h∆∞·ªüng t·∫°i t·ªânh Vi√™ng ChƒÉn, CHDCND L√†o, nƒÉm 2017')\n[OK] ds['test'] size: 2943\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": "import os, json, time\nimport torch\nimport sacrebleu\nfrom tqdm.auto import tqdm\n\nassert \"ds\" in globals() and \"test\" in ds, \"Missing ds['test'] - run TEST-LOAD cell first.\"\nassert \"translate_en2vi\" in globals(), \"Missing translate_en2vi() - run inference cell first.\"\nassert \"OUTPUT_DIR\" in globals(), \"Missing OUTPUT_DIR (e.g., 'mbart50_envi').\"\nassert \"trainer\" in globals(), \"Missing trainer - run training cell first (to push easily).\"\n\ndef bleu_on_public_test_and_push(batch_size=16):\n    srcs = ds[\"test\"][\"src_text\"]\n    refs = ds[\"test\"][\"tgt_text\"]\n\n    bs = batch_size if torch.cuda.is_available() else max(2, batch_size // 4)\n\n    hyps = []\n    t0 = time.time()\n\n    for i in tqdm(range(0, len(srcs), bs), desc=\"BLEU on public_test\", total=(len(srcs) + bs - 1)//bs):\n        batch = srcs[i:i+bs]\n        hyps.extend(\n            translate_en2vi(\n                batch,\n                num_beams=5,\n                length_penalty=1.0,\n                no_repeat_ngram_size=3,\n                max_new_tokens=128\n            )\n        )\n\n    bleu = sacrebleu.corpus_bleu(hyps, [refs]).score\n    elapsed = time.time() - t0\n\n    metrics = {\n        \"public_test_sacrebleu\": float(bleu),\n        \"public_test_size\": int(len(srcs)),\n        \"batch_size\": int(bs),\n        \"elapsed_sec\": float(elapsed),\n    }\n\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n    json_path = os.path.join(OUTPUT_DIR, \"public_test_metrics.json\")\n    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(metrics, f, ensure_ascii=False, indent=2)\n\n    txt_path = os.path.join(OUTPUT_DIR, \"public_test_bleu.txt\")\n    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(f\"public_test_sacrebleu\\t{bleu:.4f}\\n\")\n\n    print(f\"\\n[PUBLIC_TEST] sacreBLEU ({len(srcs)} samples): {bleu:.2f}\")\n    print(\"[OK] Saved:\", json_path)\n    print(\"[OK] Saved:\", txt_path)\n\n    # push l√™n repo model\n    trainer.push_to_hub(commit_message=f\"Add public_test sacreBLEU={bleu:.2f}\")\n    print(\"[OK] Pushed public_test BLEU files to Hugging Face.\")\n\n    return bleu, metrics\n\n_ = bleu_on_public_test_and_push(batch_size=16)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-16T20:23:41.505018Z",
     "iopub.execute_input": "2025-12-16T20:23:41.505336Z",
     "iopub.status.idle": "2025-12-16T20:43:09.589454Z",
     "shell.execute_reply.started": "2025-12-16T20:23:41.505317Z",
     "shell.execute_reply": "2025-12-16T20:43:09.588800Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "BLEU on public_test:   0%|          | 0/184 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2cd45016f1344b99a5f2d577bccf83b3"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\n[PUBLIC_TEST] sacreBLEU (2943 samples): 38.09\n[OK] Saved: mbart50_envi/public_test_metrics.json\n[OK] Saved: mbart50_envi/public_test_bleu.txt\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Processing Files (0 / 0): |          |  0.00B /  0.00B            ",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "341aefa189254fd3b4bf26cf6e1f90fc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "New Data Upload: |          |  0.00B /  0.00B            ",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e43af965f8384091bf5065b6c2855868"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "[OK] Pushed public_test BLEU files to Hugging Face.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 18
  }
 ]
}