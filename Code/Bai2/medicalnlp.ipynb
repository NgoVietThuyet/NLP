{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-13T04:14:23.969234Z",
     "iopub.status.busy": "2025-12-13T04:14:23.968968Z",
     "iopub.status.idle": "2025-12-13T04:14:27.560301Z",
     "shell.execute_reply": "2025-12-13T04:14:27.559595Z",
     "shell.execute_reply.started": "2025-12-13T04:14:23.969211Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Device: cuda\n",
      "CKPT_PATH: /kaggle/input/modeltrained/best_transformer_en_vi (3).pt\n",
      "SRC_PATH: /kaggle/input/databaitoanphu/train.en.txt\n",
      "TGT_PATH: /kaggle/input/databaitoanphu/train.vi.txt\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# --- CELL 1: SETUP ---\n",
    "# =========================\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tqdm\n",
    "import warnings\n",
    "\n",
    "# C√†i ƒë·∫∑t th∆∞ vi·ªán tokenizers n·∫øu ch∆∞a c√≥\n",
    "try:\n",
    "    from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers\n",
    "except ImportError:\n",
    "    !pip -q install tokenizers\n",
    "    from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Fix seed ---\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"‚úÖ Device:\", device)\n",
    "\n",
    "# checkpoint best_transformer_en_vi.pt l√† tr·∫°ng th√°i c·ªßa model khi ƒë√£ train xong t·∫°i b√†i to√°n ch√≠nh.\n",
    "\n",
    "CKPT_PATH = \"/kaggle/input/modeltrained/best_transformer_en_vi (3).pt\"\n",
    "\n",
    "# data m·ªõi train.en/train.vi l√† data trong b√†i to√°n ph·ª•\n",
    "DATA_DIR = \"/kaggle/input/databaitoanphu\"\n",
    "\n",
    "SRC_PATH = os.path.join(DATA_DIR, \"train.en.txt\")\n",
    "TGT_PATH = os.path.join(DATA_DIR, \"train.vi.txt\")\n",
    "\n",
    "print(\"CKPT_PATH:\", CKPT_PATH)\n",
    "print(\"SRC_PATH:\", SRC_PATH)\n",
    "print(\"TGT_PATH:\", TGT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T04:14:27.562711Z",
     "iopub.status.busy": "2025-12-13T04:14:27.562106Z",
     "iopub.status.idle": "2025-12-13T04:14:27.568270Z",
     "shell.execute_reply": "2025-12-13T04:14:27.567450Z",
     "shell.execute_reply.started": "2025-12-13T04:14:27.562689Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# --- CELL 2: DATA READING FUNCTION ---\n",
    "# ==================================\n",
    "def read_parallel_data(src_path, tgt_path, max_lines=None):\n",
    "    pairs = []\n",
    "    with open(src_path, \"r\", encoding=\"utf-8\") as fsrc, open(tgt_path, \"r\", encoding=\"utf-8\") as ftgt:\n",
    "        for i, (s, t) in enumerate(zip(fsrc, ftgt)):\n",
    "            if max_lines and i >= max_lines:\n",
    "                break\n",
    "            s, t = s.strip(), t.strip()\n",
    "            if s and t:\n",
    "                pairs.append((s, t))\n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T04:14:27.569418Z",
     "iopub.status.busy": "2025-12-13T04:14:27.569138Z",
     "iopub.status.idle": "2025-12-13T04:14:29.670737Z",
     "shell.execute_reply": "2025-12-13T04:14:29.669971Z",
     "shell.execute_reply.started": "2025-12-13T04:14:27.569392Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LOAD DATA M·ªöI (train.en / train.vi) ---\n",
      "T·ªïng s·ªë c·∫∑p c√¢u: 500000\n",
      "Train pairs: 450000\n",
      "Val pairs  : 50000\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# --- CELL 3: LOAD DATA ---\n",
    "# =========================\n",
    "print(\"\\n--- LOAD DATA M·ªöI (train.en / train.vi) ---\")\n",
    "\n",
    "all_pairs = read_parallel_data(SRC_PATH, TGT_PATH, max_lines=None)\n",
    "print(\"T·ªïng s·ªë c·∫∑p c√¢u:\", len(all_pairs))\n",
    "\n",
    "# Shuffle + split 90% train, 10% val\n",
    "random.shuffle(all_pairs)\n",
    "split_idx = int(0.9 * len(all_pairs))\n",
    "train_pairs = all_pairs[:split_idx]\n",
    "val_pairs   = all_pairs[split_idx:]\n",
    "\n",
    "print(\"Train pairs:\", len(train_pairs))\n",
    "print(\"Val pairs  :\", len(val_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T04:14:29.671711Z",
     "iopub.status.busy": "2025-12-13T04:14:29.671512Z",
     "iopub.status.idle": "2025-12-13T04:14:57.358463Z",
     "shell.execute_reply": "2025-12-13T04:14:57.357807Z",
     "shell.execute_reply.started": "2025-12-13T04:14:29.671696Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- HU·∫§N LUY·ªÜN TOKENIZER ---\n",
      "‚úÖ Tokenizer ƒë√£ s·∫µn s√†ng.\n",
      "PAD_ID: 0 START_ID: 1 END_ID: 2\n",
      "SRC vocab: 10000 TGT vocab: 10000\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# --- CELL 4: TRAIN TOKENIZERS ---\n",
    "# ==============================\n",
    "print(\"\\n--- HU·∫§N LUY·ªÜN TOKENIZER ---\")\n",
    "\n",
    "def train_bpe_tokenizer(texts, vocab_size=8000):\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "    \n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=[\"[PAD]\", \"[START]\", \"[END]\", \"[UNK]\"],\n",
    "        show_progress=False\n",
    "    )\n",
    "    tokenizer.train_from_iterator(texts, trainer=trainer)\n",
    "    return tokenizer\n",
    "\n",
    "all_src_text = [p[0] for p in train_pairs + val_pairs]\n",
    "all_tgt_text = [p[1] for p in train_pairs + val_pairs]\n",
    "\n",
    "if not all_src_text:  # Dummy data n·∫øu ch∆∞a load ƒë∆∞·ª£c file\n",
    "    all_src_text = [\"Hello world\"]\n",
    "    all_tgt_text = [\"Xin ch√†o\"]\n",
    "\n",
    "en_tokenizer = train_bpe_tokenizer(all_src_text, vocab_size=10000)\n",
    "vi_tokenizer = train_bpe_tokenizer(all_tgt_text, vocab_size=10000)\n",
    "\n",
    "# L·∫•y ID c√°c token ƒë·∫∑c bi·ªát\n",
    "PAD_ID = en_tokenizer.token_to_id(\"[PAD]\")\n",
    "START_ID = vi_tokenizer.token_to_id(\"[START]\")\n",
    "END_ID = vi_tokenizer.token_to_id(\"[END]\")\n",
    "\n",
    "print(\"Tokenizer ƒë√£ s·∫µn s√†ng.\")\n",
    "print(\"PAD_ID:\", PAD_ID, \"START_ID:\", START_ID, \"END_ID:\", END_ID)\n",
    "print(\"SRC vocab:\", en_tokenizer.get_vocab_size(), \"TGT vocab:\", vi_tokenizer.get_vocab_size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T04:14:57.359570Z",
     "iopub.status.busy": "2025-12-13T04:14:57.359362Z",
     "iopub.status.idle": "2025-12-13T04:14:57.365954Z",
     "shell.execute_reply": "2025-12-13T04:14:57.365189Z",
     "shell.execute_reply.started": "2025-12-13T04:14:57.359552Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# --- CELL 5: DATASET CLASS ---\n",
    "# ===========================\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, pairs, src_tokenizer, tgt_tokenizer, max_len=64):\n",
    "        self.pairs = pairs\n",
    "        self.src_tok = src_tokenizer\n",
    "        self.tgt_tok = tgt_tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src, tgt = self.pairs[idx]\n",
    "\n",
    "        src_ids = self.src_tok.encode(src).ids[:self.max_len]\n",
    "        tgt_ids = self.tgt_tok.encode(tgt).ids[:self.max_len]\n",
    "\n",
    "        # Th√™m START/END cho tgt\n",
    "        tgt_ids = [START_ID] + tgt_ids + [END_ID]\n",
    "\n",
    "        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(tgt_ids, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_batch = nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=PAD_ID)\n",
    "    tgt_batch = nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=PAD_ID)\n",
    "    return src_batch, tgt_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T04:14:57.366970Z",
     "iopub.status.busy": "2025-12-13T04:14:57.366706Z",
     "iopub.status.idle": "2025-12-13T04:14:57.388773Z",
     "shell.execute_reply": "2025-12-13T04:14:57.388217Z",
     "shell.execute_reply.started": "2025-12-13T04:14:57.366948Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DataLoader ready.\n",
      "Train batches: 7031 Val batches: 782\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# --- CELL 6: DATALOADERS ---\n",
    "# ============================\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_ds = TranslationDataset(train_pairs, en_tokenizer, vi_tokenizer, max_len=64)\n",
    "val_ds   = TranslationDataset(val_pairs, en_tokenizer, vi_tokenizer, max_len=64)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(\"‚úÖ DataLoader ready.\")\n",
    "print(\"Train batches:\", len(train_loader), \"Val batches:\", len(val_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T04:14:57.391223Z",
     "iopub.status.busy": "2025-12-13T04:14:57.390930Z",
     "iopub.status.idle": "2025-12-13T04:14:57.440539Z",
     "shell.execute_reply": "2025-12-13T04:14:57.439826Z",
     "shell.execute_reply.started": "2025-12-13T04:14:57.391206Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# --- CELL 7: MODEL COMPONENTS ---\n",
    "# ===================================\n",
    "# --- Rotary Positional Embeddings ---\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(x, cos, sin):\n",
    "    return (x * cos) + (rotate_half(x) * sin)\n",
    "\n",
    "class RotaryPositionalEncoding(nn.Module):\n",
    "    def __init__(self, head_dim, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "        t = torch.arange(max_seq_len).float()\n",
    "        freqs = torch.outer(t, inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos\", emb.cos()[None, None, :, :])\n",
    "        self.register_buffer(\"sin\", emb.sin()[None, None, :, :])\n",
    "\n",
    "    def forward(self, x, seq_len):\n",
    "        return self.cos[:, :, :seq_len, :], self.sin[:, :, :seq_len, :]\n",
    "\n",
    "# --- SwiGLU ---\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, hidden_dim, ffn_dim):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(hidden_dim, ffn_dim)\n",
    "        self.w2 = nn.Linear(hidden_dim, ffn_dim)\n",
    "        self.w3 = nn.Linear(ffn_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w3(F.silu(self.w1(x)) * self.w2(x))\n",
    "\n",
    "# --- GQA Attention ---\n",
    "class GQA(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, num_kv_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        self.num_groups = num_heads // num_kv_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, num_kv_heads * self.head_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, num_kv_heads * self.head_dim)\n",
    "        self.o_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, enc_out=None, mask=None, rope_cos=None, rope_sin=None):\n",
    "        batch, seq_len, _ = x.shape\n",
    "        kv_input = enc_out if enc_out is not None else x\n",
    "\n",
    "        q = self.q_proj(x).view(batch, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(kv_input).view(batch, -1, self.num_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(kv_input).view(batch, -1, self.num_kv_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        if rope_cos is not None:\n",
    "            q = apply_rotary_pos_emb(q, rope_cos, rope_sin)\n",
    "            k = apply_rotary_pos_emb(k, rope_cos, rope_sin)\n",
    "\n",
    "        k = k.repeat_interleave(self.num_groups, dim=1)\n",
    "        v = v.repeat_interleave(self.num_groups, dim=1)\n",
    "\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores + mask\n",
    "\n",
    "        attn = F.softmax(attn_scores, dim=-1)\n",
    "        attn = F.dropout(attn, p=self.dropout, training=self.training)\n",
    "\n",
    "        out = attn @ v\n",
    "        out = out.transpose(1, 2).contiguous().view(batch, seq_len, -1)\n",
    "        return self.o_proj(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T04:14:57.441362Z",
     "iopub.status.busy": "2025-12-13T04:14:57.441177Z",
     "iopub.status.idle": "2025-12-13T04:14:57.456918Z",
     "shell.execute_reply": "2025-12-13T04:14:57.456262Z",
     "shell.execute_reply.started": "2025-12-13T04:14:57.441338Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# --- CELL 8: TRANSFORMER MODEL ---\n",
    "# ==================================\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, num_kv_heads, dropout=0.1, is_decoder=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.RMSNorm(hidden_dim)\n",
    "        self.attn = GQA(hidden_dim, num_heads, num_kv_heads, dropout)\n",
    "        self.is_decoder = is_decoder\n",
    "        if is_decoder:\n",
    "            self.norm2 = nn.RMSNorm(hidden_dim)\n",
    "            self.cross_attn = GQA(hidden_dim, num_heads, num_kv_heads, dropout)\n",
    "        self.norm_ffn = nn.RMSNorm(hidden_dim)\n",
    "        self.ffn = SwiGLU(hidden_dim, hidden_dim * 4)\n",
    "\n",
    "    def forward(self, x, enc_out=None, mask=None, cross_mask=None, rope_cos=None, rope_sin=None):\n",
    "        x = x + self.attn(self.norm1(x), mask=mask, rope_cos=rope_cos, rope_sin=rope_sin)\n",
    "        if self.is_decoder:\n",
    "            x = x + self.cross_attn(self.norm2(x), enc_out=enc_out, mask=cross_mask)\n",
    "        x = x + self.ffn(self.norm_ffn(x))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab, tgt_vocab, hidden_dim=256, num_layers=4, num_heads=8, num_kv_heads=4):\n",
    "        super().__init__()\n",
    "        self.src_emb = nn.Embedding(src_vocab, hidden_dim)\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab, hidden_dim)\n",
    "        self.rope = RotaryPositionalEncoding(hidden_dim // num_heads)\n",
    "        \n",
    "        self.encoders = nn.ModuleList([\n",
    "            TransformerBlock(hidden_dim, num_heads, num_kv_heads, is_decoder=False)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.decoders = nn.ModuleList([\n",
    "            TransformerBlock(hidden_dim, num_heads, num_kv_heads, is_decoder=True)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.final_norm = nn.RMSNorm(hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, tgt_vocab)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        x = self.src_emb(src)\n",
    "        rope_cos, rope_sin = self.rope(x, x.shape[1])\n",
    "        for layer in self.encoders:\n",
    "            x = layer(x, mask=src_mask, rope_cos=rope_cos, rope_sin=rope_sin)\n",
    "        enc_out = x\n",
    "        \n",
    "        x = self.tgt_emb(tgt)\n",
    "        rope_cos_tgt, rope_sin_tgt = self.rope(x, x.shape[1])\n",
    "        for layer in self.decoders:\n",
    "            x = layer(x, enc_out=enc_out, mask=tgt_mask, cross_mask=src_mask, rope_cos=rope_cos_tgt, rope_sin=rope_sin_tgt)\n",
    "        return self.fc_out(self.final_norm(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T04:14:57.457915Z",
     "iopub.status.busy": "2025-12-13T04:14:57.457669Z",
     "iopub.status.idle": "2025-12-13T04:15:00.490807Z",
     "shell.execute_reply": "2025-12-13T04:15:00.490217Z",
     "shell.execute_reply.started": "2025-12-13T04:14:57.457889Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# --- CELL 9: INIT TRAINING ---\n",
    "# ============================\n",
    "def create_masks(src, tgt):\n",
    "    src_mask = (src == PAD_ID).unsqueeze(1).unsqueeze(2).float() * -1e9\n",
    "    batch, seq_len = tgt.shape\n",
    "    causal = torch.triu(torch.full((seq_len, seq_len), float('-inf'), device=device), diagonal=1)\n",
    "    tgt_pad = (tgt == PAD_ID).unsqueeze(1).unsqueeze(2).float() * -1e9\n",
    "    return src_mask, causal + tgt_pad\n",
    "\n",
    "model = Transformer(\n",
    "    src_vocab=en_tokenizer.get_vocab_size(),\n",
    "    tgt_vocab=vi_tokenizer.get_vocab_size(),\n",
    "    hidden_dim=256, num_layers=4, num_heads=8, num_kv_heads=4\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID, label_smoothing=0.1)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.0001)\n",
    "\n",
    "print(\"Model Initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T04:15:00.491946Z",
     "iopub.status.busy": "2025-12-13T04:15:00.491548Z",
     "iopub.status.idle": "2025-12-13T04:15:02.578774Z",
     "shell.execute_reply": "2025-12-13T04:15:02.578028Z",
     "shell.execute_reply.started": "2025-12-13T04:15:00.491924Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LOAD best_transformer_en_vi.pt & TRAIN TI·∫æP ---\n",
      "‚úÖ Loaded checkpoint from: /kaggle/input/modeltrained/best_transformer_en_vi (3).pt\n",
      "   start_epoch = 7\n",
      "   prev_best_val_loss = 3.338132079766721\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# --- CELL 10: LOAD BEST CHECKPOINT + RESUME ---\n",
    "# ==========================================\n",
    "print(\"\\n--- LOAD best_transformer_en_vi.pt & TRAIN TI·∫æP ---\")\n",
    "\n",
    "ckpt = torch.load(CKPT_PATH, map_location=device)\n",
    "\n",
    "state_dict = ckpt[\"model_state_dict\"]\n",
    "\n",
    "# B·ªé 2 buffer RoPE b·ªã l·ªách shape gi·ªØa checkpoint v√† model hi·ªán t·∫°i\n",
    "for k in [\"rope.cos\", \"rope.sin\"]:\n",
    "    if k in state_dict:\n",
    "        state_dict.pop(k)\n",
    "\n",
    "# load v·ªõi strict=False ƒë·ªÉ ch·∫•p nh·∫≠n missing ƒë√∫ng 2 key tr√™n\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "# resume optimizer n·∫øu anh mu·ªën train ti·∫øp ƒë√∫ng tr·∫°ng th√°i (khuy·∫øn ngh·ªã)\n",
    "if \"optimizer_state_dict\" in ckpt:\n",
    "    optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
    "    # ƒë·∫£m b·∫£o optimizer states n·∫±m ƒë√∫ng device\n",
    "    for state in optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if torch.is_tensor(v):\n",
    "                state[k] = v.to(device)\n",
    "\n",
    "start_epoch = ckpt.get(\"epoch\", 0)\n",
    "best_prev_val = ckpt.get(\"val_loss\", None)\n",
    "\n",
    "print(f\"‚úÖ Loaded checkpoint from: {CKPT_PATH}\")\n",
    "print(f\"   start_epoch = {start_epoch}\")\n",
    "print(f\"   prev_best_val_loss = {best_prev_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T04:15:02.579767Z",
     "iopub.status.busy": "2025-12-13T04:15:02.579565Z",
     "iopub.status.idle": "2025-12-13T04:15:02.585901Z",
     "shell.execute_reply": "2025-12-13T04:15:02.585314Z",
     "shell.execute_reply.started": "2025-12-13T04:15:02.579744Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# --- CELL 11: EARLY STOP ---\n",
    "# =========================\n",
    "import math\n",
    "import torch\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=1e-4, mode=\"min\"):\n",
    "        \"\"\"\n",
    "        mode=\"min\": metric c√†ng nh·ªè c√†ng t·ªët (val_loss)\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "\n",
    "    def _is_improvement(self, current):\n",
    "        if self.best is None:\n",
    "            return True\n",
    "        if self.mode == \"min\":\n",
    "            return current < (self.best - self.min_delta)\n",
    "        else:\n",
    "            return current > (self.best + self.min_delta)\n",
    "\n",
    "    def step(self, current):\n",
    "        if self._is_improvement(current):\n",
    "            self.best = current\n",
    "            return False\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "            print(f\"S·ªë epoch k√©m ch·∫•t l∆∞·ª£ng {self.num_bad_epochs}\" )\n",
    "            return self.num_bad_epochs >= self.patience\n",
    "\n",
    "\n",
    "def save_checkpoint(path, model, optimizer, epoch, val_loss):\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"val_loss\": val_loss,\n",
    "    }, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T04:19:53.288184Z",
     "iopub.status.busy": "2025-12-13T04:19:53.287851Z",
     "iopub.status.idle": "2025-12-13T10:57:00.426909Z",
     "shell.execute_reply": "2025-12-13T10:57:00.426177Z",
     "shell.execute_reply.started": "2025-12-13T04:19:53.288164Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN TI·∫æP ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7031/7031 [12:52<00:00,  9.11it/s, loss=2.7842]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 3.2456 | Val Loss: 2.7712 | Time: 00:13:24.62\n",
      "  ‚Ü≥ Saved BEST to best_transformer_en_vi_resume.pt (val_loss=2.7712)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7031/7031 [12:45<00:00,  9.18it/s, loss=2.5473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 2.6633 | Val Loss: 2.5809 | Time: 00:13:17.36\n",
      "  ‚Ü≥ Saved BEST to best_transformer_en_vi_resume.pt (val_loss=2.5809)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7031/7031 [12:39<00:00,  9.26it/s, loss=2.5249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 2.5061 | Val Loss: 2.4935 | Time: 00:13:10.75\n",
      "  ‚Ü≥ Saved BEST to best_transformer_en_vi_resume.pt (val_loss=2.4935)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7031/7031 [12:40<00:00,  9.24it/s, loss=2.3307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss: 2.4143 | Val Loss: 2.4400 | Time: 00:13:12.39\n",
      "  ‚Ü≥ Saved BEST to best_transformer_en_vi_resume.pt (val_loss=2.4400)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7031/7031 [12:41<00:00,  9.24it/s, loss=2.5297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Train Loss: 2.3491 | Val Loss: 2.4013 | Time: 00:13:13.24\n",
      "  ‚Ü≥ Saved BEST to best_transformer_en_vi_resume.pt (val_loss=2.4013)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7031/7031 [12:40<00:00,  9.25it/s, loss=2.3175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Train Loss: 2.2985 | Val Loss: 2.3724 | Time: 00:13:12.14\n",
      "  ‚Ü≥ Saved BEST to best_transformer_en_vi_resume.pt (val_loss=2.3724)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7031/7031 [12:37<00:00,  9.28it/s, loss=2.2185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Train Loss: 2.2576 | Val Loss: 2.3510 | Time: 00:13:09.97\n",
      "  ‚Ü≥ Saved BEST to best_transformer_en_vi_resume.pt (val_loss=2.3510)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7031/7031 [12:39<00:00,  9.26it/s, loss=2.2005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Train Loss: 2.2225 | Val Loss: 2.3366 | Time: 00:13:10.75\n",
      "  ‚Ü≥ Saved BEST to best_transformer_en_vi_resume.pt (val_loss=2.3366)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7031/7031 [12:41<00:00,  9.24it/s, loss=2.1629]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | Train Loss: 2.1922 | Val Loss: 2.3200 | Time: 00:13:13.74\n",
      "  ‚Ü≥ Saved BEST to best_transformer_en_vi_resume.pt (val_loss=2.3200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7031/7031 [12:51<00:00,  9.12it/s, loss=2.1965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 | Train Loss: 2.1655 | Val Loss: 2.3063 | Time: 00:13:23.57\n",
      "  ‚Ü≥ Saved BEST to best_transformer_en_vi_resume.pt (val_loss=2.3063)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7031/7031 [12:51<00:00,  9.11it/s, loss=2.1364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | Train Loss: 2.1419 | Val Loss: 2.2936 | Time: 00:13:24.29\n",
      "  ‚Ü≥ Saved BEST to best_transformer_en_vi_resume.pt (val_loss=2.2936)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7031/7031 [12:51<00:00,  9.11it/s, loss=2.1977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 | Train Loss: 2.1205 | Val Loss: 2.2880 | Time: 00:13:24.53\n",
      "  ‚Ü≥ Saved BEST to best_transformer_en_vi_resume.pt (val_loss=2.2880)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7031/7031 [12:49<00:00,  9.13it/s, loss=2.1918]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 | Train Loss: 2.1010 | Val Loss: 2.2798 | Time: 00:13:21.83\n",
      "  ‚Ü≥ Saved BEST to best_transformer_en_vi_resume.pt (val_loss=2.2798)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7031/7031 [12:40<00:00,  9.25it/s, loss=2.0429]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 | Train Loss: 2.0832 | Val Loss: 2.2759 | Time: 00:13:11.67\n",
      "  ‚Ü≥ Saved BEST to best_transformer_en_vi_resume.pt (val_loss=2.2759)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7031/7031 [12:36<00:00,  9.30it/s, loss=2.0821]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 | Train Loss: 2.0664 | Val Loss: 2.2651 | Time: 00:13:07.79\n",
      "  ‚Ü≥ Saved BEST to best_transformer_en_vi_resume.pt (val_loss=2.2651)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7031/7031 [12:39<00:00,  9.26it/s, loss=2.0927]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 | Train Loss: 2.0514 | Val Loss: 2.2620 | Time: 00:13:10.70\n",
      "  ‚Ü≥ Saved BEST to best_transformer_en_vi_resume.pt (val_loss=2.2620)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7031/7031 [12:38<00:00,  9.27it/s, loss=2.1392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 | Train Loss: 2.0373 | Val Loss: 2.2551 | Time: 00:13:10.41\n",
      "  ‚Ü≥ Saved BEST to best_transformer_en_vi_resume.pt (val_loss=2.2551)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7031/7031 [12:38<00:00,  9.27it/s, loss=2.1346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 | Train Loss: 2.0238 | Val Loss: 2.2501 | Time: 00:13:10.56\n",
      "  ‚Ü≥ Saved BEST to best_transformer_en_vi_resume.pt (val_loss=2.2501)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7031/7031 [12:36<00:00,  9.29it/s, loss=2.2158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 | Train Loss: 2.0113 | Val Loss: 2.2468 | Time: 00:13:08.05\n",
      "  ‚Ü≥ Saved BEST to best_transformer_en_vi_resume.pt (val_loss=2.2468)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7031/7031 [12:36<00:00,  9.30it/s, loss=1.9397]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 | Train Loss: 1.9997 | Val Loss: 2.2395 | Time: 00:13:07.92\n",
      "  ‚Ü≥ Saved BEST to best_transformer_en_vi_resume.pt (val_loss=2.2395)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7031/7031 [12:37<00:00,  9.28it/s, loss=2.0281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 | Train Loss: 1.9888 | Val Loss: 2.2408 | Time: 00:13:09.67\n",
      "S·ªë epoch k√©m ch·∫•t l∆∞·ª£ng 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7031/7031 [12:37<00:00,  9.28it/s, loss=1.9171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 | Train Loss: 1.9783 | Val Loss: 2.2344 | Time: 00:13:09.49\n",
      "  ‚Ü≥ Saved BEST to best_transformer_en_vi_resume.pt (val_loss=2.2344)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7031/7031 [12:41<00:00,  9.23it/s, loss=1.9716]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 | Train Loss: 1.9685 | Val Loss: 2.2322 | Time: 00:13:13.80\n",
      "  ‚Ü≥ Saved BEST to best_transformer_en_vi_resume.pt (val_loss=2.2322)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7031/7031 [12:41<00:00,  9.23it/s, loss=1.9761]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 | Train Loss: 1.9340 | Val Loss: 2.2214 | Time: 00:13:13.32\n",
      "  ‚Ü≥ Saved BEST to best_transformer_en_vi_resume.pt (val_loss=2.2214)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7031/7031 [12:41<00:00,  9.24it/s, loss=1.9008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 | Train Loss: 1.9260 | Val Loss: 2.2210 | Time: 00:13:13.00\n",
      "  ‚Ü≥ Saved BEST to best_transformer_en_vi_resume.pt (val_loss=2.2210)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7031/7031 [12:39<00:00,  9.25it/s, loss=1.9553]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 | Train Loss: 1.9186 | Val Loss: 2.2194 | Time: 00:13:11.42\n",
      "  ‚Ü≥ Saved BEST to best_transformer_en_vi_resume.pt (val_loss=2.2194)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7031/7031 [12:38<00:00,  9.26it/s, loss=1.9509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 | Train Loss: 1.9117 | Val Loss: 2.2168 | Time: 00:13:10.74\n",
      "  ‚Ü≥ Saved BEST to best_transformer_en_vi_resume.pt (val_loss=2.2168)\n",
      "‚úÖ Done. Saved: best_transformer_en_vi_resume.pt & last_transformer_en_vi_resume.pth\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# --- CELL 12: CONTINUE TRAINING LOOP ---\n",
    "# ======================================\n",
    "import time \n",
    "EPOCHS_MORE = 30                \n",
    "early = EarlyStopping(patience=3, min_delta=1e-4, mode=\"min\")\n",
    "\n",
    "# n·∫øu checkpoint c√≥ best val tr∆∞·ªõc ƒë√≥ th√¨ set l√†m m·ªëc\n",
    "if best_prev_val is not None:\n",
    "    early.best = best_prev_val\n",
    "\n",
    "best_path = \"best_transformer_en_vi_resume.pt\"\n",
    "\n",
    "print(\"\\n--- B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN TI·∫æP ---\")\n",
    "\n",
    "for ep in range(EPOCHS_MORE):\n",
    "    epoch = start_epoch + ep  # epoch th·ª±c t√≠nh ti·∫øp\n",
    "    t0 = time.time() \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    pbar = tqdm.tqdm(train_loader, desc=f\"Epoch {epoch+1}/{start_epoch+EPOCHS_MORE}\")\n",
    "\n",
    "    for src, tgt in pbar:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        tgt_input, tgt_real = tgt[:, :-1], tgt[:, 1:]\n",
    "        src_mask, tgt_mask = create_masks(src, tgt_input)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt_input, src_mask, tgt_mask)\n",
    "\n",
    "        loss = criterion(output.reshape(-1, output.shape[-1]), tgt_real.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    avg_train = train_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in val_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input, tgt_real = tgt[:, :-1], tgt[:, 1:]\n",
    "            src_mask, tgt_mask = create_masks(src, tgt_input)\n",
    "            output = model(src, tgt_input, src_mask, tgt_mask)\n",
    "            val_loss += criterion(output.reshape(-1, output.shape[-1]), tgt_real.reshape(-1)).item()\n",
    "\n",
    "    avg_val = val_loss / len(val_loader)\n",
    "    epoch_time = time.time() - t0\n",
    "    m, s = divmod(epoch_time, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {avg_train:.4f} | Val Loss: {avg_val:.4f} | Time: {int(h):02d}:{int(m):02d}:{s:05.2f}\")\n",
    "\n",
    "    # save best + early stopping\n",
    "    if early.best is None or avg_val < early.best - early.min_delta:\n",
    "        save_checkpoint(best_path, model, optimizer, epoch+1, avg_val)\n",
    "        print(f\"  ‚Ü≥ Saved BEST to {best_path} (val_loss={avg_val:.4f})\")\n",
    "\n",
    "    if early.step(avg_val):\n",
    "        print(f\"üõë Early stopping at epoch {epoch+1}. Best val_loss = {early.best:.4f}\")\n",
    "        break\n",
    "\n",
    "torch.save(model.state_dict(), \"last_transformer_en_vi_resume.pth\")\n",
    "print(\"‚úÖ Done. Saved: best_transformer_en_vi_resume.pt & last_transformer_en_vi_resume.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T13:02:18.222617Z",
     "iopub.status.busy": "2025-12-13T13:02:18.222110Z",
     "iopub.status.idle": "2025-12-13T13:02:20.113426Z",
     "shell.execute_reply": "2025-12-13T13:02:20.112832Z",
     "shell.execute_reply.started": "2025-12-13T13:02:18.222596Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ckpt keys: ['epoch', 'model_state_dict', 'optimizer_state_dict', 'val_loss']\n",
      "epoch: 23 val_loss: 2.261958536589542\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# LOAD BEST CHECKPOINT\n",
    "# ==========================================\n",
    "import os, torch\n",
    "\n",
    "# CKPT_PATH = \"best_transformer_en_vi_resume.pt\"  (ƒë√¢y l√† tr·∫°ng th√°i khi train h·∫øt 37 epoch)\n",
    "CKPT_PATH = \"/kaggle/input/startstatus/best_transformer_en_vi_resume.pt\" # ƒë√¢y l√† tr·∫°ng th√°i ·ªü epoch: 23 val_loss: 2.261958536589542\n",
    "assert os.path.exists(CKPT_PATH), f\"Kh√¥ng th·∫•y file ckpt: {CKPT_PATH}\"\n",
    "\n",
    "ckpt = torch.load(CKPT_PATH, map_location=device)\n",
    "if isinstance(ckpt, dict):\n",
    "    print(\"‚úÖ ckpt keys:\", list(ckpt.keys())[:20])\n",
    "    print(\"epoch:\", ckpt.get(\"epoch\"), \"val_loss:\", ckpt.get(\"val_loss\"))\n",
    "else:\n",
    "    print(\"‚úÖ ckpt is a raw state_dict (not a dict wrapper)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T13:02:30.783882Z",
     "iopub.status.busy": "2025-12-13T13:02:30.783366Z",
     "iopub.status.idle": "2025-12-13T13:02:30.802425Z",
     "shell.execute_reply": "2025-12-13T13:02:30.801876Z",
     "shell.execute_reply.started": "2025-12-13T13:02:30.783848Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded state_dict\n",
      "missing keys: 0\n",
      "unexpected keys: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (src_emb): Embedding(10000, 256)\n",
       "  (tgt_emb): Embedding(10000, 256)\n",
       "  (rope): RotaryPositionalEncoding()\n",
       "  (encoders): ModuleList(\n",
       "    (0-3): 4 x TransformerBlock(\n",
       "      (norm1): RMSNorm((256,), eps=None, elementwise_affine=True)\n",
       "      (attn): GQA(\n",
       "        (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (o_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_ffn): RMSNorm((256,), eps=None, elementwise_affine=True)\n",
       "      (ffn): SwiGLU(\n",
       "        (w1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (w2): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (w3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoders): ModuleList(\n",
       "    (0-3): 4 x TransformerBlock(\n",
       "      (norm1): RMSNorm((256,), eps=None, elementwise_affine=True)\n",
       "      (attn): GQA(\n",
       "        (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (o_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm2): RMSNorm((256,), eps=None, elementwise_affine=True)\n",
       "      (cross_attn): GQA(\n",
       "        (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (o_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_ffn): RMSNorm((256,), eps=None, elementwise_affine=True)\n",
       "      (ffn): SwiGLU(\n",
       "        (w1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (w2): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (w3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): RMSNorm((256,), eps=None, elementwise_affine=True)\n",
       "  (fc_out): Linear(in_features=256, out_features=10000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==========================================\n",
    "# LOAD WEIGHTS INTO EXISTING MODEL\n",
    "# ==========================================\n",
    "# 1) N·∫øu checkpoint l√† wrapper dict\n",
    "state_dict = ckpt[\"model_state_dict\"] if isinstance(ckpt, dict) and \"model_state_dict\" in ckpt else ckpt\n",
    "\n",
    "# 2) model ph·∫£i t·ªìn t·∫°i t·ª´ tr∆∞·ªõc (∆∞u ti√™n t√°i s·ª≠ d·ª•ng)\n",
    "assert \"model\" in globals(), \"Ch∆∞a th·∫•y bi·∫øn model. H√£y ch·∫°y cell t·∫°o model tr∆∞·ªõc (gi·ªëng l√∫c train).\"\n",
    "\n",
    "missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
    "print(\"‚úÖ Loaded state_dict\")\n",
    "print(\"missing keys:\", len(missing))\n",
    "print(\"unexpected keys:\", len(unexpected))\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T11:30:41.653477Z",
     "iopub.status.busy": "2025-12-13T11:30:41.652780Z",
     "iopub.status.idle": "2025-12-13T11:30:41.703015Z",
     "shell.execute_reply": "2025-12-13T11:30:41.702433Z",
     "shell.execute_reply.started": "2025-12-13T11:30:41.653448Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ #test pairs: 3000\n",
      "sample: ('Knowledge, practices in public health service utilization among health insurance card‚Äôs holders and influencing factors in Vientiane, Lao', 'Th·ª±c tr·∫°ng ki·∫øn th·ª©c v√† th·ª±c h√†nh c·ªßa ng∆∞·ªùi c√≥ th·∫ª b·∫£o hi·ªÉm y t·∫ø trong s·ª≠ d·ª•ng d·ªãch v·ª• kh√°m ch·ªØa b·ªánh ·ªü c√°c c∆° s·ªü y t·∫ø c√¥ng v√† m·ªôt s·ªë y·∫øu t·ªë ·∫£nh h∆∞·ªüng t·∫°i t·ªânh Vi√™ng ChƒÉn, CHDCND L√†o, nƒÉm 2017')\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# LOAD TEST DATA (EN->VI)\n",
    "# ==========================================\n",
    "TEST_SRC_PATH = \"/kaggle/input/databaitoanphu/public_test.en.txt\"\n",
    "TEST_TGT_PATH = \"/kaggle/input/databaitoanphu/public_test.vi.txt\"\n",
    "\n",
    "assert os.path.exists(TEST_SRC_PATH), f\"Kh√¥ng th·∫•y test src: {TEST_SRC_PATH}\"\n",
    "assert os.path.exists(TEST_TGT_PATH), f\"Kh√¥ng th·∫•y test tgt: {TEST_TGT_PATH}\"\n",
    "\n",
    "if \"read_parallel_data\" in globals():\n",
    "    test_pairs = read_parallel_data(TEST_SRC_PATH, TEST_TGT_PATH)\n",
    "else:\n",
    "    # fallback ƒë∆°n gi·∫£n\n",
    "    test_pairs = []\n",
    "    with open(TEST_SRC_PATH, \"r\", encoding=\"utf-8\") as fs, open(TEST_TGT_PATH, \"r\", encoding=\"utf-8\") as ft:\n",
    "        for s, t in zip(fs, ft):\n",
    "            s, t = s.strip(), t.strip()\n",
    "            if s and t:\n",
    "                test_pairs.append((s, t))\n",
    "\n",
    "print(\"‚úÖ #test pairs:\", len(test_pairs))\n",
    "print(\"sample:\", test_pairs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T13:02:47.958610Z",
     "iopub.status.busy": "2025-12-13T13:02:47.958128Z",
     "iopub.status.idle": "2025-12-13T13:02:47.969453Z",
     "shell.execute_reply": "2025-12-13T13:02:47.968690Z",
     "shell.execute_reply.started": "2025-12-13T13:02:47.958588Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# --- FULL: BEAM SEARCH DECODING ---\n",
    "# =====================================\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def beam_search_decode(\n",
    "    model,\n",
    "    src,                      # (1, src_len)\n",
    "    beam_size=5,\n",
    "    max_len=80,\n",
    "    length_penalty=0.7\n",
    "):\n",
    "    \"\"\"\n",
    "    Beam search decode for 1 sentence.\n",
    "\n",
    "    Assumes these globals already exist (as in your notebook):\n",
    "      - device\n",
    "      - PAD_ID   (src pad id)\n",
    "      - START_ID (tgt start id)\n",
    "      - END_ID   (tgt end id)\n",
    "\n",
    "    Returns:\n",
    "      best_tokens: List[int] (includes START_ID, may include END_ID)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    src = src.to(device)\n",
    "\n",
    "    # -------------------------\n",
    "    # 1) Encoder (run once)\n",
    "    # -------------------------\n",
    "    src_mask = (src == PAD_ID).unsqueeze(1).unsqueeze(2).float() * -1e9  # (1,1,1,src_len)\n",
    "\n",
    "    enc = model.src_emb(src)  # (1, src_len, d)\n",
    "    rope_cos, rope_sin = model.rope(enc, enc.shape[1])\n",
    "\n",
    "    for layer in model.encoders:\n",
    "        enc = layer(enc, mask=src_mask, rope_cos=rope_cos, rope_sin=rope_sin)\n",
    "\n",
    "    # -------------------------\n",
    "    # 2) Beam init\n",
    "    # -------------------------\n",
    "    beams = [([START_ID], 0.0)]   # list of (token_ids, log_prob_sum)\n",
    "    finished = []                # finished beams that ended with END_ID\n",
    "\n",
    "    # helper for normalized score\n",
    "    def norm_score(tokens, score):\n",
    "        L = max(1, len(tokens))\n",
    "        return score / (L ** length_penalty)\n",
    "\n",
    "    # -------------------------\n",
    "    # 3) Decode steps\n",
    "    # -------------------------\n",
    "    for step in range(max_len):\n",
    "        new_beams = []\n",
    "\n",
    "        for tokens, score in beams:\n",
    "            # If ended, keep it\n",
    "            if tokens[-1] == END_ID:\n",
    "                finished.append((tokens, score))\n",
    "                new_beams.append((tokens, score))\n",
    "                continue\n",
    "\n",
    "            ys = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0)  # (1, t)\n",
    "\n",
    "            # causal mask (t, t): -inf for future positions\n",
    "            tgt_mask = torch.triu(\n",
    "                torch.full((ys.size(1), ys.size(1)), float(\"-inf\"), device=device),\n",
    "                diagonal=1\n",
    "            )\n",
    "\n",
    "            # ---- Decoder forward ----\n",
    "            dec = model.tgt_emb(ys)  # (1, t, d)\n",
    "            rope_cos_t, rope_sin_t = model.rope(dec, dec.shape[1])\n",
    "\n",
    "            x = dec\n",
    "            for layer in model.decoders:\n",
    "                x = layer(\n",
    "                    x,\n",
    "                    enc_out=enc,\n",
    "                    mask=tgt_mask,\n",
    "                    cross_mask=src_mask,\n",
    "                    rope_cos=rope_cos_t,\n",
    "                    rope_sin=rope_sin_t\n",
    "                )\n",
    "\n",
    "            logits = model.fc_out(model.final_norm(x))[:, -1, :]  # (1, vocab)\n",
    "            log_probs = F.log_softmax(logits, dim=-1)             # (1, vocab)\n",
    "\n",
    "            topk_log_probs, topk_ids = log_probs.topk(beam_size, dim=-1)\n",
    "\n",
    "            for k in range(beam_size):\n",
    "                next_id = topk_ids[0, k].item()\n",
    "                next_score = score + topk_log_probs[0, k].item()\n",
    "                new_beams.append((tokens + [next_id], next_score))\n",
    "\n",
    "        # -------------------------\n",
    "        # 4) Prune (keep best beams)\n",
    "        # -------------------------\n",
    "        new_beams = sorted(\n",
    "            new_beams,\n",
    "            key=lambda x: norm_score(x[0], x[1]),\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        beams = new_beams[:beam_size]\n",
    "\n",
    "        # Early stop: if all current beams are finished\n",
    "        if all(toks[-1] == END_ID for toks, _ in beams):\n",
    "            break\n",
    "\n",
    "    # -------------------------\n",
    "    # 5) Pick best candidate\n",
    "    # -------------------------\n",
    "    candidates = finished if finished else beams\n",
    "    best_tokens, best_score = max(candidates, key=lambda x: norm_score(x[0], x[1]))\n",
    "    return best_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T11:33:44.221510Z",
     "iopub.status.busy": "2025-12-13T11:33:44.220776Z",
     "iopub.status.idle": "2025-12-13T11:33:50.246949Z",
     "shell.execute_reply": "2025-12-13T11:33:50.246220Z",
     "shell.execute_reply.started": "2025-12-13T11:33:44.221479Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SRC: Knowledge, practices in public health service utilization among health insurance card‚Äôs holders and influencing factors in Vientiane, Lao\n",
      "REF: Th·ª±c tr·∫°ng ki·∫øn th·ª©c v√† th·ª±c h√†nh c·ªßa ng∆∞·ªùi c√≥ th·∫ª b·∫£o hi·ªÉm y t·∫ø trong s·ª≠ d·ª•ng d·ªãch v·ª• kh√°m ch·ªØa b·ªánh ·ªü c√°c c∆° s·ªü y t·∫ø c√¥ng v√† m·ªôt s·ªë y·∫øu t·ªë ·∫£nh h∆∞·ªüng t·∫°i t·ªânh Vi√™ng ChƒÉn, CHDCND L√†o, nƒÉm 2017\n",
      "PRED:  Ki·∫øn th·ª©c, th·ª±c h√†nh s·ª≠ d·ª•ng d·ªãch v·ª• y t·∫ø c√¥ng c·ªông c·ªßa ng∆∞·ªùi d√¢n b·∫£o hi·ªÉm y t·∫ø v√† c√°c y·∫øu t·ªë ·∫£nh h∆∞·ªüng t·∫°i Vii, L√†o Cai\n",
      "================================================================================\n",
      "SRC: Describe knowledge, practices in public health service utilization among health insurance card's holders and influencing factors in Vientiane, Lao PDR, 2017.\n",
      "REF: M√¥ t·∫£ th·ª±c tr·∫°ng ki·∫øn th·ª©c, th·ª±c h√†nh c·ªßa ng∆∞·ªùi c√≥ th·∫ª b·∫£o hi·ªÉm y t·∫ø trong s·ª≠ d·ª•ng d·ªãch v·ª• kh√°m ch·ªØa b·ªánh ·ªü c√°c c∆° s·ªü y t·∫ø c√¥ng v√† m·ªôt s·ªë y·∫øu t·ªë li√™n quan t·∫°i t·ªânh Vi√™ng ChƒÉn, C·ªông ho√† D√¢n ch·ªß Nh√¢n d√¢n L√†o nƒÉm 2017.\n",
      "PRED:  M√¥ t·∫£ ki·∫øn th·ª©c, th·ª±c h√†nh v·ªÅ s·ª≠ d·ª•ng d·ªãch v·ª• y t·∫ø c√¥ng c·ªông c·ªßa ng∆∞·ªùi tham gia b·∫£o hi·ªÉm y t·∫ø (BHYT) v√† c√°c y·∫øu t·ªë ·∫£nh h∆∞·ªüng t·∫°i Viventan, L√†o Cai nƒÉm 2017.\n",
      "================================================================================\n",
      "SRC: Methodology: A cross sectional study was used among 928 adult health insurance card's holders in Phone Hong and Keo Oudom districts, Vientiane province.\n",
      "REF: Ph∆∞∆°ng ph√°p: Thi·∫øt k·∫ø nghi√™n m√¥ t·∫£ c·∫Øt ngang ƒë∆∞·ª£c th·ª±c hi·ªán tr√™n 928 ng∆∞·ªùi tr∆∞·ªüng th√†nh c√≥ th·∫ª b·∫£o hi·ªÉm y t·∫ø t·∫°i 2 huy·ªán Phone Hong v√† Keo Oudom, t·ªânh Vi√™ng ChƒÉn.\n",
      "PRED:  ƒê·ªëi t∆∞·ª£ng v√† ph∆∞∆°ng ph√°p nghi√™n c·ª©u: Nghi√™n c·ª©u c·∫Øt ngang m√¥ t·∫£ ƒë∆∞·ª£c th·ª±c hi·ªán tr√™n 928 nh√¢n vi√™n b·∫£o hi·ªÉm y t·∫ø t·∫°i c√°c qu·∫≠n, huy·ªán Phil Hong v√† K·ªÉ t·ª´ th√°ng 9/2011 ƒë·∫øn th√°ng 9/2012.\n",
      "================================================================================\n",
      "SRC: Results: Percentage of card's holders who knew the finance-free utilization of the first registered public health services was 44.5% and being provided health insurance information was 34.8%.\n",
      "REF: K·∫øt qu·∫£: T·ª∑ l·ªá ng∆∞·ªùi bi·∫øt ƒë∆∞·ª£c kh√°m ch·ªØa b·ªánh (KCB) mi·ªÖn ph√≠ t·∫°i n∆°i ƒëƒÉng k√Ω ban ƒë·∫ßu chi·∫øm 44,5%, ƒë∆∞·ª£c cung c·∫•p th√¥ng tin v·ªÅ b·∫£o hi·ªÉm y t·∫ø (BHYT) chi·∫øm 34,8%.\n",
      "PRED:  K·∫øt qu·∫£: T·ª∑ l·ªá c√°n b·ªô c·ªßa tr·∫°m y t·∫ø bi·∫øt s·ª≠ d·ª•ng d·ªãch v·ª• y t·∫ø c√¥ng c·ªông kh√¥ng h·ª£p l√Ω l√† 44,5% v√† ƒë∆∞·ª£c cung c·∫•p th√¥ng tin v·ªÅ BHYT l√† 34,8%.\n",
      "================================================================================\n",
      "SRC: Percentage of card's holders who went to the first registered public health services was 61.8%.\n",
      "REF: T·ª∑ l·ªá ng∆∞·ªùi c√≥ th·∫ª BHYT th·ª±c h√†nh kh√°m ch·ªØa b·ªánh ƒë√∫ng n∆°i ƒëƒÉng k√Ω KCB ban ƒë·∫ßu chi·∫øm 61,8%.\n",
      "PRED:  T·ª∑ l·ªá c√°n b·ªô c·ªßa tr·∫°m y t·∫ø c√¥ng l·∫≠p ƒëƒÉng k√Ω ban ƒë·∫ßu l√† 61,8%.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# QUICK SANITY CHECK: TRANSLATE FEW SAMPLES\n",
    "# ==========================================\n",
    "NUM_SHOW = 5\n",
    "\n",
    "for i in range(NUM_SHOW):\n",
    "    src, ref = test_pairs[i]\n",
    "\n",
    "    # (A) N·∫øu anh ƒë√£ c√≥ h√†m translate_en2vi(text) / translate_sentence(...)\n",
    "    if \"translate_en2vi\" in globals():\n",
    "        pred = translate_en2vi(src)\n",
    "    elif \"translate_sentence\" in globals():\n",
    "        # tu·ª≥ signature c·ªßa anh: translate_sentence(model, sentence, ...)\n",
    "        pred = translate_sentence(model, src)\n",
    "    else:\n",
    "        # (B) N·∫øu anh c√≥ beam_search_decode(model, src_ids, ...)\n",
    "        assert \"beam_search_decode\" in globals() or \"greedy_decode\" in globals(), \\\n",
    "            \"Kh√¥ng th·∫•y h√†m translate/beam/greedy c√≥ s·∫µn. Anh ch·∫°y cell ƒë·ªãnh nghƒ©a decode tr∆∞·ªõc.\"\n",
    "\n",
    "        # Tokenize theo ƒë√∫ng tokenizer anh ƒë√£ d√πng\n",
    "        # ƒë·ªïi t√™n tokenizer cho ƒë√∫ng notebook anh:\n",
    "        src_ids = en_tokenizer.encode(src).ids\n",
    "        src_ids = torch.tensor(src_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "        if \"beam_search_decode\" in globals():\n",
    "            out_ids = beam_search_decode(model, src_ids)  # n·∫øu h√†m anh signature kh√°c th√¨ ch·ªânh 1 d√≤ng n√†y\n",
    "        else:\n",
    "            out_ids = greedy_decode(model, src_ids)\n",
    "\n",
    "        # decode target\n",
    "        pred = vi_tokenizer.decode(out_ids)\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"SRC:\", src)\n",
    "    print(\"REF:\", ref)\n",
    "    print(\"PRED:\", pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T13:04:30.183560Z",
     "iopub.status.busy": "2025-12-13T13:04:30.182963Z",
     "iopub.status.idle": "2025-12-13T13:04:30.472659Z",
     "shell.execute_reply": "2025-12-13T13:04:30.472115Z",
     "shell.execute_reply.started": "2025-12-13T13:04:30.183539Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC: Group 1: non-obese mice.\n",
      "TRG: Nh√≥m 1: Chu·ªôt kh√¥ng b√©o ph√¨.\n",
      "PRED:  Nh√≥m 1: chu·ªôt kh√¥ng thu·∫ßn ch·ªßng.\n"
     ]
    }
   ],
   "source": [
    "sentence_en = \"Group 1: non-obese mice.\"\n",
    "sentence_vi = \"Nh√≥m 1: Chu·ªôt kh√¥ng b√©o ph√¨.\"\n",
    "\n",
    "print(\"SRC:\", sentence_en)\n",
    "print(\"TRG:\", sentence_vi)\n",
    "\n",
    "# 1) Encode EN -> ids tensor\n",
    "src_ids = en_tokenizer.encode(sentence_en).ids\n",
    "src = torch.tensor(src_ids, dtype=torch.long).unsqueeze(0).to(device)  # (1, src_len)\n",
    "\n",
    "# 2) Beam search -> ids\n",
    "out_ids = beam_search_decode(model, src, beam_size=5, max_len=80)\n",
    "\n",
    "# 3) B·ªè START/END tr∆∞·ªõc khi decode text\n",
    "if len(out_ids) > 0 and out_ids[0] == START_ID:\n",
    "    out_ids = out_ids[1:]\n",
    "if END_ID in out_ids:\n",
    "    out_ids = out_ids[:out_ids.index(END_ID)]\n",
    "\n",
    "pred_vi = vi_tokenizer.decode(out_ids)\n",
    "print(\"PRED:\", pred_vi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T11:34:59.603987Z",
     "iopub.status.busy": "2025-12-13T11:34:59.603697Z",
     "iopub.status.idle": "2025-12-13T11:36:10.312890Z",
     "shell.execute_reply": "2025-12-13T11:36:10.312167Z",
     "shell.execute_reply.started": "2025-12-13T11:34:59.603965Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext\n",
      "  Downloading torchtext-0.18.0-cp311-cp311-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext) (4.67.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext) (2.32.5)\n",
      "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from torchtext) (2.6.0+cu124)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext) (1.26.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (4.15.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (2025.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.3.0->torchtext)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.3.0->torchtext)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.3.0->torchtext)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.3.0->torchtext)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.3.0->torchtext)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.3.0->torchtext)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.3.0->torchtext)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.3.0->torchtext)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.3.0->torchtext)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.3.0->torchtext)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.3.0->torchtext) (1.3.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchtext) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchtext) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchtext) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchtext) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchtext) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchtext) (2.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (2025.10.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3.0->torchtext) (3.0.3)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchtext) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchtext) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchtext) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchtext) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchtext) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchtext) (2024.2.0)\n",
      "Downloading torchtext-0.18.0-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m120.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchtext\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchtext-0.18.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T11:50:48.346555Z",
     "iopub.status.busy": "2025-12-13T11:50:48.345733Z",
     "iopub.status.idle": "2025-12-13T11:50:51.570836Z",
     "shell.execute_reply": "2025-12-13T11:50:51.569883Z",
     "shell.execute_reply.started": "2025-12-13T11:50:48.346525Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip -q install sacrebleu tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T11:50:54.778487Z",
     "iopub.status.busy": "2025-12-13T11:50:54.777736Z",
     "iopub.status.idle": "2025-12-13T12:44:22.920034Z",
     "shell.execute_reply": "2025-12-13T12:44:22.919391Z",
     "shell.execute_reply.started": "2025-12-13T11:50:54.778456Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1bf860686794d3b94265b611900f9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing BLEU:   0%|          | 0/3000 [00:00<?, ?sent/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 46.57571741460038\n",
      "detail: BLEU = 46.58 75.8/58.4/46.4/37.7 (BP = 0.883 ratio = 0.890 hyp_len = 89734 ref_len = 100870)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# --- BLEU (SACREBLEU) + PROGRESS BAR ---\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "import sacrebleu\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "MAX_EVAL = len(test_pairs)   # ho·∫∑c 2000 ƒë·ªÉ test nhanh\n",
    "\n",
    "pred_texts = []\n",
    "ref_texts  = []\n",
    "\n",
    "for i in tqdm(range(MAX_EVAL), desc=\"Computing BLEU\", unit=\"sent\"):\n",
    "    src, ref = test_pairs[i]\n",
    "\n",
    "    # d√πng h√†m d·ªãch c√≥ s·∫µn c·ªßa anh (∆∞u ti√™n)\n",
    "    if \"translate_en2vi\" in globals():\n",
    "        pred = translate_en2vi(src)\n",
    "    elif \"translate_sentence\" in globals():\n",
    "        pred = translate_sentence(model, src)\n",
    "    else:\n",
    "        # fallback n·∫øu anh ch·ªâ c√≥ beam_search_decode/greedy_decode\n",
    "        src_ids = en_tokenizer.encode(src).ids\n",
    "        src_ids = torch.tensor(src_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        out_ids = beam_search_decode(model, src_ids) if \"beam_search_decode\" in globals() else greedy_decode(model, src_ids)\n",
    "\n",
    "        # b·ªè START/END n·∫øu c·∫ßn (tu·ª≥ model anh)\n",
    "        if isinstance(out_ids, torch.Tensor):\n",
    "            out_ids = out_ids.squeeze(0).tolist() if out_ids.ndim > 1 else out_ids.tolist()\n",
    "\n",
    "        if len(out_ids) > 0 and out_ids[0] == START_ID:\n",
    "            out_ids = out_ids[1:]\n",
    "        if END_ID in out_ids:\n",
    "            out_ids = out_ids[:out_ids.index(END_ID)]\n",
    "\n",
    "        pred = vi_tokenizer.decode(out_ids)\n",
    "\n",
    "    pred_texts.append(pred.strip())\n",
    "    ref_texts.append(ref.strip())\n",
    "\n",
    "bleu = sacrebleu.corpus_bleu(pred_texts, [ref_texts])\n",
    "print(\"BLEU =\", bleu.score)\n",
    "print(\"detail:\", bleu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T√≠nh Bleu khi train ·ªü epoch: 23 val_loss: 2.261958536589542 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T13:09:50.877702Z",
     "iopub.status.busy": "2025-12-13T13:09:50.877395Z",
     "iopub.status.idle": "2025-12-13T14:03:34.716461Z",
     "shell.execute_reply": "2025-12-13T14:03:34.715551Z",
     "shell.execute_reply.started": "2025-12-13T13:09:50.877681Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acdc2dd123544e6a968ef76737a1a37f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing BLEU:   0%|          | 0/3000 [00:00<?, ?sent/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 44.982471593436834\n",
      "detail: BLEU = 44.98 74.8/56.8/44.3/35.4 (BP = 0.886 ratio = 0.892 hyp_len = 89958 ref_len = 100870)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# --- BLEU (SACREBLEU) + PROGRESS BAR ---\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "import sacrebleu\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "MAX_EVAL = len(test_pairs)   # ho·∫∑c 2000 ƒë·ªÉ test nhanh\n",
    "\n",
    "pred_texts = []\n",
    "ref_texts  = []\n",
    "\n",
    "for i in tqdm(range(MAX_EVAL), desc=\"Computing BLEU\", unit=\"sent\"):\n",
    "    src, ref = test_pairs[i]\n",
    "\n",
    "    # d√πng h√†m d·ªãch c√≥ s·∫µn c·ªßa anh (∆∞u ti√™n)\n",
    "    if \"translate_en2vi\" in globals():\n",
    "        pred = translate_en2vi(src)\n",
    "    elif \"translate_sentence\" in globals():\n",
    "        pred = translate_sentence(model, src)\n",
    "    else:\n",
    "        # fallback n·∫øu anh ch·ªâ c√≥ beam_search_decode/greedy_decode\n",
    "        src_ids = en_tokenizer.encode(src).ids\n",
    "        src_ids = torch.tensor(src_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        out_ids = beam_search_decode(model, src_ids) if \"beam_search_decode\" in globals() else greedy_decode(model, src_ids)\n",
    "\n",
    "        # b·ªè START/END n·∫øu c·∫ßn (tu·ª≥ model anh)\n",
    "        if isinstance(out_ids, torch.Tensor):\n",
    "            out_ids = out_ids.squeeze(0).tolist() if out_ids.ndim > 1 else out_ids.tolist()\n",
    "\n",
    "        if len(out_ids) > 0 and out_ids[0] == START_ID:\n",
    "            out_ids = out_ids[1:]\n",
    "        if END_ID in out_ids:\n",
    "            out_ids = out_ids[:out_ids.index(END_ID)]\n",
    "\n",
    "        pred = vi_tokenizer.decode(out_ids)\n",
    "\n",
    "    pred_texts.append(pred.strip())\n",
    "    ref_texts.append(ref.strip())\n",
    "\n",
    "bleu = sacrebleu.corpus_bleu(pred_texts, [ref_texts])\n",
    "print(\"BLEU =\", bleu.score)\n",
    "print(\"detail:\", bleu)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 9001785,
     "sourceId": 14128026,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9002085,
     "sourceId": 14128405,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9011925,
     "sourceId": 14141279,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
