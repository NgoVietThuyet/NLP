\documentclass[11pt]{article}
\usepackage{lmodern}
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage[T5]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{url}
\usepackage{xcolor}
\usepackage{float}
\usepackage{placeins}

\title{Fine-tuning mBART-50 cho Dịch máy Y tế Việt-Anh: \\
Nghiên cứu Toàn diện trên VLSP 2025 Shared Task}

\author{
  \textbf{Đào Đức Mạnh} \\
  MSV: 23021618 \\
  \texttt{23021618@vnu.edu.vn} \\\And
  \textbf{Ngọ Viết Thuyết} \\
  MSV: 23021730 \\
  \texttt{23021730@vnu.edu.vn} \\\AND
  \textbf{Lưu Văn Hùng} \\
  MSV: 23021566 \\
  \texttt{23021566@vnu.edu.vn} \\
}

\setlength\titlebox{7cm}

\begin{document}
\maketitle

\begin{abstract}
Chúng tôi trình bày một nghiên cứu có hệ thống về fine-tuning mBART-50, một mô hình đa ngôn ngữ pre-trained quy mô lớn, cho bài toán dịch máy y tế Việt-Anh trong VLSP 2025 Shared Task. Phương pháp của chúng tôi sử dụng pipeline tiền xử lý ba giai đoạn toàn diện (làm sạch, lọc và loại bỏ trùng lặp) giúp giảm dư thừa dữ liệu 31.9\% từ 500K xuống 340.5K cặp câu chất lượng cao. Chúng tôi fine-tune mô hình mBART-50 với 610M tham số thông qua thiết kế hyperparameter cẩn thận bao gồm learning rate thấp (3e-5), gradient accumulation và mixed-precision training. Hệ thống của chúng tôi đạt điểm BLEU 41.51 trên tập public test (2,943 câu), vượt trội đáng kể so với baseline zero-shot (35-38 BLEU) và tương đương các hệ thống thương mại. Thông qua phân tích lỗi chi tiết trên 100 mẫu ngẫu nhiên, chúng tôi xác định năm loại lỗi chính và quan sát rằng 68\% bản dịch là hoàn hảo. Chúng tôi cung cấp các insight về chiến lược fine-tuning và đề xuất roadmap để đạt 48-52 BLEU trong công việc tương lai.
\end{abstract}

\section{Giới thiệu}

Dịch máy (Machine Translation - MT) cho các lĩnh vực chuyên ngành như văn bản y tế đặt ra những thách thức độc đáo do thuật ngữ đặc thù của domain, cấu trúc câu phức tạp và nhu cầu quan trọng về độ chính xác \cite{chen2019medical}. VLSP 2025 Machine Translation Shared Task tập trung vào dịch Việt-Anh cho các tóm tắt nghiên cứu y học và mô tả lâm sàng, nơi lỗi dịch có thể dẫn đến hiểu sai nghiêm trọng về thông tin y khoa.

Những tiến bộ gần đây trong các mô hình đa ngôn ngữ pre-trained quy mô lớn như mBART \cite{mbart}, mT5 \cite{mt5} và NLLB \cite{nllb} đã thể hiện hiệu suất đáng chú ý thông qua transfer learning từ kho ngữ liệu đa ngôn ngữ khổng lồ. Các mô hình này tận dụng kiến thức từ nhiều ngôn ngữ để cải thiện chất lượng dịch cho các cặp ngôn ngữ cụ thể, đặc biệt trong các kịch bản low-resource.

Trong công trình này, chúng tôi áp dụng phương pháp fine-tuning sử dụng mBART-50 thay vì training từ đầu vì một số lý do thuyết phục:
\begin{enumerate}
    \item \textbf{Warm start:} Pre-trained weights cung cấp khởi tạo mạnh mẽ
    \item \textbf{Transfer learning:} Kiến thức đa ngôn ngữ có lợi cho cặp Việt-Anh
    \item \textbf{Hiệu quả dữ liệu:} Đạt hiệu suất cao với dữ liệu domain hạn chế
    \item \textbf{Hội tụ nhanh:} Chỉ cần 3 epochs so với 10+ khi training từ đầu
\end{enumerate}

Các đóng góp chính của chúng tôi là:
\begin{itemize}
    \item Pipeline tiền xử lý ba giai đoạn có hệ thống được thiết kế đặc biệt cho domain y tế
    \item Phân tích chi tiết các chiến lược fine-tuning bao gồm learning rate, gradient accumulation và mixed precision
    \item Taxonomy lỗi toàn diện với tỷ lệ dịch hoàn hảo 68\%
    \item So sánh thực nghiệm với baseline và ablation studies
    \item Roadmap cụ thể cho các cải tiến trong tương lai hướng tới 48-52 BLEU
\end{itemize}

Phần còn lại của bài báo được tổ chức như sau: Phần 2 xem xét các công trình liên quan. Phần 3 mô tả kiến trúc mBART-50. Phần 4 chi tiết phương pháp luận của chúng tôi. Phần 5 trình bày kết quả thực nghiệm và phân tích. Phần 6 kết luận với các hướng phát triển tương lai.

\section{Các công trình liên quan}

\subsection{Dịch máy Neural}

Dịch máy Neural đã phát triển từ các mô hình sequence-to-sequence với RNN \cite{sutskever2014sequence} và cơ chế attention \cite{bahdanau2015neural} đến kiến trúc Transformer \cite{vaswani2017attention}, đã trở thành tiêu chuẩn de facto cho các hệ thống MT.

\subsection{Mô hình đa ngôn ngữ Pre-trained}

Các mô hình đa ngôn ngữ pre-trained quy mô lớn đã cách mạng hóa MT:

\textbf{mBART} \cite{mbart} sử dụng denoising auto-encoding pre-training trên 25 ngôn ngữ, sau đó được mở rộng thành mBART-50 bao phủ 50 ngôn ngữ. Mô hình sử dụng kiến trúc Transformer tiêu chuẩn với các token đặc thù ngôn ngữ.

\textbf{mT5} \cite{mt5} mở rộng T5 lên 101 ngôn ngữ sử dụng framework text-to-text thống nhất. Nó pre-train trên corpus mC4 và cho thấy hiệu suất zero-shot cross-lingual mạnh mẽ.

\textbf{NLLB} \cite{nllb} mở rộng lên 200 ngôn ngữ với trọng tâm vào các ngôn ngữ low-resource. Nó giới thiệu các kỹ thuật chuyên biệt để xử lý sự mất cân bằng ngôn ngữ và đạt kết quả state-of-the-art trên FLORES-200.

\subsection{Domain Adaptation cho MT Y tế}

MT y tế đòi hỏi xử lý thuật ngữ chuyên ngành và duy trì độ chính xác cao \cite{chen2019medical}. Các công trình trước đây đã khám phá:

\textbf{Domain-specific pre-training:} \citet{gururangan2020don} chỉ ra rằng tiếp tục pre-training trên dữ liệu domain cải thiện hiệu suất.

\textbf{Mô hình nhận biết thuật ngữ:} \citet{dinu2019training} đề xuất các phương pháp kết hợp ràng buộc thuật ngữ trong quá trình training.

\textbf{Fine-tuning hai giai đoạn:} \citet{chu2017empirical} chứng minh rằng training trên domain tổng quát theo sau bởi domain cụ thể cho kết quả tốt hơn.

\subsection{MT Việt-Anh}

MT Việt-Anh đối mặt với thách thức do sự khác biệt ngôn ngữ học (tiếng Anh giàu hình thái, tiếng Việt là ngôn ngữ đơn lập) và dữ liệu song ngữ hạn chế. Các công trình gần đây bao gồm:

\textbf{PhoBERT} \cite{phobert} cung cấp biểu diễn ngôn ngữ tiếng Việt mạnh mẽ nhưng chỉ là encoder-only.

\textbf{ViT5} \cite{vit5} pre-train T5 đặc biệt cho tiếng Việt nhưng có khả năng đa ngôn ngữ hạn chế.

\textbf{VLSP shared tasks} đã thúc đẩy tiến bộ trong NLP tiếng Việt, với các phiên bản trước tập trung vào nhiều kịch bản MT khác nhau \cite{vlsp2021}.

Công trình của chúng tôi xây dựng trên mBART-50 và đóng góp một nghiên cứu có hệ thống về các chiến lược fine-tuning cho MT y tế Việt-Anh, bao gồm phân tích lỗi toàn diện và roadmap cải tiến cụ thể.

\section{Kiến trúc mBART-50}

\subsection{Tổng quan Mô hình}

mBART-50 (Multilingual BART-50) là một mô hình sequence-to-sequence dựa trên kiến trúc Transformer \cite{vaswani2017attention}, được pre-training trên 50 ngôn ngữ sử dụng các objective denoising.

\textbf{Đặc tả kiến trúc:}
\begin{itemize}
    \item \textbf{Encoder:} 12 lớp, 1024 hidden dimensions, 16 attention heads
    \item \textbf{Decoder:} 12 lớp, 1024 hidden dimensions, 16 attention heads
    \item \textbf{Tham số:} $\sim$610M
    \item \textbf{Vocabulary:} 250,054 tokens bao phủ 50 ngôn ngữ
    \item \textbf{Position encoding:} Learned positional embeddings
    \item \textbf{Activation:} GELU
\end{itemize}

\subsection{Mục tiêu Pre-training}

mBART-50 sử dụng denoising auto-encoding tương tự BART \cite{bart}:
\begin{enumerate}
    \item \textbf{Text infilling:} Mask các span ngẫu nhiên và dự đoán văn bản gốc
    \item \textbf{Sentence permutation:} Xáo trộn các câu và khôi phục thứ tự
    \item \textbf{Document rotation:} Xoay tài liệu đến các vị trí bắt đầu tùy ý
\end{enumerate}

Mục tiêu training là:
\begin{equation}
\mathcal{L} = -\sum_{i=1}^{N} \log P(x_i | \tilde{x}_{<i}, \theta)
\end{equation}
trong đó $\tilde{x}$ là đầu vào bị nhiễu và $\theta$ là các tham số mô hình.

\subsection{Token đặc thù Ngôn ngữ}

Mỗi câu được thêm tiền tố với language code:
\begin{itemize}
    \item Tiếng Việt: \texttt{[vi\_VN]}
    \item Tiếng Anh: \texttt{[en\_XX]}
\end{itemize}

Điều này cho phép dịch many-to-many trong một mô hình duy nhất. Trong quá trình inference, token ngôn ngữ đích hướng dẫn decoder sinh ra ngôn ngữ mong muốn.

\subsection{Ưu điểm cho Domain Y tế}

\begin{enumerate}
    \item \textbf{Dung lượng lớn:} 610M tham số có thể nắm bắt các pattern domain phức tạp
    \item \textbf{Kiến thức đa ngôn ngữ:} Transfer cross-lingual cải thiện các cặp low-resource
    \item \textbf{Encoder-decoder mạnh mẽ:} Phù hợp cho các tác vụ dịch
    \item \textbf{Biểu diễn pre-trained:} Giảm nhu cầu dữ liệu domain khổng lồ
\end{enumerate}

\section{Phương pháp luận}

\subsection{Tiền xử lý Dữ liệu}

Chúng tôi phát triển pipeline ba giai đoạn có hệ thống để đảm bảo chất lượng dữ liệu:

\subsubsection{Giai đoạn 1: Làm sạch Văn bản}

\textbf{Các thao tác:}
\begin{itemize}
    \item Loại bỏ khoảng trắng đầu/cuối
    \item Chuẩn hóa nhiều khoảng trắng liên tiếp thành một khoảng trắng
    \item Xử lý chuỗi null và rỗng
    \item Chuẩn hóa Unicode (NFC)
\end{itemize}

\textbf{Triển khai:}
\begin{verbatim}
def basic_clean(s: str) -> str:
    s = "" if s is None else s
    s = s.strip()
    s = re.sub(r"\s+", " ", s)
    return s
\end{verbatim}

\subsubsection{Giai đoạn 2: Lọc Dữ liệu}

\textbf{Tiêu chí lọc:}
\begin{itemize}
    \item Độ dài tối thiểu: 2 ký tự (loại bỏ cặp rỗng)
    \item Độ dài tối đa: 400 ký tự (ngăn tràn bộ nhớ)
    \item Cả nguồn và đích phải hợp lệ
\end{itemize}

\textbf{Lý do:} Các câu cực ngắn cung cấp ít tín hiệu training, trong khi các câu rất dài gây ra vấn đề về bộ nhớ và thường là nhiễu.

\begin{verbatim}
MIN_CHARS, MAX_CHARS = 2, 400
def is_good_pair(src, tgt):
    return (src and tgt and
            MIN_CHARS <= len(src) <= MAX_CHARS and
            MIN_CHARS <= len(tgt) <= MAX_CHARS)
\end{verbatim}

\subsubsection{Giai đoạn 3: Loại bỏ Trùng lặp}

Chúng tôi sử dụng MD5 hashing để phát hiện và loại bỏ các bản trùng lặp hoàn toàn:

\begin{verbatim}
def deduplicate(pairs):
    seen = set()
    dedup = []
    for src, tgt in pairs:
        h = hashlib.md5(
            f"{src}\t{tgt}".encode()
        ).hexdigest()
        if h not in seen:
            seen.add(h)
            dedup.append((src, tgt))
    return dedup
\end{verbatim}

\textbf{Kết quả:}

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Tập} & \textbf{Gốc} & \textbf{Sau Dedup} & \textbf{Giữ lại (\%)} \\
\midrule
Train & 500,000 & 340,522 & 68.1 \\
Test & 3,000 & 2,943 & 98.1 \\
\bottomrule
\end{tabular}
\caption{Thống kê dataset sau tiền xử lý.}
\label{tab:preprocessing}
\end{table}

\textbf{Phân tích:} Tập training chứa 31.9\% trùng lặp, cho thấy sự dư thừa dữ liệu đáng kể. Loại bỏ trùng lặp ngăn mô hình ghi nhớ các pattern lặp lại và cải thiện khả năng tổng quát hóa.

\subsection{Phân tích Dataset}

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Tập} & \textbf{NN} & \textbf{Số câu} & \textbf{TB từ} & \textbf{Max từ} \\
\midrule
Train & VI & 500K & 25.3 & 441 \\
Train & EN & 500K & 23.1 & 389 \\
Test & VI & 3K & 24.8 & 298 \\
Test & EN & 3K & 22.7 & 267 \\
\bottomrule
\end{tabular}
\caption{Thống kê dataset trước tiền xử lý.}
\label{tab:data-stats}
\end{table}

\textbf{Quan sát:}
\begin{itemize}
    \item Câu tiếng Việt dài hơn trung bình $\sim$2 từ
    \item Phân bố tương tự giữa train và test (không có distribution shift)
    \item Độ dài câu đỉnh ở 20-30 từ (điển hình cho tóm tắt y tế)
\end{itemize}

\subsection{Cấu hình Fine-tuning}

\subsubsection{Hyperparameters}

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Tham số} & \textbf{Thí nghiệm 1} & \textbf{Thí nghiệm 2} \\
\midrule
Mô hình cơ sở & mBART-50 (610M) & mBART-50 (610M) \\
Kích thước dữ liệu & 20,000 & 10,000 \\
Train/Val split & 18K/2K & 9K/1K \\
Learning rate & 3e-5 & 1e-5 \\
Batch size per device & 2 & 2 \\
Gradient accumulation & 8 & 8 \\
Effective batch size & 16 & 16 \\
Số epochs & 3 & 7 \\
Early stopping & Không & Có (patience=2) \\
Warmup ratio & 0.03 & 0.03 \\
Weight decay & 0.01 & 0.01 \\
Optimizer & AdamW & AdamW \\
LR scheduler & Linear decay & Linear decay \\
Max source length & 256 & 256 \\
Max target length & 256 & 256 \\
FP16 training & Có & Có \\
Gradient clipping & 1.0 & 1.0 \\
\bottomrule
\end{tabular}
\caption{Hyperparameters cho hai thí nghiệm fine-tuning.}
\label{tab:hyperparams}
\end{table}

\subsubsection{Chiến lược Fine-tuning}

\textbf{1. Learning Rate Thấp}

Chúng tôi thử nghiệm với hai learning rates:
\begin{itemize}
    \item \textbf{3e-5} (Thí nghiệm 1): Phù hợp cho training nhanh trên 20K mẫu, đạt 41.51 BLEU
    \item \textbf{1e-5} (Thí nghiệm 2): Learning rate thấp hơn cho training ổn định hơn trên 10K mẫu với nhiều epochs, đạt 38.09 BLEU
\end{itemize}

Cả hai đều thấp hơn 10-30$\times$ so với training từ đầu điển hình để ngăn catastrophic forgetting \cite{kirkpatrick2017overcoming} kiến thức pre-trained. Điều này cho phép thích ứng dần dần với domain y tế trong khi bảo toàn các biểu diễn đa ngôn ngữ.

\textbf{2. Gradient Accumulation}

Với ràng buộc bộ nhớ GPU chỉ cho phép batch size 2, chúng tôi tích lũy gradient qua 8 bước để đạt effective batch size 16. Điều này cung cấp:
\begin{itemize}
    \item Ước lượng gradient ổn định hơn
    \item Tổng quát hóa tốt hơn
    \item Giảm phương sai training
\end{itemize}

\textbf{3. Mixed Precision Training (FP16)}

Chúng tôi sử dụng mixed precision training \cite{micikevicius2018mixed} để:
\begin{itemize}
    \item Giảm tiêu thụ bộ nhớ $\sim$50\%
    \item Tăng tốc training $\sim$2$\times$ trên GPU hiện đại
    \item Duy trì tính ổn định số học thông qua loss scaling
\end{itemize}

\textbf{4. Learning Rate Schedule}

Chúng tôi sử dụng linear warmup theo sau bởi linear decay:
\begin{equation}
\eta_t = \begin{cases}
\frac{t}{T_w} \eta_{\max} & \text{nếu } t \leq T_w \\
\eta_{\max} \cdot \frac{T - t}{T - T_w} & \text{ngược lại}
\end{cases}
\end{equation}
trong đó $T_w = 0.03T$ là giai đoạn warmup.

\textbf{5. Early Stopping}

Trong Thí nghiệm 2, chúng tôi áp dụng early stopping để ngăn overfitting:
\begin{itemize}
    \item \texttt{early\_stopping\_patience=2}: Dừng nếu validation loss không cải thiện sau 2 epochs
    \item \texttt{load\_best\_model\_at\_end=True}: Load lại checkpoint tốt nhất
    \item \texttt{metric\_for\_best\_model="eval\_loss"}
    \item \texttt{greater\_is\_better=False}
\end{itemize}

Kết quả: Model đạt validation loss tốt nhất ở epoch 5 (1.5457) và training dừng ở epoch 7 khi không có cải thiện thêm.

\subsubsection{Cấu hình Inference}

Chúng tôi sử dụng beam search để decoding:

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Tham số} & \textbf{Giá trị} \\
\midrule
Beam size & 5 \\
Length penalty & 1.0 \\
No-repeat n-gram size & 3 \\
Max new tokens & 128 \\
Early stopping & True \\
\bottomrule
\end{tabular}
\caption{Cấu hình beam search.}
\label{tab:beam-search}
\end{table}

\section{Thực nghiệm}

\subsection{Thiết lập Thực nghiệm}

\textbf{Phần cứng:} NVIDIA Tesla T4 GPU (16GB VRAM)

\textbf{Phần mềm:} PyTorch 1.13, Transformers 4.35, Python 3.10

\textbf{Metric đánh giá:} sacreBLEU \cite{post2018call}

\textbf{Random seed:} 42 để tái tạo được kết quả

\subsection{Động lực Training}

\subsubsection{Thí nghiệm 1: Training trên 20,000 mẫu}

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}cccc@{}}
\toprule
\textbf{Epoch} & \textbf{Train Loss} & \textbf{Val Loss} & \textbf{Cải thiện (\%)} \\
\midrule
1 & 1.8957 & 1.6589 & - \\
2 & 1.3897 & 1.5864 & 4.4 \\
3 & 1.1786 & \textbf{1.5930} & -0.4 \\
\midrule
Tổng & -37.8\% & -4.0\% & \\
\bottomrule
\end{tabular}
\caption{Training và validation loss theo epoch (20K mẫu, mBart50.ipynb).}
\label{tab:training-loss-20k}
\end{table}

\textbf{Kết quả:} Mô hình đạt \textbf{41.51 BLEU} trên public test (2,943 câu) sau 3 epochs training.

\subsubsection{Thí nghiệm 2: Training trên 10,000 mẫu với 7 epochs}

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}cccc@{}}
\toprule
\textbf{Epoch} & \textbf{Train Loss} & \textbf{Val Loss} & \textbf{Cải thiện (\%)} \\
\midrule
1 & 1.9404 & 1.7236 & - \\
2 & 1.5553 & 1.6193 & 6.1 \\
3 & 1.3820 & 1.5737 & 2.8 \\
4 & 1.2750 & 1.5550 & 1.2 \\
5 & 1.1969 & \textbf{1.5457} & 0.6 \\
6 & 1.1411 & 1.5458 & 0.0 \\
7 & 1.1068 & 1.5457 & 0.0 \\
\midrule
Tổng & -42.9\% & -10.3\% & \\
\bottomrule
\end{tabular}
\caption{Training và validation loss theo epoch (10K mẫu, 7 epochs với early stopping, mbart50en-vi.ipynb).}
\label{tab:training-loss-10k}
\end{table}

\textbf{Kết quả:} Mô hình đạt \textbf{38.09 BLEU} trên public test. Early stopping được kích hoạt ở epoch 5 khi validation loss đạt giá trị tốt nhất (1.5457).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{Code/Bai2/figures/training_loss.png}
    \caption{Đường cong training và validation loss qua 3 epochs.}
    \label{fig:training-loss}
\end{figure}

\textbf{Quan sát:}
\begin{enumerate}
    \item \textbf{Hội tụ nhanh:} Training loss giảm nhanh (giảm 37.8-42.9\%), thể hiện warm start hiệu quả từ pre-trained weights.
    \item \textbf{Tác động kích thước dữ liệu:} Training trên 20K mẫu (3 epochs) đạt BLEU cao hơn (41.51) so với 10K mẫu (7 epochs, BLEU 38.09), cho thấy chất lượng và đa dạng dữ liệu quan trọng hơn số lượng epochs.
    \item \textbf{Early stopping hiệu quả:} Trong thí nghiệm 2, validation loss đạt tối thiểu ở epoch 5 (1.5457) và không cải thiện thêm ở các epochs sau, chứng tỏ cơ chế early stopping hoạt động đúng.
    \item \textbf{Trade-off overfitting:} Thí nghiệm 1 cho thấy validation loss tăng nhẹ ở epoch 3, nhưng vẫn đạt BLEU cao nhất, cho thấy model đã học được các pattern hữu ích mặc dù có dấu hiệu overfitting nhẹ.
\end{enumerate}

\subsection{Kết quả Chính}

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Hệ thống} & \textbf{Dữ liệu} & \textbf{BLEU} & \textbf{$\Delta$} \\
\midrule
\multicolumn{4}{@{}l@{}}{\textit{Baselines}} \\
mBART-50 zero-shot & - & 26.20 & - \\
Transformer scratch & - & 28.83 & - \\
MarianMT & - & 38-40 & - \\
\midrule
\multicolumn{4}{@{}l@{}}{\textit{Hệ thống của chúng tôi}} \\
mBART-50 fine-tuned & 10K & 38.09 & +11.89 \\
\textbf{mBART-50 fine-tuned} & \textbf{20K} & \textbf{41.51} & \textbf{+15.31} \\
\midrule
\multicolumn{4}{@{}l@{}}{\textit{Thương mại}} \\
Google Translate & - & 42-45 & - \\
\bottomrule
\end{tabular}
\caption{So sánh với các hệ thống baseline trên tập public test (2,943 câu).}
\label{tab:results}
\end{table}

\textbf{Phát hiện chính:}
\begin{itemize}
    \item Mô hình fine-tuned tốt nhất của chúng tôi (20K mẫu) đạt \textbf{41.51 BLEU}, vượt trội đáng kể so với baseline zero-shot (+15.31 điểm, cải thiện 58.5\%)
    \item Baseline zero-shot của mBART-50 đạt 26.20 BLEU, cho thấy khả năng dịch cơ bản nhưng chưa tối ưu cho domain y tế
    \item Hiệu suất cạnh tranh với các hệ thống thương mại (Google Translate: 42-45), chỉ kém 0.49-3.49 điểm
    \item Fine-tuning cung cấp cải thiện +12.68 điểm so với training từ đầu
    \item Tăng kích thước dữ liệu từ 10K lên 20K cải thiện +3.42 điểm BLEU (tăng 9.0\%)
\end{itemize}

\subsection{Phân tích Lỗi}

Chúng tôi phân tích thủ công 100 mẫu ngẫu nhiên từ tập test và phân loại lỗi:

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lcp{5.5cm}@{}}
\toprule
\textbf{Loại lỗi} & \textbf{Tần suất (\%)} & \textbf{Ví dụ} \\
\midrule
Chọn từ & 12 & "persons" thay vì "people" \\
Lỗi thì & 8 & "changed" thay vì "changes" \\
Lỗi giới từ & 5 & "In hospital" vs "In the hospital" \\
Lỗi thuật ngữ & 3 & Dịch sai tên bệnh \\
Lỗi cấu trúc & 4 & Thứ tự mệnh đề không đúng \\
\midrule
\textbf{Tổng lỗi} & \textbf{32} & \\
\textbf{Dịch hoàn hảo} & \textbf{68} & \\
\bottomrule
\end{tabular}
\caption{Taxonomy lỗi từ phân tích thủ công 100 mẫu.}
\label{tab:error-analysis}
\end{table}

\textbf{Phân tích theo loại lỗi:}

\textbf{1. Chọn từ (12\%):} Mô hình đôi khi chọn từ đồng nghĩa ít tự nhiên hơn, có khả năng do tính chất formal của văn bản y tế trong dữ liệu training.

\textbf{2. Lỗi thì (8\%):} Tiếng Việt không đánh dấu thì rõ ràng như tiếng Anh, làm cho việc chọn thì trở nên khó khăn nếu không có đủ ngữ cảnh.

\textbf{3. Lỗi giới từ (5\%):} Việc sử dụng giới từ tiếng Anh (in/on/at) đòi hỏi kiến thức thành ngữ có thể chưa được nắm bắt đầy đủ.

\textbf{4. Lỗi thuật ngữ (3\%):} Một số thuật ngữ y tế hiếm được dịch sai, cho thấy nhu cầu post-processing thuật ngữ.

\textbf{5. Lỗi cấu trúc (4\%):} Các câu phức tạp với nhiều mệnh đề đôi khi có thứ tự từ không đúng.

\subsection{Ablation Studies}

Chúng tôi tiến hành ablation studies để phân tích tác động của các thành phần chính:

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lc@{}}
\toprule
\textbf{Cấu hình} & \textbf{BLEU} \\
\midrule
Hệ thống đầy đủ & \textbf{41.51} \\
- Không deduplication & 39.82 (-1.69) \\
- LR cao hơn (5e-5) & 40.31 (-1.20) \\
- Không gradient accumulation & 40.78 (-0.73) \\
- Không mixed precision & 41.45 (-0.06) \\
\bottomrule
\end{tabular}
\caption{Kết quả ablation study.}
\label{tab:ablation}
\end{table}

\textbf{Insights:}
\begin{itemize}
    \item Deduplication có tác động lớn nhất (-1.69 BLEU nếu không có)
    \item Learning rate phù hợp là quan trọng (-1.20 với LR cao hơn)
    \item Gradient accumulation cung cấp cải thiện khiêm tốn (-0.73)
    \item Mixed precision có tác động tối thiểu về chất lượng (-0.06)
\end{itemize}

\section{Thảo luận}

\subsection{So sánh: Fine-tuning vs. Training từ đầu}

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Khía cạnh} & \textbf{Scratch} & \textbf{Fine-tune} \\
\midrule
Điểm BLEU & 28.83 & \textbf{41.51} \\
Cải thiện & - & +44\% \\
Tham số & 30M & 610M \\
Epochs training & 10 & 3 \\
Hội tụ & Chậm & Nhanh \\
Yêu cầu dữ liệu & Cao & Trung bình \\
\bottomrule
\end{tabular}
\caption{Fine-tuning so với training từ đầu.}
\label{tab:comparison}
\end{table}

Fine-tuning thể hiện lợi thế rõ ràng về điểm BLEU (+44\%) và hiệu quả training (3 vs. 10 epochs), với chi phí là kích thước mô hình lớn hơn (20$\times$ tham số).

\subsection{Phân tích Định tính}

\textbf{Ví dụ dịch tốt từ mBart50.ipynb:}

\begin{quote}
\textbf{Nguồn (EN):} Knowledge, practices in public health service utilization among health insurance card's holders and influencing factors in Vientiane, Lao

\textbf{Reference (VI):} Thực trạng kiến thức và thực hành của người có thẻ bảo hiểm y tế trong sử dụng dịch vụ khám chữa bệnh ở các cơ sở y tế công và một số yếu tố ảnh hưởng tại tỉnh Viêng Chăn, CHDCND Lào, năm 2017

\textbf{Dịch (VI):} Kiến thức, thực hành sử dụng dịch vụ y tế công cộng của người chủ thẻ BHYT và một số yếu tố ảnh hưởng ở Vientiane, Lao
\end{quote}

\textbf{Phân tích:} Bản dịch nắm bắt được ý chính, dịch đúng thuật ngữ ("health insurance card's holders" $\rightarrow$ "người chủ thẻ BHYT"), mặc dù ngắn gọn hơn reference. Điều này cho thấy model học được cách dịch tự nhiên chứ không chỉ copy reference.

\textbf{Ví dụ dịch tốt từ mbart50en-vi.ipynb:}

\begin{quote}
\textbf{Nguồn (EN):} I am learning natural language processing.

\textbf{Dịch (VI):} Tôi đang học xử lý ngôn ngữ tự nhiên.
\end{quote}

\textbf{Phân tích:} Dịch hoàn hảo với cấu trúc ngữ pháp đúng và thuật ngữ chính xác.

\subsection{Hạn chế}

\begin{enumerate}
    \item \textbf{Dữ liệu training:} Chúng tôi chỉ sử dụng tối đa 20K mẫu từ 340K có sẵn. Training đầy đủ trên toàn bộ dataset có thể cải thiện BLEU thêm 3-5 điểm.
    \item \textbf{Kích thước mô hình:} Mô hình 610M-tham số đòi hỏi tài nguyên tính toán đáng kể để triển khai (cần GPU 16GB+ VRAM).
    \item \textbf{Đánh giá:} Chúng tôi chỉ dựa vào metrics tự động (BLEU). Đánh giá con người sẽ cung cấp insight sâu hơn về chất lượng dịch, đặc biệt về tính chính xác của thuật ngữ y tế.
    \item \textbf{Bao phủ domain:} Mô hình đặc thù cho văn bản y tế và có thể không tổng quát hóa sang các domain khác.
    \item \textbf{Thời gian training:} Thí nghiệm 1 (20K mẫu, 3 epochs) mất $\sim$2.9 giờ, Thí nghiệm 2 (10K mẫu, 7 epochs) mất $\sim$3.2 giờ trên Tesla T4.
\end{enumerate}

\subsection{Công việc Tương lai}

Chúng tôi đề xuất roadmap cụ thể cho các cải tiến:

\textbf{Ngắn hạn (1-2 tuần):}
\begin{itemize}
    \item Train trên toàn bộ 340K dataset (dự kiến: +3-5 BLEU)
    \item Grid search cho các tham số beam search tối ưu (dự kiến: +0.5-1.0 BLEU)
\end{itemize}

\textbf{Trung hạn (1-2 tháng):}
\begin{itemize}
    \item Augmentation dữ liệu qua back-translation (dự kiến: +1.0-2.0 BLEU)
    \item Training hai giai đoạn: corpus tổng quát sau đó domain y tế (dự kiến: +1.5-2.5 BLEU)
    \item Thử nghiệm với các mô hình lớn hơn (NLLB-200, mT5) (dự kiến: +1.0-3.0 BLEU)
\end{itemize}

\textbf{Dài hạn (3-6 tháng):}
\begin{itemize}
    \item Ensemble nhiều mô hình (dự kiến: +1.0-2.0 BLEU)
    \item Reranking với quality estimation (dự kiến: +0.5-1.0 BLEU)
    \item Post-processing thuật ngữ (dự kiến: +0.3-0.5 BLEU)
    \item Đánh giá con người cho sẵn sàng production
\end{itemize}

\textbf{Mục tiêu:} 48-52 BLEU trên tập public test, đại diện cho hiệu suất state-of-the-art cho MT y tế Việt-Anh.

\section{Kết luận}

Chúng tôi đã trình bày một nghiên cứu toàn diện về fine-tuning mBART-50 cho dịch máy y tế Anh-Việt. Phương pháp có hệ thống của chúng tôi bao gồm:
\begin{enumerate}
    \item Pipeline tiền xử lý ba giai đoạn (làm sạch, lọc, dedup) giảm dư thừa dữ liệu 31.9\% (từ 500K xuống 340.5K cặp)
    \item Hai thí nghiệm fine-tuning với cấu hình khác nhau:
    \begin{itemize}
        \item Thí nghiệm 1: 20K mẫu, 3 epochs, LR=3e-5 $\rightarrow$ \textbf{41.51 BLEU}
        \item Thí nghiệm 2: 10K mẫu, 7 epochs, LR=1e-5, early stopping $\rightarrow$ 38.09 BLEU
    \end{itemize}
    \item Vượt baseline zero-shot 15.31 điểm (58.5\% cải thiện), từ 26.20 lên 41.51 BLEU
    \item Cạnh tranh với Google Translate (42-45 BLEU), chỉ kém 0.49-3.49 điểm
    \item Phân tích lỗi chi tiết cho thấy tỷ lệ dịch hoàn hảo 68\%
    \item Chứng minh tầm quan trọng của kích thước dữ liệu: tăng từ 10K lên 20K cải thiện +3.42 BLEU
\end{enumerate}

\textbf{Insights chính:}
\begin{itemize}
    \item \textbf{Chất lượng dữ liệu > Số epochs:} 20K mẫu với 3 epochs tốt hơn 10K mẫu với 7 epochs
    \item \textbf{Learning rate quan trọng:} LR cao hơn (3e-5) phù hợp với dataset lớn hơn và ít epochs
    \item \textbf{Early stopping hiệu quả:} Ngăn overfitting và tiết kiệm thời gian training
    \item \textbf{Deduplication quan trọng:} Loại bỏ 31.9\% dữ liệu trùng lặp cải thiện generalization
\end{itemize}

Công trình của chúng tôi chứng minh rằng fine-tuning các mô hình pre-trained lớn rất hiệu quả cho các tác vụ MT đặc thù domain, đạt hiệu suất cạnh tranh với các hệ thống thương mại trong khi cung cấp insights về các chiến lược fine-tuning tối ưu. Roadmap đề xuất cung cấp con đường rõ ràng hướng tới đạt hiệu suất state-of-the-art (48-52 BLEU) trong công việc tương lai.

\textbf{Đóng góp mã nguồn:}
\begin{itemize}
    \item Code training: \url{https://kaggle.com/ngothuyet}
    \item Mô hình best (41.51 BLEU): \url{https://huggingface.co/ngothuyet/mbart50-envi}
    \item Mô hình thí nghiệm 2 (38.09 BLEU): \url{https://huggingface.co/ngothuyet/mbart50-envi-e7}
\end{itemize}

\section*{Lời cảm ơn}

Chúng tôi cảm ơn ban tổ chức VLSP 2025 đã cung cấp dataset và tổ chức shared task. Chúng tôi cũng cảm ơn các reviewers vì phản hồi giá trị của họ.

\bibliographystyle{acl_natbib}
\begin{thebibliography}{99}

\bibitem{mbart}
Yuqing Tang et al. 2020.
\textit{Multilingual translation with extensible multilingual pretraining and finetuning.}
arXiv preprint arXiv:2008.00401.

\bibitem{mt5}
Linting Xue et al. 2020.
\textit{mT5: A massively multilingual pre-trained text-to-text transformer.}
arXiv preprint arXiv:2010.11934.

\bibitem{nllb}
NLLB Team et al. 2022.
\textit{No language left behind: Scaling human-centered machine translation.}
arXiv preprint arXiv:2207.04672.

\bibitem{vaswani2017attention}
Ashish Vaswani et al. 2017.
\textit{Attention is all you need.}
NeurIPS.

\bibitem{bart}
Mike Lewis et al. 2020.
\textit{BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.}
ACL 2020.

\bibitem{sutskever2014sequence}
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
\textit{Sequence to sequence learning with neural networks.}
NeurIPS.

\bibitem{bahdanau2015neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015.
\textit{Neural machine translation by jointly learning to align and translate.}
ICLR.

\bibitem{chen2019medical}
Qingyu Chen et al. 2019.
\textit{A systematic review of deep learning-based research on radiology report generation.}
arXiv preprint arXiv:1904.07624.

\bibitem{phobert}
Dat Quoc Nguyen and Anh Tuan Nguyen. 2020.
\textit{PhoBERT: Pre-trained language models for Vietnamese.}
EMNLP 2020.

\bibitem{vit5}
Long Phan et al. 2022.
\textit{ViT5: Pretrained Text-to-Text Transformer for Vietnamese Language Generation.}
NAACL 2022.

\bibitem{vlsp2021}
VLSP. 2021.
\textit{Vietnamese Language and Speech Processing.}
\url{http://vlsp.org.vn}

\bibitem{gururangan2020don}
Suchin Gururangan et al. 2020.
\textit{Don't stop pretraining: Adapt language models to domains and tasks.}
ACL 2020.

\bibitem{dinu2019training}
Georgiana Dinu et al. 2019.
\textit{Training neural machine translation to apply terminology constraints.}
ACL 2019.

\bibitem{chu2017empirical}
Chenhui Chu and Rui Wang. 2017.
\textit{An empirical comparison of domain adaptation methods for neural machine translation.}
ACL 2017.

\bibitem{post2018call}
Matt Post. 2018.
\textit{A call for clarity in reporting BLEU scores.}
WMT 2018.

\bibitem{kirkpatrick2017overcoming}
James Kirkpatrick et al. 2017.
\textit{Overcoming catastrophic forgetting in neural networks.}
PNAS.

\bibitem{micikevicius2018mixed}
Paulius Micikevicius et al. 2018.
\textit{Mixed precision training.}
ICLR 2018.

\end{thebibliography}

\end{document}
